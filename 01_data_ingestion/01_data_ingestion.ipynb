{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion for Fake News Detection\n",
    "\n",
    "This notebook demonstrates the data ingestion process for our fake news detection pipeline using Hive metastore tables. Data ingestion is the first critical step in any data science project, as it involves collecting, loading, and preparing the raw data for further processing.\n",
    "\n",
    "## Why Data Ingestion is Important\n",
    "\n",
    "In fake news detection, proper data ingestion ensures:\n",
    "1. Data quality and consistency\n",
    "2. Appropriate labeling of real and fake news articles\n",
    "3. Balanced representation of both classes\n",
    "4. Efficient storage for distributed processing\n",
    "\n",
    "This notebook will guide you through the process of loading, combining, and processing news data using Apache Spark for distributed processing, with a focus on leveraging Hive metastore tables in Databricks for the complete dataset (approximately 45,000 articles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, when, count, desc, rand\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Import our custom Hive data ingestion module\n",
    "# Note: In Databricks, you may need to adjust this import path\n",
    "# You can use %run ./hive_data_ingestion instead\n",
    "import sys\n",
    "sys.path.append('/dbfs/FileStore/tables')\n",
    "from hive_data_ingestion import HiveDataIngestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spark Session with Hive Support\n",
    "\n",
    "We'll use Apache Spark for distributed data processing, with Hive support enabled to access the metastore tables. Let's create a properly configured Spark session optimized for the Databricks Community Edition limitations (1 driver, 15.3 GB Memory, 2 Cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session with configuration optimized for Databricks Community Edition\n",
    "# - appName: Identifies this application in the Spark UI and logs\n",
    "# - spark.sql.shuffle.partitions: Set to 8 (4x number of cores) for Community Edition\n",
    "# - spark.driver.memory: Set to 8g to utilize available memory while leaving room for system\n",
    "# - enableHiveSupport: Enables access to Hive metastore tables\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display Spark version information\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark configuration: {spark.sparkContext.getConf().getAll()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Hive Metastore Tables\n",
    "\n",
    "In Databricks, the Hive metastore provides a centralized repository to store metadata for tables and partitions. For our fake news detection project, we have two tables in the Hive metastore:\n",
    "\n",
    "1. **`fake`**: Contains fake news articles with columns: title, text, subject, date\n",
    "2. **`real`**: Contains real news articles with columns: title, text, subject, date\n",
    "\n",
    "### Why Use Hive Tables?\n",
    "\n",
    "Using Hive metastore tables offers several advantages over reading from CSV files:\n",
    "\n",
    "1. **Centralized Metadata**: Schema information is stored centrally, ensuring consistency across sessions and users\n",
    "2. **Optimized Performance**: Databricks optimizes query execution on Hive tables\n",
    "3. **Access Control**: Tables can have access controls applied at the table level\n",
    "4. **Persistence**: Tables persist across cluster restarts and sessions\n",
    "5. **Catalog Integration**: Tables are visible in the Databricks catalog UI\n",
    "\n",
    "Let's explore these tables using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in the default database\n",
    "print(\"Available tables in the Hive metastore:\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Directory Structure in DBFS\n",
    "\n",
    "Let's create a directory structure in DBFS (Databricks File System) to organize our processed data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_structure():\n",
    "    \"\"\"Create directory structure for data storage in DBFS.\n",
    "    \n",
    "    This function creates the necessary directories for storing:\n",
    "    - Combined data: The full dataset with both real and fake news\n",
    "    - Processed data: Data after preprocessing steps\n",
    "    - Model data: Data used for model training and evaluation\n",
    "    - Sample data: Optional balanced samples for development with limited resources\n",
    "    \"\"\"\n",
    "    # In Databricks, we use dbutils to interact with DBFS\n",
    "    directories = [\n",
    "        \"dbfs:/FileStore/fake_news_detection/data/combined_data\",\n",
    "        \"dbfs:/FileStore/fake_news_detection/data/processed_data\",\n",
    "        \"dbfs:/FileStore/fake_news_detection/data/model_data\",\n",
    "        \"dbfs:/FileStore/fake_news_detection/data/sample_data\"\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        # Remove dbfs: prefix for dbutils.fs.mkdirs\n",
    "        dir_path = directory.replace(\"dbfs:\", \"\")\n",
    "        dbutils.fs.mkdirs(dir_path)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "\n",
    "# Create the directory structure\n",
    "create_directory_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Exploring Data from Hive Tables\n",
    "\n",
    "Now, let's load the data from Hive metastore tables and explore their structure. We'll use our custom `HiveDataIngestion` class to handle this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HiveDataIngestion class\n",
    "# We specify the table names in the Hive metastore\n",
    "ingestion = HiveDataIngestion(spark, real_table=\"real\", fake_table=\"fake\")\n",
    "\n",
    "# Load data from Hive tables\n",
    "try:\n",
    "    # This loads data from the Hive tables and registers them as temporary views\n",
    "    real_df, fake_df = ingestion.load_data_from_hive()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets from Hive: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management for Community Edition\n",
    "\n",
    "Since we're working with the Databricks Community Edition (15.3 GB Memory, 2 Cores), we need to be careful about memory usage. Let's check the size of our datasets and implement memory-efficient processing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset sizes\n",
    "real_count = real_df.count()\n",
    "fake_count = fake_df.count()\n",
    "total_count = real_count + fake_count\n",
    "\n",
    "print(f\"Real news dataset: {real_count} records\")\n",
    "print(f\"Fake news dataset: {fake_count} records\")\n",
    "print(f\"Total dataset size: {total_count} records\")\n",
    "\n",
    "# Memory management tips for Community Edition\n",
    "print(\"\\nMemory Management Tips for Databricks Community Edition:\")\n",
    "print(\"1. Process data in smaller batches when possible\")\n",
    "print(\"2. Use .unpersist() to release cached DataFrames when no longer needed\")\n",
    "print(\"3. Consider using sampling for exploratory analysis and model development\")\n",
    "print(\"4. Minimize the number of wide transformations (joins, groupBy, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration with Spark SQL\n",
    "\n",
    "Let's use Spark SQL to explore our datasets. SQL provides a familiar syntax for data exploration while leveraging Spark's distributed processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore real news data using Spark SQL\n",
    "print(\"Sample of real news articles:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT title, text, subject, date\n",
    "    FROM true_news\n",
    "    LIMIT 3\n",
    "\"\"\").show(truncate=50)\n",
    "\n",
    "# Explore fake news data using Spark SQL\n",
    "print(\"\\nSample of fake news articles:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT title, text, subject, date\n",
    "    FROM fake_news\n",
    "    LIMIT 3\n",
    "\"\"\").show(truncate=50)\n",
    "\n",
    "# Count articles by subject in real news\n",
    "print(\"\\nReal news articles by subject:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT subject, COUNT(*) as count\n",
    "    FROM true_news\n",
    "    GROUP BY subject\n",
    "    ORDER BY count DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Count articles by subject in fake news\n",
    "print(\"\\nFake news articles by subject:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT subject, COUNT(*) as count\n",
    "    FROM fake_news\n",
    "    GROUP BY subject\n",
    "    ORDER BY count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Subject Distribution and Potential Data Leakage\n",
    "\n",
    "Let's analyze the distribution of the 'subject' column across real and fake news to check if it might be a perfect discriminator between classes, which would indicate potential data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze subject distribution across real and fake news\n",
    "print(\"Analyzing subject distribution across classes...\")\n",
    "\n",
    "# First, let's combine the datasets with labels\n",
    "real_df_with_label = real_df.withColumn(\"label\", lit(1))  # 1 for real news\n",
    "fake_df_with_label = fake_df.withColumn(\"label\", lit(0))  # 0 for fake news\n",
    "temp_combined = real_df_with_label.unionByName(fake_df_with_label)\n",
    "temp_combined.createOrReplaceTempView(\"temp_combined\")\n",
    "\n",
    "# Check if any subjects appear in both real and fake news\n",
    "print(\"\\nSubjects that appear in both real and fake news:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT subject, \n",
    "           SUM(CASE WHEN label = 1 THEN 1 ELSE 0 END) as real_count,\n",
    "           SUM(CASE WHEN label = 0 THEN 1 ELSE 0 END) as fake_count\n",
    "    FROM temp_combined\n",
    "    GROUP BY subject\n",
    "    HAVING real_count > 0 AND fake_count > 0\n",
    "    ORDER BY real_count + fake_count DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Calculate correlation between subject and label\n",
    "print(\"\\nAnalyzing correlation between subject and label...\")\n",
    "# Convert subject to numeric using StringIndexer in a later step\n",
    "# For now, let's check how many unique subjects are in each class\n",
    "print(\"\\nNumber of unique subjects in each class:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT label, COUNT(DISTINCT subject) as unique_subjects\n",
    "    FROM temp_combined\n",
    "    GROUP BY label\n",
    "    ORDER BY label DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Check if subject perfectly separates the classes\n",
    "print(\"\\nCombined dataset statistics:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT label, COUNT(*) as count, COUNT(DISTINCT subject) as unique_subjects\n",
    "    FROM temp_combined\n",
    "    GROUP BY label\n",
    "    ORDER BY label DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Show top subjects by class\n",
    "print(\"\\nSubject distribution by label:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        subject,\n",
    "        SUM(CASE WHEN label = 1 THEN 1 ELSE 0 END) as real_count,\n",
    "        SUM(CASE WHEN label = 0 THEN 1 ELSE 0 END) as fake_count\n",
    "    FROM temp_combined\n",
    "    GROUP BY subject\n",
    "    ORDER BY real_count + fake_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()\n",
    "\n",
    "# Release memory\n",
    "temp_combined.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Leakage Warning\n",
    "\n",
    "**Important**: Based on the analysis above, the 'subject' column appears to be a strong predictor of whether an article is real or fake news. This could indicate potential data leakage, as the subject categories might be directly related to the source of the news rather than the content itself.\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **Consider removing the 'subject' column** from the feature set to ensure the model learns from the actual content rather than the source categorization.\n",
    "\n",
    "2. **Alternatively, create two model variants** - one with and one without the 'subject' feature - to compare performance and understand the impact.\n",
    "\n",
    "3. **Perform cross-validation** with careful stratification to ensure the model generalizes well across different subjects.\n",
    "\n",
    "Let's proceed with adding labels and combining the datasets, keeping this potential data leakage in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Labels and Combining Datasets\n",
    "\n",
    "Now, let's add labels to our datasets (1 for real news, 0 for fake news) and combine them into a single dataset. This labeling is crucial for training machine learning models to distinguish between real and fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets with labels using our ingestion class\n",
    "combined_df = ingestion.combine_datasets(real_df, fake_df)\n",
    "\n",
    "# Show distribution of subjects across labels\n",
    "print(\"\\nSubject distribution by label:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        subject,\n",
    "        SUM(CASE WHEN label = 1 THEN 1 ELSE 0 END) as real_count,\n",
    "        SUM(CASE WHEN label = 0 THEN 1 ELSE 0 END) as fake_count\n",
    "    FROM combined_news\n",
    "    GROUP BY subject\n",
    "    ORDER BY real_count + fake_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Combined Dataset\n",
    "\n",
    "Let's save the combined dataset in Parquet format in DBFS, which is optimized for distributed processing. We'll also save it as a Hive table for easier access. We'll use partitioning to optimize query performance on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined dataset in Parquet format to DBFS\n",
    "print(\"Saving combined dataset to DBFS...\")\n",
    "ingestion.save_to_parquet(\n",
    "    combined_df, \n",
    "    \"dbfs:/FileStore/fake_news_detection/data/combined_data/combined_news.parquet\",\n",
    "    partition_by=\"label\"\n",
    ")\n",
    "\n",
    "# Save combined dataset as a Hive table\n",
    "print(\"Saving combined dataset as a Hive table...\")\n",
    "ingestion.save_to_hive_table(\n",
    "    combined_df,\n",
    "    \"combined_news\",\n",
    "    partition_by=\"label\"\n",
    ")\n",
    "\n",
    "# Verify the saved data\n",
    "print(\"\\nVerifying saved data...\")\n",
    "saved_df = spark.read.parquet(\"dbfs:/FileStore/fake_news_detection/data/combined_data/combined_news.parquet\")\n",
    "print(f\"Saved dataset record count: {saved_df.count()}\")\n",
    "\n",
    "# Check partition structure\n",
    "print(\"\\nPartition structure:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT label, COUNT(*) as count\n",
    "    FROM parquet.`dbfs:/FileStore/fake_news_detection/data/combined_data/combined_news.parquet`\n",
    "    GROUP BY label\n",
    "    ORDER BY label DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Release memory\n",
    "saved_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Development Sample (Optional)\n",
    "\n",
    "For development and testing with limited resources in the Community Edition, we can create a balanced sample dataset. This is particularly useful for model development and testing, where the full dataset might be too large for efficient iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a balanced sample for development and testing\n",
    "print(\"Creating balanced sample for development...\")\n",
    "sample_size = 1000  # Adjust based on your memory constraints\n",
    "\n",
    "# Use stratified sampling to maintain class distribution\n",
    "sample_fractions = {\n",
    "    0: min(1.0, sample_size/fake_count),\n",
    "    1: min(1.0, sample_size/real_count)\n",
    "}\n",
    "\n",
    "sample_df = combined_df.sampleBy(\"label\", fractions=sample_fractions, seed=42)\n",
    "\n",
    "# Save sample to DBFS\n",
    "print(\"Saving sample to DBFS...\")\n",
    "sample_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"label\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet(\"dbfs:/FileStore/fake_news_detection/data/sample_data/sample_news.parquet\")\n",
    "\n",
    "# Save sample as Hive table\n",
    "print(\"Saving sample as Hive table...\")\n",
    "sample_df.write.mode(\"overwrite\").saveAsTable(\"sample_news\")\n",
    "\n",
    "# Verify sample\n",
    "print(\"\\nSample dataset statistics:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT label, COUNT(*) as count\n",
    "    FROM sample_news\n",
    "    GROUP BY label\n",
    "    ORDER BY label DESC\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\nNote: This sample is intended for development and testing only.\")\n",
    "print(\"For production models, use the full dataset or larger samples as resources permit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community Edition Resource Management\n",
    "\n",
    "When working with the Databricks Community Edition (15.3 GB Memory, 2 Cores), consider these resource management strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display resource management tips\n",
    "print(\"Resource Management Tips for Databricks Community Edition:\")\n",
    "print(\"\\n1. Spark Configuration:\")\n",
    "print(\"   - Use spark.sql.shuffle.partitions = 8 (4x number of cores)\")\n",
    "print(\"   - Allocate more memory to driver (8g) than executors\")\n",
    "print(\"   - Avoid excessive caching of large DataFrames\")\n",
    "\n",
    "print(\"\\n2. Data Processing:\")\n",
    "print(\"   - Use .unpersist() to release memory when DataFrames are no longer needed\")\n",
    "print(\"   - Process data in smaller batches when possible\")\n",
    "print(\"   - Use sampling for exploratory analysis and initial model development\")\n",
    "\n",
    "print(\"\\n3. Model Training:\")\n",
    "print(\"   - Use stratified sampling for complex models (LSTM, deep learning)\")\n",
    "print(\"   - Consider simpler models for the full dataset (Naive Bayes, Logistic Regression)\")\n",
    "print(\"   - Implement cross-validation with smaller fold counts (3 instead of 5)\")\n",
    "\n",
    "print(\"\\n4. Storage Optimization:\")\n",
    "print(\"   - Use Parquet format with Snappy compression\")\n",
    "print(\"   - Partition data by frequently filtered columns (e.g., label)\")\n",
    "print(\"   - Use Hive tables for efficient metadata management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Let's clean up our session to free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release memory\n",
    "if 'combined_df' in locals():\n",
    "    combined_df.unpersist()\n",
    "if 'real_df' in locals():\n",
    "    real_df.unpersist()\n",
    "if 'fake_df' in locals():\n",
    "    fake_df.unpersist()\n",
    "if 'sample_df' in locals():\n",
    "    sample_df.unpersist()\n",
    "\n",
    "print(\"Data ingestion completed successfully!\")\n",
    "print(\"The data is now ready for preprocessing and feature engineering.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
