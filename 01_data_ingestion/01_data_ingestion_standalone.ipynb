{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18180116",
   "metadata": {},
   "source": [
    "# Fake News Detection: Data Ingestion\n",
    "\n",
    "This notebook contains all the necessary code to load, process, and prepare data for the fake news detection project. The code is organized into independent functions, without dependencies on external modules or classes, to facilitate execution in Databricks Community Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd3e44",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93974c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, lower, regexp_replace, rand, when, concat\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235ef04",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Show Spark version\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0523d",
   "metadata": {},
   "source": [
    "## Directory Structure Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258b385",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_directory_structure(base_dir=\"/dbfs/FileStore/fake_news_detection\"):\n",
    "    \"\"\"\n",
    "    Creates the necessary directory structure for the fake news detection project.\n",
    "    \n",
    "    This function ensures all required directories exist in the Databricks environment.\n",
    "    It's essential to run this function before executing the rest of the pipeline.\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): Base directory for the project\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with paths to all created directories\n",
    "    \"\"\"\n",
    "    print(f\"Creating directory structure in {base_dir}...\")\n",
    "    \n",
    "    # Define directory paths\n",
    "    directories = {\n",
    "        \"data\": f\"{base_dir}/data\",\n",
    "        \"raw_data\": f\"{base_dir}/data/raw\",\n",
    "        \"processed_data\": f\"{base_dir}/data/processed\",\n",
    "        \"sample_data\": f\"{base_dir}/data/sample\",\n",
    "        \"models\": f\"{base_dir}/models\",\n",
    "        \"logs\": f\"{base_dir}/logs\",\n",
    "        \"visualizations\": f\"{base_dir}/visualizations\",\n",
    "        \"temp\": f\"{base_dir}/temp\"\n",
    "    }\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_name, dir_path in directories.items():\n",
    "        # Use dbutils in Databricks environment\n",
    "        try:\n",
    "            dbutils.fs.mkdirs(dir_path)\n",
    "            print(f\"Created directory: {dir_path}\")\n",
    "        except NameError:\n",
    "            # Fallback for non-Databricks environments\n",
    "            os.makedirs(dir_path.replace(\"/dbfs\", \"\"), exist_ok=True)\n",
    "            print(f\"Created directory: {dir_path} (local mode)\")\n",
    "    \n",
    "    print(\"Directory structure created successfully\")\n",
    "    return directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125fe34a",
   "metadata": {},
   "source": [
    "## Reusable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e6e0b",
   "metadata": {},
   "source": [
    "### Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432f52f0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_csv_files(fake_path, true_path, cache=True):\n",
    "    \"\"\"\n",
    "    Loads CSV files containing fake and true news articles.\n",
    "    \n",
    "    Args:\n",
    "        fake_path (str): Path to the CSV file with fake news\n",
    "        true_path (str): Path to the CSV file with true news\n",
    "        cache (bool): Whether to cache the DataFrames in memory\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (fake_df, true_df) DataFrames with loaded data\n",
    "    \"\"\"\n",
    "    print(f\"Loading CSV files from {fake_path} and {true_path}...\")\n",
    "    \n",
    "    # Load CSV files\n",
    "    fake_df = spark.read.csv(fake_path, header=True, inferSchema=True)\n",
    "    true_df = spark.read.csv(true_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Add labels (0 for fake, 1 for true)\n",
    "    fake_df = fake_df.withColumn(\"label\", lit(0))\n",
    "    true_df = true_df.withColumn(\"label\", lit(1))\n",
    "    \n",
    "    # Cache DataFrames if requested (improves performance for multiple operations)\n",
    "    if cache:\n",
    "        fake_df.cache()\n",
    "        true_df.cache()\n",
    "    \n",
    "    # Show information about the DataFrames\n",
    "    print(f\"Fake news loaded: {fake_df.count()} records\")\n",
    "    print(f\"True news loaded: {true_df.count()} records\")\n",
    "    \n",
    "    # Analyze subject distribution for data leakage detection\n",
    "    analyze_subject_distribution(fake_df, true_df)\n",
    "    \n",
    "    return fake_df, true_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341176f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_subject_distribution(fake_df, true_df):\n",
    "    \"\"\"\n",
    "    Analyzes the distribution of subjects in fake and true news datasets to detect potential data leakage.\n",
    "    \n",
    "    Args:\n",
    "        fake_df: DataFrame with fake news\n",
    "        true_df: DataFrame with true news\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing subject distribution for potential data leakage...\")\n",
    "    \n",
    "    # Check if subject column exists in both DataFrames\n",
    "    if \"subject\" in fake_df.columns and \"subject\" in true_df.columns:\n",
    "        # Get subject distribution in fake news\n",
    "        print(\"Subject distribution in fake news:\")\n",
    "        fake_subjects = fake_df.groupBy(\"subject\").count().orderBy(\"count\", ascending=False)\n",
    "        fake_subjects.show(truncate=False)\n",
    "        \n",
    "        # Get subject distribution in true news\n",
    "        print(\"Subject distribution in true news:\")\n",
    "        true_subjects = true_df.groupBy(\"subject\").count().orderBy(\"count\", ascending=False)\n",
    "        true_subjects.show(truncate=False)\n",
    "        \n",
    "        # Check for potential data leakage\n",
    "        print(\"\\nDATA LEAKAGE WARNING:\")\n",
    "        print(\"The 'subject' column may cause data leakage as it perfectly separates fake from true news.\")\n",
    "        print(\"Fake news articles are predominantly labeled with subjects like 'News', while\")\n",
    "        print(\"true news articles are labeled with subjects like 'politicsNews'.\")\n",
    "        print(\"This column should be removed before model training to prevent unrealistic performance.\")\n",
    "    else:\n",
    "        print(\"No 'subject' column found in the datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fc4319",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_hive_tables(fake_df, true_df, fake_table_name=\"fake\", true_table_name=\"real\"):\n",
    "    \"\"\"\n",
    "    Creates Hive tables for fake and true news DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        fake_df: DataFrame with fake news\n",
    "        true_df: DataFrame with true news\n",
    "        fake_table_name (str): Name of the Hive table for fake news\n",
    "        true_table_name (str): Name of the Hive table for true news\n",
    "    \"\"\"\n",
    "    print(f\"Creating Hive tables '{fake_table_name}' and '{true_table_name}'...\")\n",
    "    \n",
    "    # Create table for fake news\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {fake_table_name}\")\n",
    "    fake_df.write.mode(\"overwrite\").saveAsTable(fake_table_name)\n",
    "    print(f\"Table '{fake_table_name}' created successfully\")\n",
    "    \n",
    "    # Create table for true news\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {true_table_name}\")\n",
    "    true_df.write.mode(\"overwrite\").saveAsTable(true_table_name)\n",
    "    print(f\"Table '{true_table_name}' created successfully\")\n",
    "    \n",
    "    # Verify that tables were created correctly\n",
    "    print(\"\\nAvailable tables in catalog:\")\n",
    "    spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87e7ae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_data_from_hive(fake_table_name=\"fake\", true_table_name=\"real\", cache=True):\n",
    "    \"\"\"\n",
    "    Loads data from Hive tables.\n",
    "    \n",
    "    Args:\n",
    "        fake_table_name (str): Name of the Hive table with fake news\n",
    "        true_table_name (str): Name of the Hive table with true news\n",
    "        cache (bool): Whether to cache the DataFrames in memory\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (true_df, fake_df) DataFrames with loaded data\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from Hive tables '{true_table_name}' and '{fake_table_name}'...\")\n",
    "    \n",
    "    # Check if tables exist\n",
    "    tables = [row.tableName for row in spark.sql(\"SHOW TABLES\").collect()]\n",
    "    \n",
    "    if true_table_name not in tables or fake_table_name not in tables:\n",
    "        raise ValueError(f\"Hive tables '{true_table_name}' and/or '{fake_table_name}' do not exist\")\n",
    "    \n",
    "    # Load data from Hive tables\n",
    "    true_df = spark.table(true_table_name)\n",
    "    fake_df = spark.table(fake_table_name)\n",
    "    \n",
    "    # Cache DataFrames if requested\n",
    "    if cache:\n",
    "        true_df.cache()\n",
    "        fake_df.cache()\n",
    "    \n",
    "    # Register as temporary views for SQL queries\n",
    "    true_df.createOrReplaceTempView(\"true_news\")\n",
    "    fake_df.createOrReplaceTempView(\"fake_news\")\n",
    "    \n",
    "    # Show information about the DataFrames\n",
    "    print(f\"True news loaded: {true_df.count()} records\")\n",
    "    print(f\"Fake news loaded: {fake_df.count()} records\")\n",
    "    \n",
    "    return true_df, fake_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea58ed7",
   "metadata": {},
   "source": [
    "### Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062bdd6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def combine_datasets(true_df, fake_df, cache=True):\n",
    "    \"\"\"\n",
    "    Combines DataFrames of true and fake news.\n",
    "    \n",
    "    Args:\n",
    "        true_df: DataFrame with true news\n",
    "        fake_df: DataFrame with fake news\n",
    "        cache (bool): Whether to cache the combined DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Combined DataFrame\n",
    "    \"\"\"\n",
    "    print(\"Combining true and fake news datasets...\")\n",
    "    \n",
    "    # Check available columns\n",
    "    true_cols = set(true_df.columns)\n",
    "    fake_cols = set(fake_df.columns)\n",
    "    common_cols = true_cols.intersection(fake_cols)\n",
    "    \n",
    "    print(f\"Common columns: {common_cols}\")\n",
    "    \n",
    "    # Select common columns to ensure compatibility\n",
    "    if \"title\" in common_cols and \"text\" in common_cols:\n",
    "        # If we have title and text, combine for better context\n",
    "        true_df = true_df.select(\"title\", \"text\", \"label\")\n",
    "        fake_df = fake_df.select(\"title\", \"text\", \"label\")\n",
    "        \n",
    "        # Combine title and text for better context\n",
    "        true_df = true_df.withColumn(\"full_text\", \n",
    "                                    concat(col(\"title\"), lit(\". \"), col(\"text\")))\n",
    "        fake_df = fake_df.withColumn(\"full_text\", \n",
    "                                    concat(col(\"title\"), lit(\". \"), col(\"text\")))\n",
    "        \n",
    "        # Select final columns\n",
    "        true_df = true_df.select(\"full_text\", \"label\")\n",
    "        fake_df = fake_df.select(\"full_text\", \"label\")\n",
    "        \n",
    "        # Rename column\n",
    "        true_df = true_df.withColumnRenamed(\"full_text\", \"text\")\n",
    "        fake_df = fake_df.withColumnRenamed(\"full_text\", \"text\")\n",
    "    else:\n",
    "        # Otherwise, just use text and label\n",
    "        true_df = true_df.select(\"text\", \"label\")\n",
    "        fake_df = fake_df.select(\"text\", \"label\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_df = true_df.unionByName(fake_df)\n",
    "    \n",
    "    # Cache the combined DataFrame if requested\n",
    "    if cache:\n",
    "        combined_df.cache()\n",
    "    \n",
    "    # Show information about the combined DataFrame\n",
    "    print(f\"Combined dataset: {combined_df.count()} records\")\n",
    "    print(f\"Label distribution:\")\n",
    "    combined_df.groupBy(\"label\").count().show()\n",
    "    \n",
    "    # Unpersist individual DataFrames to free up memory\n",
    "    true_df.unpersist()\n",
    "    fake_df.unpersist()\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd637dac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_text(df, cache=True):\n",
    "    \"\"\"\n",
    "    Preprocesses text by converting to lowercase and removing special characters.\n",
    "    Also checks for and removes problematic columns that may cause data leakage.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text column\n",
    "        cache (bool): Whether to cache the preprocessed DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with preprocessed text\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing text...\")\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    df = df.withColumn(\"text\", lower(col(\"text\")))\n",
    "    \n",
    "    # Remove special characters\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), \"[^a-zA-Z0-9\\\\s]\", \" \"))\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), \"\\\\s+\", \" \"))\n",
    "    \n",
    "    # Check for problematic columns that may cause data leakage\n",
    "    if \"subject\" in df.columns:\n",
    "        print(\"\\nWARNING: Removing 'subject' column to prevent data leakage\")\n",
    "        print(\"The 'subject' column perfectly discriminates between true and fake news\")\n",
    "        print(\"True news: subject='politicsNews', Fake news: subject='News'\")\n",
    "        df = df.drop(\"subject\")\n",
    "        print(\"'subject' column successfully removed\")\n",
    "    \n",
    "    # Cache the preprocessed DataFrame if requested\n",
    "    if cache:\n",
    "        df.cache()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88af435",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_balanced_sample(df, sample_size=1000, seed=42, cache=True):\n",
    "    \"\"\"\n",
    "    Creates a balanced sample of the dataset.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with data\n",
    "        sample_size (int): Sample size for each class\n",
    "        seed (int): Seed for reproducibility\n",
    "        cache (bool): Whether to cache the sample DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Balanced sample\n",
    "    \"\"\"\n",
    "    print(f\"Creating balanced sample with {sample_size} records per class...\")\n",
    "    \n",
    "    # Sample of true news (label=1)\n",
    "    real_sample = df.filter(col(\"label\") == 1) \\\n",
    "                    .orderBy(rand(seed=seed)) \\\n",
    "                    .limit(sample_size)\n",
    "    \n",
    "    # Sample of fake news (label=0)\n",
    "    fake_sample = df.filter(col(\"label\") == 0) \\\n",
    "                    .orderBy(rand(seed=seed)) \\\n",
    "                    .limit(sample_size)\n",
    "    \n",
    "    # Combine the samples\n",
    "    sample_df = real_sample.unionByName(fake_sample)\n",
    "    \n",
    "    # Cache the sample DataFrame if requested\n",
    "    if cache:\n",
    "        sample_df.cache()\n",
    "    \n",
    "    # Register the sample DataFrame as a temporary view\n",
    "    sample_df.createOrReplaceTempView(\"sample_news\")\n",
    "    \n",
    "    # Show sample statistics\n",
    "    print(\"\\nSample statistics:\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            label, \n",
    "            COUNT(*) as count\n",
    "        FROM sample_news\n",
    "        GROUP BY label\n",
    "        ORDER BY label DESC\n",
    "    \"\"\").show()\n",
    "    \n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf76b0a",
   "metadata": {},
   "source": [
    "### Data Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce1400",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_to_parquet(df, path, partition_by=None):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame in Parquet format.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        path (str): Path where to save the DataFrame\n",
    "        partition_by (str): Column to partition by (optional)\n",
    "    \"\"\"\n",
    "    print(f\"Saving DataFrame to {path}...\")\n",
    "    \n",
    "    writer = df.write.mode(\"overwrite\")\n",
    "    \n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(partition_by)\n",
    "    \n",
    "    writer.parquet(path)\n",
    "    print(f\"DataFrame saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0321aad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_to_hive_table(df, table_name, partition_by=None):\n",
    "    \"\"\"\n",
    "    Saves a DataFrame to a Hive table.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        table_name (str): Name of the Hive table to create or replace\n",
    "        partition_by (str): Column to partition by (optional)\n",
    "    \"\"\"\n",
    "    print(f\"Saving DataFrame to Hive table {table_name}...\")\n",
    "    \n",
    "    writer = df.write.mode(\"overwrite\").format(\"parquet\")\n",
    "    \n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(partition_by)\n",
    "    \n",
    "    writer.saveAsTable(table_name)\n",
    "    print(f\"DataFrame saved to Hive table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73757099",
   "metadata": {},
   "source": [
    "### Data Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44c6c8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_dataset_characteristics(df):\n",
    "    \"\"\"\n",
    "    Analyzes dataset characteristics to identify potential issues.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text and label columns\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    print(\"Analyzing dataset characteristics...\")\n",
    "    \n",
    "    # Convert to pandas for easier analysis\n",
    "    pandas_df = df.toPandas()\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    total_samples = len(pandas_df)\n",
    "    class_distribution = pandas_df['label'].value_counts().to_dict()\n",
    "    class_balance = min(class_distribution.values()) / max(class_distribution.values())\n",
    "    \n",
    "    # Calculate text length statistics\n",
    "    pandas_df['text_length'] = pandas_df['text'].apply(len)\n",
    "    avg_text_length = pandas_df['text_length'].mean()\n",
    "    min_text_length = pandas_df['text_length'].min()\n",
    "    max_text_length = pandas_df['text_length'].max()\n",
    "    \n",
    "    # Check for empty or very short texts\n",
    "    short_texts = (pandas_df['text_length'] < 10).sum()\n",
    "    \n",
    "    # Check for duplicate texts\n",
    "    duplicate_texts = pandas_df['text'].duplicated().sum()\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'total_samples': total_samples,\n",
    "        'class_distribution': class_distribution,\n",
    "        'class_balance': class_balance,\n",
    "        'avg_text_length': avg_text_length,\n",
    "        'min_text_length': min_text_length,\n",
    "        'max_text_length': max_text_length,\n",
    "        'short_texts': short_texts,\n",
    "        'duplicate_texts': duplicate_texts\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"Dataset Characteristics:\")\n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Class distribution: {class_distribution}\")\n",
    "    print(f\"Class balance ratio: {class_balance:.2f}\")\n",
    "    print(f\"Average text length: {avg_text_length:.2f} characters\")\n",
    "    print(f\"Text length range: {min_text_length} to {max_text_length} characters\")\n",
    "    print(f\"Number of very short texts (<10 chars): {short_texts}\")\n",
    "    print(f\"Number of duplicate texts: {duplicate_texts}\")\n",
    "    \n",
    "    # Create plots\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Class distribution plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.countplot(x='label', data=pandas_df)\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xlabel('Class (0=Fake, 1=True)')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Text length distribution plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(pandas_df['text_length'], bins=30)\n",
    "    plt.title('Text Length Distribution')\n",
    "    plt.xlabel('Length (characters)')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076af2e1",
   "metadata": {},
   "source": [
    "## Complete Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca461597",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_and_save_data(fake_path=\"/FileStore/tables/fake.csv\", \n",
    "                         true_path=\"/FileStore/tables/real.csv\",\n",
    "                         output_dir=\"dbfs:/FileStore/fake_news_detection/data\",\n",
    "                         create_tables=True):\n",
    "    \"\"\"\n",
    "    Processes and saves fake and true news data.\n",
    "    \n",
    "    This complete pipeline loads CSV data, combines datasets, creates samples,\n",
    "    and saves results in Parquet format and as Hive tables.\n",
    "    \n",
    "    Args:\n",
    "        fake_path (str): Path to the CSV file with fake news\n",
    "        true_path (str): Path to the CSV file with true news\n",
    "        output_dir (str): Directory to save processed data\n",
    "        create_tables (bool): Whether to create Hive tables\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with references to processed DataFrames\n",
    "    \"\"\"\n",
    "    print(\"Starting data processing pipeline...\")\n",
    "    \n",
    "    # 0. Create directory structure\n",
    "    directories = create_directory_structure()\n",
    "    \n",
    "    # 1. Load CSV files\n",
    "    fake_df, true_df = load_csv_files(fake_path, true_path)\n",
    "    \n",
    "    # 2. Create Hive tables (optional)\n",
    "    if create_tables:\n",
    "        create_hive_tables(fake_df, true_df)\n",
    "    \n",
    "    # 3. Combine datasets\n",
    "    combined_df = combine_datasets(true_df, fake_df)\n",
    "    \n",
    "    # 4. Preprocess text\n",
    "    combined_df = preprocess_text(combined_df)\n",
    "    \n",
    "    # 5. Create balanced sample\n",
    "    sample_df = create_balanced_sample(combined_df)\n",
    "    \n",
    "    # 6. Analyze dataset characteristics\n",
    "    analyze_dataset_characteristics(combined_df)\n",
    "    \n",
    "    # 7. Save combined dataset to DBFS\n",
    "    combined_path = f\"{output_dir}/combined_data/combined_news.parquet\"\n",
    "    save_to_parquet(combined_df, combined_path, partition_by=\"label\")\n",
    "    \n",
    "    # 8. Save sample to DBFS\n",
    "    sample_path = f\"{output_dir}/sample_data/sample_news.parquet\"\n",
    "    save_to_parquet(sample_df, sample_path)\n",
    "    \n",
    "    # 9. Save to Hive tables for easier access\n",
    "    save_to_hive_table(combined_df, \"combined_news\", partition_by=\"label\")\n",
    "    save_to_hive_table(sample_df, \"sample_news\")\n",
    "    \n",
    "    # 10. Unpersist DataFrames to free up memory\n",
    "    combined_df.unpersist()\n",
    "    sample_df.unpersist()\n",
    "    \n",
    "    print(\"\\nData processing pipeline completed successfully!\")\n",
    "    \n",
    "    return {\n",
    "        \"true_df\": true_df,\n",
    "        \"fake_df\": fake_df,\n",
    "        \"combined_df\": combined_df,\n",
    "        \"sample_df\": sample_df,\n",
    "        \"directories\": directories\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf62a10f",
   "metadata": {},
   "source": [
    "## Memory Management Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eac8f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def optimize_memory_usage():\n",
    "    \"\"\"\n",
    "    Displays best practices for memory management in Databricks Community Edition.\n",
    "    \"\"\"\n",
    "    print(\"Memory Management Best Practices for Databricks Community Edition:\")\n",
    "    print(\"\\n1. Cache and Unpersist Strategy:\")\n",
    "    print(\"   - Cache DataFrames only when they will be reused multiple times\")\n",
    "    print(\"   - Always unpersist DataFrames when they are no longer needed\")\n",
    "    print(\"   - Monitor memory usage with Spark UI\")\n",
    "    \n",
    "    print(\"\\n2. Partition Management:\")\n",
    "    print(\"   - Use appropriate number of partitions (8-16 for Community Edition)\")\n",
    "    print(\"   - Repartition large DataFrames to avoid memory issues\")\n",
    "    print(\"   - Use coalesce() for reducing partitions without shuffle\")\n",
    "    \n",
    "    print(\"\\n3. Column Pruning:\")\n",
    "    print(\"   - Select only necessary columns as early as possible\")\n",
    "    print(\"   - Drop unnecessary columns to reduce memory footprint\")\n",
    "    \n",
    "    print(\"\\n4. Checkpointing:\")\n",
    "    print(\"   - Use checkpointing for complex operations to truncate lineage\")\n",
    "    print(\"   - Set checkpoint directory with spark.sparkContext.setCheckpointDir()\")\n",
    "    \n",
    "    print(\"\\n5. Broadcast Variables:\")\n",
    "    print(\"   - Use broadcast variables for small lookup tables\")\n",
    "    print(\"   - Example: broadcast(small_df).value for joins\")\n",
    "    \n",
    "    print(\"\\n6. Garbage Collection:\")\n",
    "    print(\"   - Monitor GC with spark.conf.get('spark.executor.extraJavaOptions')\")\n",
    "    print(\"   - Consider adding -XX:+PrintGCDetails to Java options\")\n",
    "    \n",
    "    print(\"\\nImplemented in this notebook:\")\n",
    "    print(\"- Strategic caching of DataFrames\")\n",
    "    print(\"- Explicit unpersist calls when DataFrames are no longer needed\")\n",
    "    print(\"- Early column selection to reduce memory footprint\")\n",
    "    print(\"- Appropriate partition management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a2f1ee",
   "metadata": {},
   "source": [
    "## Step-by-Step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd10259",
   "metadata": {},
   "source": [
    "### 1. Set Up Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a821a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "directories = create_directory_structure()\n",
    "print(f\"Working with directories: {directories}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9274f",
   "metadata": {},
   "source": [
    "### 2. Load CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfba476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to CSV files\n",
    "# Note: Adjust paths as needed for your environment\n",
    "fake_path = \"/FileStore/tables/fake.csv\"\n",
    "true_path = \"/FileStore/tables/real.csv\"\n",
    "\n",
    "# Load the CSV files\n",
    "fake_df, true_df = load_csv_files(fake_path, true_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322eaacf",
   "metadata": {},
   "source": [
    "### 3. Create Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad43dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hive tables\n",
    "create_hive_tables(fake_df, true_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb79367",
   "metadata": {},
   "source": [
    "### 4. Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35224e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine true and fake news datasets\n",
    "combined_df = combine_datasets(true_df, fake_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae3525",
   "metadata": {},
   "source": [
    "### 5. Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ebff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text data\n",
    "combined_df = preprocess_text(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57e671",
   "metadata": {},
   "source": [
    "### 6. Create Balanced Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf6f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a balanced sample\n",
    "sample_df = create_balanced_sample(combined_df, sample_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b4542",
   "metadata": {},
   "source": [
    "### 7. Analyze Dataset Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73759475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset characteristics\n",
    "analysis_results = analyze_dataset_characteristics(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5e07e4",
   "metadata": {},
   "source": [
    "### 8. Save Data to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a08eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined dataset to Parquet\n",
    "save_to_parquet(combined_df, f\"{directories['processed_data']}/combined_news.parquet\", partition_by=\"label\")\n",
    "\n",
    "# Save sample to Parquet\n",
    "save_to_parquet(sample_df, f\"{directories['sample_data']}/sample_news.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088788a0",
   "metadata": {},
   "source": [
    "### 9. Save to Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Hive tables\n",
    "save_to_hive_table(combined_df, \"combined_news\", partition_by=\"label\")\n",
    "save_to_hive_table(sample_df, \"sample_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6534e70",
   "metadata": {},
   "source": [
    "### 10. Clean Up Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f16ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersist DataFrames to free up memory\n",
    "combined_df.unpersist()\n",
    "sample_df.unpersist()\n",
    "fake_df.unpersist()\n",
    "true_df.unpersist()\n",
    "\n",
    "# Display memory management best practices\n",
    "optimize_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6cef8e",
   "metadata": {},
   "source": [
    "## Run Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54820e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete data ingestion pipeline\n",
    "results = process_and_save_data(\n",
    "    fake_path=\"/FileStore/tables/fake.csv\",\n",
    "    true_path=\"/FileStore/tables/real.csv\",\n",
    "    output_dir=\"dbfs:/FileStore/fake_news_detection/data\",\n",
    "    create_tables=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c98c0d",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "1. **Directory Structure**: The `create_directory_structure()` function must be called before running the pipeline to ensure all necessary directories exist.\n",
    "\n",
    "2. **Data Leakage**: The 'subject' column in the original datasets perfectly separates fake from true news, which would cause data leakage. This column is automatically removed during preprocessing.\n",
    "\n",
    "3. **Memory Management**: Databricks Community Edition has limited memory. The functions in this notebook implement best practices for memory management:\n",
    "   - Strategic caching of DataFrames\n",
    "   - Explicit unpersist calls when DataFrames are no longer needed\n",
    "   - Early column selection to reduce memory footprint\n",
    "\n",
    "4. **File Paths**: The default paths assume files are uploaded to Databricks FileStore. Adjust paths as needed for your environment.\n",
    "\n",
    "5. **Hive Tables**: Creating Hive tables is optional but recommended for easier data access in subsequent notebooks.\n",
    "\n",
    "6. **Balanced Sample**: A balanced sample is created for exploratory analysis and initial model development. The full dataset should be used for final model training.\n",
    "\n",
    "7. **Partitioning**: Data is partitioned by label when saved to improve query performance when filtering by class.\n",
    "\n",
    "8. **Reproducibility**: A fixed seed is used for sampling to ensure reproducible results."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
