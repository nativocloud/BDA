{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae5ca776",
   "metadata": {},
   "source": [
    "# Data Ingestion for Fake News Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f3e2d9",
   "metadata": {},
   "source": [
    "## Why Data Ingestion is Important\n",
    "\n",
    "In fake news detection, proper data ingestion ensures:\n",
    "1. Data quality and consistency\n",
    "2. Appropriate labeling of real and fake news articles\n",
    "3. Balanced representation of both classes\n",
    "4. Efficient storage for distributed processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2097a2",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's set up our Spark environment and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa31afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, lower, regexp_replace, rand, when, concat\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73253fb9",
   "metadata": {},
   "source": [
    "## Creating a Spark Session with Hive Support\n",
    "\n",
    "We'll use Apache Spark for distributed data processing, with Hive support enabled to access the metastore tables. Let's create a properly configured Spark session optimized for the Databricks Community Edition limitations (1 driver, 15.3 GB Memory, 2 Cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c114cf26",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create a Spark session with configuration optimized for Databricks Community Edition\n",
    "# - appName: Identifies this application in the Spark UI and logs\n",
    "# - spark.sql.shuffle.partitions: Set to 8 (4x number of cores) for Community Edition\n",
    "# - spark.driver.memory: Set to 8g to utilize available memory while leaving room for system\n",
    "# - enableHiveSupport: Enables access to Hive metastore tables\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd1a6e",
   "metadata": {},
   "source": [
    "## Create directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b869d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_directory_structure(base_dir=\"/dbfs/FileStore/fake_news_detection\"):\n",
    "    \"\"\"\n",
    "    Creates the necessary directory structure for the fake news detection project.\n",
    "    \n",
    "    This function ensures all required directories exist in the Databricks environment.\n",
    "    It's essential to run this function before executing the rest of the pipeline.\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): Base directory for the project\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with paths to all created directories\n",
    "    \"\"\"\n",
    "    print(f\"Creating directory structure in {base_dir}...\")\n",
    "    \n",
    "    # Define directory paths\n",
    "    directories = {\n",
    "        \"data\": f\"{base_dir}/data\",\n",
    "        \"raw_data\": f\"{base_dir}/data/raw\",\n",
    "        \"processed_data\": f\"{base_dir}/data/processed\",\n",
    "        \"sample_data\": f\"{base_dir}/data/sample\",\n",
    "        \"models\": f\"{base_dir}/models\",\n",
    "        \"logs\": f\"{base_dir}/logs\",\n",
    "        \"visualizations\": f\"{base_dir}/visualizations\",\n",
    "        \"temp\": f\"{base_dir}/temp\"\n",
    "    }\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_name, dir_path in directories.items():\n",
    "        # Use dbutils in Databricks environment\n",
    "        try:\n",
    "            dbutils.fs.mkdirs(dir_path)\n",
    "            print(f\"Created directory: {dir_path}\")\n",
    "        except NameError:\n",
    "            # Fallback for non-Databricks environments\n",
    "            os.makedirs(dir_path.replace(\"/dbfs\", \"\"), exist_ok=True)\n",
    "            print(f\"Created directory: {dir_path} (local mode)\")\n",
    "    \n",
    "    print(\"Directory structure created successfully\")\n",
    "    return directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66559fd1",
   "metadata": {},
   "source": [
    "## Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae9a7ab",
   "metadata": {},
   "source": [
    "### Data Loading functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0dfab3",
   "metadata": {},
   "source": [
    "#### Load CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a832554f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_csv_files(fake_path, true_path, cache=True):\n",
    "    \"\"\"\n",
    "    Loads CSV files containing fake and true news articles.\n",
    "    \n",
    "    Args:\n",
    "        fake_path (str): Path to the CSV file with fake news\n",
    "        true_path (str): Path to the CSV file with true news\n",
    "        cache (bool): Whether to cache the DataFrames in memory\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (fake_df, true_df) DataFrames with loaded data\n",
    "    \"\"\"\n",
    "    print(f\"Loading CSV files from {fake_path} and {true_path}...\")\n",
    "    \n",
    "    # Load CSV files\n",
    "    fake_df = spark.read.csv(fake_path, header=True, inferSchema=True)\n",
    "    true_df = spark.read.csv(true_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Add labels (0 for fake, 1 for true)\n",
    "    fake_df = fake_df.withColumn(\"label\", lit(0))\n",
    "    true_df = true_df.withColumn(\"label\", lit(1))\n",
    "    \n",
    "    # Cache DataFrames if requested (improves performance for multiple operations)\n",
    "    if cache:\n",
    "        fake_df.cache()\n",
    "        true_df.cache()\n",
    "        # Force materialization\n",
    "        fake_count = fake_df.count()\n",
    "        true_count = true_df.count()\n",
    "    \n",
    "    # Show information about the DataFrames\n",
    "    print(f\"Fake news loaded: {fake_df.count()} records\")\n",
    "    print(f\"True news loaded: {true_df.count()} records\")\n",
    "    \n",
    "    return fake_df, true_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3479b16",
   "metadata": {},
   "source": [
    "#### Analyse subject distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c5ccb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, lit\n",
    "# Alias PySpark's min/max to avoid conflict with Python's built-in min/max\n",
    "from pyspark.sql.functions import min as spark_min, max as spark_max\n",
    "from pyspark.sql.types import StringType # For type checking\n",
    "import builtins  # For Python's built-in min/max functions\n",
    "\n",
    "# spark (SparkSession) is assumed to be globally available in Databricks\n",
    "\n",
    "def analyze_subject_distribution(fake_df, true_df):\n",
    "    \"\"\"\n",
    "    Analyzes the distribution of subjects in fake and true news datasets to detect potential data leakage.\n",
    "    Provides essential checks using native PySpark functionality.\n",
    "    Optimized for Databricks environment with display() visualizations by minimizing collect() calls.\n",
    "\n",
    "    Args:\n",
    "        fake_df: DataFrame with fake news.\n",
    "        true_df: DataFrame with true news.\n",
    "\n",
    "    Returns:\n",
    "        None (This function prints analysis directly to the Databricks notebook output).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä SUBJECT DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # --- Initial Checks for Robustness ---\n",
    "    # Calculate total counts upfront (this is a necessary action)\n",
    "    fake_total = fake_df.count()\n",
    "    true_total = true_df.count()\n",
    "\n",
    "    if fake_total == 0:\n",
    "        print(\"\\n‚ö†Ô∏è Fake news DataFrame is empty. Analysis cannot proceed.\")\n",
    "        return\n",
    "    if true_total == 0:\n",
    "        print(\"\\n‚ö†Ô∏è True news DataFrame is empty. Analysis cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    # Check if 'subject' column exists in both DataFrames\n",
    "    if \"subject\" not in fake_df.columns or \"subject\" not in true_df.columns:\n",
    "        print(\"\\n‚ö†Ô∏è 'subject' column not found in one or both datasets. Analysis cannot proceed.\")\n",
    "        print(f\"  Fake DF columns: {fake_df.columns}\")\n",
    "        print(f\"  True DF columns: {true_df.columns}\")\n",
    "        return\n",
    "    \n",
    "    # Check if 'subject' column is of a string type for proper analysis\n",
    "    fake_subject_type = fake_df.schema[\"subject\"].dataType\n",
    "    true_subject_type = true_df.schema[\"subject\"].dataType\n",
    "    if not isinstance(fake_subject_type, StringType) or not isinstance(true_subject_type, StringType):\n",
    "        print(f\"\\n‚ö†Ô∏è 'subject' column expected to be 'string' type for distribution analysis, but found '{fake_subject_type.typeName()}' in fake_df and '{true_subject_type.typeName()}' in true_df. Analysis cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    # --- Step 1 & 2: Get and Display Subject Distributions ---\n",
    "   \n",
    "    print(\"\\n1Ô∏è‚É£ FAKE NEWS SUBJECT DISTRIBUTION\")\n",
    "    fake_subjects_df = fake_df.groupBy(\"subject\").count().orderBy(col(\"count\").desc())\n",
    "    print(\"‚Ä¢ Subject distribution in fake news:\")\n",
    "    display(fake_subjects_df)\n",
    "\n",
    "    print(\"\\n2Ô∏è‚É£ TRUE NEWS SUBJECT DISTRIBUTION\")\n",
    "    true_subjects_df = true_df.groupBy(\"subject\").count().orderBy(col(\"count\").desc())\n",
    "    print(\"‚Ä¢ Subject distribution in true news:\")\n",
    "    display(true_subjects_df)\n",
    "\n",
    "    # --- Step 3: Subject Overlap Analysis ---\n",
    "    print(\"\\n3Ô∏è‚É£ SUBJECT OVERLAP ANALYSIS\")\n",
    "\n",
    "    # Get total unique subjects in each dataset directly in Spark\n",
    "    num_fake_unique_subjects = fake_subjects_df.count()\n",
    "    num_true_unique_subjects = true_subjects_df.count()\n",
    "\n",
    "    # Find common subjects \n",
    "    common_subjects_df = fake_subjects_df.join(true_subjects_df, on=\"subject\", how=\"inner\")\n",
    "    num_common_subjects = common_subjects_df.count()\n",
    "\n",
    "    # Find subjects exclusive to fake news using left_anti join\n",
    "    fake_exclusive_df = fake_subjects_df.join(true_subjects_df, on=\"subject\", how=\"left_anti\")\n",
    "    num_fake_exclusive = fake_exclusive_df.count()\n",
    "\n",
    "    # Find subjects exclusive to true news using right_anti join (or left_anti with roles swapped)\n",
    "    true_exclusive_df = true_subjects_df.join(fake_subjects_df, on=\"subject\", how=\"left_anti\") \n",
    "    # returns only the rows from the left DataFrame that have no match in the right DataFrame.\n",
    "    num_true_exclusive = true_exclusive_df.count()\n",
    "\n",
    "    print(f\"‚Ä¢ Total unique subjects in fake news: {num_fake_unique_subjects}\")\n",
    "    print(f\"‚Ä¢ Total unique subjects in true news: {num_true_unique_subjects}\")\n",
    "    print(f\"‚Ä¢ Subjects common to both datasets: {num_common_subjects}\")\n",
    "    print(f\"‚Ä¢ Subjects exclusive to fake news: {num_fake_exclusive}\")\n",
    "    print(f\"‚Ä¢ Subjects exclusive to true news: {num_true_exclusive}\")\n",
    "\n",
    "    # Create a comparison view for common subjects\n",
    "    if num_common_subjects > 0:\n",
    "        print(\"\\n‚Ä¢ Distribution of common subjects (count and percentage):\")\n",
    "\n",
    "        # Create temporary views for SQL query \n",
    "        # Note: Using distinct temp view names to avoid conflicts if the notebook runs multiple times\n",
    "        fake_df.createOrReplaceTempView(\"fake_news_temp_view_subject_analysis\")\n",
    "        true_df.createOrReplaceTempView(\"true_news_temp_view_subject_analysis\")\n",
    "\n",
    "        # SQL query to compare subject distributions and their percentages\n",
    "        comparison_query = f\"\"\"\n",
    "        SELECT\n",
    "            f.subject,\n",
    "            f.count AS fake_count,\n",
    "            t.count AS true_count,\n",
    "            CAST(f.count AS DOUBLE) / {fake_total} * 100 AS fake_percentage,\n",
    "            CAST(t.count AS DOUBLE) / {true_total} * 100 AS true_percentage\n",
    "        FROM\n",
    "            (SELECT subject, COUNT(*) AS count FROM fake_news_temp_view_subject_analysis GROUP BY subject) f\n",
    "        JOIN\n",
    "            (SELECT subject, COUNT(*) AS count FROM true_news_temp_view_subject_analysis GROUP BY subject) t\n",
    "        ON\n",
    "            f.subject = t.subject\n",
    "        ORDER BY\n",
    "            ABS((CAST(f.count AS DOUBLE) / {fake_total} * 100) - (CAST(t.count AS DOUBLE) / {true_total} * 100)) DESC\n",
    "        \"\"\"\n",
    "\n",
    "        comparison_df = spark.sql(comparison_query)\n",
    "        display(comparison_df)\n",
    "    else:\n",
    "        print(\"\\n‚Ä¢ No common subjects found between fake and true news datasets, skipping detailed comparison table.\")\n",
    "\n",
    "    # --- Step 4: Data Leakage Assessment ---\n",
    "    print(\"\\n4Ô∏è‚É£ DATA LEAKAGE ASSESSMENT\")\n",
    "    \n",
    "    # Check if there's a perfect separation by subject\n",
    "    if num_common_subjects == 0 and num_fake_unique_subjects > 0 and num_true_unique_subjects > 0:\n",
    "        print(\"\\nüö® HIGH RISK OF DATA LEAKAGE DETECTED!\")\n",
    "        print(\"‚Ä¢ The 'subject' column perfectly separates fake and true news articles.\")\n",
    "        print(\"‚Ä¢ This is a clear case of data leakage that would artificially inflate model performance.\")\n",
    "        print(\"‚Ä¢ RECOMMENDATION: Remove the 'subject' column before model training.\")\n",
    "    elif num_common_subjects > 0:\n",
    "        # Get the most biased subjects (those that appear predominantly in one class)\n",
    "        if comparison_df.count() > 0:\n",
    "            # Calculate the absolute difference between fake and true percentages\n",
    "            comparison_df = comparison_df.withColumn(\n",
    "                \"percentage_difference\", \n",
    "                abs(col(\"fake_percentage\") - col(\"true_percentage\"))\n",
    "            )\n",
    "            \n",
    "            # Find subjects with high bias (>80% difference)\n",
    "            highly_biased = comparison_df.filter(col(\"percentage_difference\") > 80).count()\n",
    "            \n",
    "            if highly_biased > 0:\n",
    "                print(\"\\nüî∂ MODERATE RISK OF DATA LEAKAGE DETECTED!\")\n",
    "                print(f\"‚Ä¢ Found {highly_biased} subjects with >80% difference in distribution between classes.\")\n",
    "                print(\"‚Ä¢ These subjects may create partial data leakage.\")\n",
    "                print(\"‚Ä¢ RECOMMENDATION: Consider removing the 'subject' column or perform careful cross-validation.\")\n",
    "            else:\n",
    "                print(\"\\n‚úÖ LOW RISK OF DATA LEAKAGE\")\n",
    "                print(\"‚Ä¢ Subject distributions do not show strong bias toward either class.\")\n",
    "                print(\"‚Ä¢ The 'subject' column may be used as a feature with caution.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc36f51",
   "metadata": {},
   "source": [
    "#### Analyze dataset characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fdc556",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_dataset_characteristics(df):\n",
    "    \"\"\"\n",
    "    Very basic dataset analysis function with minimal Spark operations.\n",
    "    Only performs essential checks to avoid overwhelming the cluster.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with basic analysis results\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import col, count, when, length\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä BASIC DATASET ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get column names\n",
    "    columns = df.columns\n",
    "    print(f\"‚Ä¢ Columns: {', '.join(columns)}\")\n",
    "    \n",
    "    # Get total count (single Spark action)\n",
    "    total_count = df.count()\n",
    "    print(f\"‚Ä¢ Total records: {total_count}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    has_text = \"text\" in columns\n",
    "    has_label = \"label\" in columns\n",
    "    \n",
    "    # Basic class distribution if label exists\n",
    "    if has_label:\n",
    "        print(\"\\n‚Ä¢ Class distribution:\")\n",
    "        # Use a single SQL query instead of multiple DataFrame operations\n",
    "        df.createOrReplaceTempView(\"temp_data\")\n",
    "        class_dist = spark.sql(\"\"\"\n",
    "            SELECT label, COUNT(*) as count\n",
    "            FROM temp_data\n",
    "            GROUP BY label\n",
    "            ORDER BY label\n",
    "        \"\"\")\n",
    "        display(class_dist)\n",
    "    \n",
    "    # Check for null values in important columns\n",
    "    print(\"\\n‚Ä¢ Null value check:\")\n",
    "    null_counts = {}\n",
    "    \n",
    "    # Only check a few important columns to minimize operations\n",
    "    columns_to_check = []\n",
    "    if has_text:\n",
    "        columns_to_check.append(\"text\")\n",
    "    if has_label:\n",
    "        columns_to_check.append(\"label\")\n",
    "    if \"location\" in columns:\n",
    "        columns_to_check.append(\"location\")\n",
    "    if \"news_source\" in columns:\n",
    "        columns_to_check.append(\"news_source\")\n",
    "    \n",
    "    for column_name in columns_to_check:\n",
    "        null_count = df.filter(col(column_name).isNull()).count()\n",
    "        null_counts[column_name] = null_count\n",
    "        print(f\"  - Null values in '{column_name}': {null_count}\")\n",
    "    \n",
    "    # Check for duplicates in text column if it exists\n",
    "    if has_text:\n",
    "        print(\"\\n‚Ä¢ Duplicate check:\")\n",
    "        unique_count = df.select(\"text\").distinct().count()\n",
    "        duplicate_count = total_count - unique_count\n",
    "        print(f\"  - Duplicate texts: {duplicate_count}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Return minimal results\n",
    "    return {\n",
    "        \"total_count\": total_count,\n",
    "        \"columns\": columns,\n",
    "        \"null_counts\": null_counts\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da0db6",
   "metadata": {},
   "source": [
    "#### Check random records with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2173b739",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_random_records(df, num_records=10):\n",
    "    \"\"\"\n",
    "    Uses SQL to select random records from a DataFrame for inspection.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to sample\n",
    "        num_records: Number of random records to return\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Sample of random records\n",
    "    \"\"\"\n",
    "    print(f\"Selecting {num_records} random records for inspection...\")\n",
    "    \n",
    "    # Create a temporary view\n",
    "    df.createOrReplaceTempView(\"temp_data\")\n",
    "    \n",
    "    # Use SQL to select random records\n",
    "    query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM temp_data\n",
    "    ORDER BY rand()\n",
    "    LIMIT {num_records}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query\n",
    "    result_df = spark.sql(query)\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"\\nRandom sample of records:\")\n",
    "    display(result_df)\n",
    "    \n",
    "    # Show schema information\n",
    "    print(\"\\nSchema information:\")\n",
    "    result_df.printSchema()\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe6d5d4",
   "metadata": {},
   "source": [
    "#### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a68bd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_text(df, cache=True):\n",
    "    \"\"\"\n",
    "    Preprocesses text by converting to lowercase, normalizing acronyms, and extracting metadata.\n",
    "    Also checks for and removes problematic columns that may cause data leakage.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text column\n",
    "        cache (bool): Whether to cache the preprocessed DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with preprocessed text\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing text...\")\n",
    "    \n",
    "    # Define acronym replacements\n",
    "    acronyms = {\n",
    "        \"U.S.\": \"US\",\n",
    "        \"U.S.A.\": \"USA\",\n",
    "        \"U.K.\": \"UK\",\n",
    "        \"U.N.\": \"UN\",\n",
    "        \"F.B.I.\": \"FBI\",\n",
    "        \"C.I.A.\": \"CIA\",\n",
    "        \"D.C.\": \"DC\",\n",
    "        \"E.U.\": \"EU\",\n",
    "        \"N.A.T.O.\": \"NATO\",\n",
    "        \"W.H.O.\": \"WHO\"\n",
    "    }\n",
    "    \n",
    "    # Function to replace acronyms\n",
    "    def replace_acronyms(text):\n",
    "        if text is None:\n",
    "            return None\n",
    "        \n",
    "        result = text\n",
    "        for acronym, replacement in acronyms.items():\n",
    "            # Use word boundaries to ensure we're replacing complete acronyms\n",
    "            result = result.replace(acronym, replacement)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Register the UDF\n",
    "    from pyspark.sql.functions import udf\n",
    "    replace_acronyms_udf = udf(replace_acronyms, StringType())\n",
    "    \n",
    "    # Apply acronym normalization first\n",
    "    df = df.withColumn(\"text\", replace_acronyms_udf(col(\"text\")))\n",
    "    \n",
    "    # Extract location information if present in text (e.g., \"WASHINGTON ‚Äî\" or \"NEW YORK (Reuters) -\")\n",
    "    df = df.withColumn(\n",
    "        \"location\",\n",
    "        when(\n",
    "            regexp_replace(col(\"text\"), \"[^a-zA-Z0-9\\\\s]\", \" \").rlike(\"^[A-Z]{2,}\\\\s+‚Äî\"),\n",
    "            regexp_replace(regexp_replace(col(\"text\"), \"^([A-Z]{2,})\\\\s+‚Äî.*\", \"$1\"), \"\\\\s+\", \"\")\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    \n",
    "    # Extract news source information if present (e.g., \"(Reuters)\" or \"(AP)\")\n",
    "    df = df.withColumn(\n",
    "        \"news_source\",\n",
    "        when(\n",
    "            col(\"text\").rlike(\"\\\\([A-Za-z]+\\\\)\"),\n",
    "            regexp_replace(regexp_replace(col(\"text\"), \".*\\\\(([A-Za-z]+)\\\\).*\", \"$1\"), \"\\\\s+\", \"\")\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    df = df.withColumn(\"text\", lower(col(\"text\")))\n",
    "    \n",
    "    # Remove special characters but preserve hashtags and mentions\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), \"[^a-zA-Z0-9\\\\s#@]\", \" \"))\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), \"\\\\s+\", \" \"))\n",
    "    \n",
    "    # Check for problematic columns that may cause data leakage\n",
    "    if \"subject\" in df.columns:\n",
    "        print(\"\\nWARNING: Removing 'subject' column to prevent data leakage\")\n",
    "        print(\"The 'subject' column perfectly discriminates between true and fake news\")\n",
    "        print(\"True news: subject='politicsNews', Fake news: subject='News'\")\n",
    "        df = df.drop(\"subject\")\n",
    "        print(\"'subject' column successfully removed\")\n",
    "    \n",
    "    # Cache the preprocessed DataFrame if requested\n",
    "    if cache:\n",
    "        df.cache()\n",
    "        # Force materialization\n",
    "        df.count()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1373b4",
   "metadata": {},
   "source": [
    "#### Save to Hive tables safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd8ba49",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_to_hive_table_safely(df, table_name, partition_by=None, mode=\"overwrite\"):\n",
    "    \"\"\"\n",
    "    Safely saves a DataFrame to a Hive table, handling existing tables and locations.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        table_name: Name of the Hive table\n",
    "        partition_by: Column(s) to partition by (optional)\n",
    "        mode: Write mode (default: \"overwrite\")\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if successful\n",
    "    \"\"\"\n",
    "    print(f\"Safely saving DataFrame to Hive table: {table_name}\")\n",
    "    \n",
    "    # Check if table exists\n",
    "    tables = spark.sql(\"SHOW TABLES\").select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "    table_exists = table_name in tables\n",
    "    \n",
    "    if table_exists:\n",
    "        print(f\"Table '{table_name}' already exists. Dropping it...\")\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "        print(f\"Table '{table_name}' dropped successfully.\")\n",
    "    \n",
    "    # Check if the location exists and remove it if necessary\n",
    "    # This is needed because dropping the table might not always remove the underlying data\n",
    "    try:\n",
    "        location_path = f\"dbfs:/user/hive/warehouse/{table_name}\"\n",
    "        print(f\"Checking if location exists: {location_path}\")\n",
    "        \n",
    "        # Use dbutils to check if path exists and delete it if it does\n",
    "        if dbutils.fs.ls(location_path):\n",
    "            print(f\"Location exists. Removing directory: {location_path}\")\n",
    "            dbutils.fs.rm(location_path, recurse=True)\n",
    "            print(f\"Directory removed successfully.\")\n",
    "    except Exception as e:\n",
    "        # Path might not exist, which is fine\n",
    "        print(f\"Note: {str(e)}\")\n",
    "    \n",
    "    # Save the DataFrame to the Hive table\n",
    "    print(f\"Saving DataFrame to table '{table_name}'...\")\n",
    "    \n",
    "    if partition_by:\n",
    "        df.write.format(\"parquet\").partitionBy(partition_by).mode(mode).saveAsTable(table_name)\n",
    "        print(f\"DataFrame saved to table '{table_name}' with partitioning on '{partition_by}'.\")\n",
    "    else:\n",
    "        df.write.format(\"parquet\").mode(mode).saveAsTable(table_name)\n",
    "        print(f\"DataFrame saved to table '{table_name}'.\")\n",
    "    \n",
    "    # Verify the table was created\n",
    "    tables = spark.sql(\"SHOW TABLES\").select(\"tableName\").rdd.flatMap(lambda x: x).collect()\n",
    "    if table_name in tables:\n",
    "        print(f\"Verified: Table '{table_name}' exists.\")\n",
    "        \n",
    "        # Show table information\n",
    "        print(\"\\nTable information:\")\n",
    "        spark.sql(f\"DESCRIBE TABLE {table_name}\").show(truncate=False)\n",
    "        \n",
    "        # Show record count\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0]['count']\n",
    "        print(f\"\\nRecord count: {count:,}\")\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Error: Failed to create table '{table_name}'.\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c3d239",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e86fa",
   "metadata": {},
   "source": [
    "### Step 1: Create Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory structure\n",
    "directories = create_directory_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2ae7c0",
   "metadata": {},
   "source": [
    "### Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the CSV files\n",
    "fake_news_path = \"/dbfs/FileStore/fake_news_detection/data/raw/Fake.csv\"\n",
    "true_news_path = \"/dbfs/FileStore/fake_news_detection/data/raw/True.csv\"\n",
    "\n",
    "# Load the CSV files\n",
    "fake_df, true_df = load_csv_files(fake_news_path, true_news_path, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b19ef0",
   "metadata": {},
   "source": [
    "### Step 3: Analyze Subject Distribution (Data Leakage Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5985bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of subjects to detect potential data leakage\n",
    "analyze_subject_distribution(fake_df, true_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c698dc",
   "metadata": {},
   "source": [
    "### Step 4: Combine and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afdc34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine fake and true news DataFrames\n",
    "combined_df = fake_df.union(true_df)\n",
    "\n",
    "# Analyze the combined dataset\n",
    "analyze_dataset_characteristics(combined_df)\n",
    "\n",
    "# Check a random sample of records\n",
    "check_random_records(combined_df, num_records=5)\n",
    "\n",
    "# Preprocess the text\n",
    "combined_df = preprocess_text(combined_df, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996735ff",
   "metadata": {},
   "source": [
    "### Step 5: Analyze Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070887ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the preprocessed dataset\n",
    "analyze_dataset_characteristics(combined_df)\n",
    "\n",
    "# Check a random sample of preprocessed records\n",
    "check_random_records(combined_df, num_records=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675390c",
   "metadata": {},
   "source": [
    "### Step 6: Save to Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03549d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined dataset to a Hive table\n",
    "save_to_hive_table_safely(combined_df, \"combined_news\", partition_by=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc006a",
   "metadata": {},
   "source": [
    "## Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e966a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersist DataFrames to free up memory\n",
    "if fake_df._jdf.storageLevel().useMemory():\n",
    "    fake_df.unpersist()\n",
    "    print(\"Unpersisted fake_df from memory\")\n",
    "\n",
    "if true_df._jdf.storageLevel().useMemory():\n",
    "    true_df.unpersist()\n",
    "    print(\"Unpersisted true_df from memory\")\n",
    "\n",
    "if combined_df._jdf.storageLevel().useMemory():\n",
    "    combined_df.unpersist()\n",
    "    print(\"Unpersisted combined_df from memory\")\n",
    "\n",
    "print(\"Memory management completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a9ca2b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Created a directory structure for the fake news detection project\n",
    "2. Loaded fake and true news datasets from CSV files\n",
    "3. Analyzed the subject distribution to detect potential data leakage\n",
    "4. Combined and preprocessed the data, including:\n",
    "   - Acronym normalization\n",
    "   - Location and news source extraction\n",
    "   - Text cleaning and normalization\n",
    "   - Removal of the 'subject' column to prevent data leakage\n",
    "5. Saved the preprocessed data to a Hive table for use in subsequent phases\n",
    "6. Implemented proper memory management by unpersisting DataFrames when no longer needed\n",
    "\n",
    "The data is now ready for the next phase: feature engineering."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
