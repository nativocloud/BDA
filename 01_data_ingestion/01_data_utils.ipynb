{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8ef16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data utility functions for fake news detection project.\n",
    "This module contains functions for loading, preprocessing, and transforming data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a04cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, lit\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0417208",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def initialize_spark():\n",
    "    \"\"\"\n",
    "    Initialize a Spark session for data processing.\n",
    "    \n",
    "    Returns:\n",
    "        SparkSession: Initialized Spark session\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"FakeNewsDetection\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "        .config(\"spark.default.parallelism\", \"8\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64214a85",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_data(spark, fake_path, true_path):\n",
    "    \"\"\"\n",
    "    Load and combine fake and true news datasets.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): Spark session\n",
    "        fake_path (str): Path to fake news CSV file\n",
    "        true_path (str): Path to true news CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Combined dataset with labels\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    df_fake = spark.read.csv(fake_path, header=True, inferSchema=True)\n",
    "    df_real = spark.read.csv(true_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Add labels (0 for fake, 1 for real)\n",
    "    df_fake = df_fake.withColumn(\"label\", lit(0))\n",
    "    df_real = df_real.withColumn(\"label\", lit(1))\n",
    "    \n",
    "    # Combine datasets\n",
    "    df = df_fake.unionByName(df_real).select(\"text\", \"label\").na.drop()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ebe8e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    \"\"\"\n",
    "    Preprocess text data by converting to lowercase and removing special characters.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with text column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with preprocessed text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and remove special characters\n",
    "    df = df.withColumn(\"text\", lower(regexp_replace(col(\"text\"), \"[^a-zA-Z\\\\s]\", \"\")))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ae89ad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_feature_pipeline(input_col=\"text\", output_col=\"features\", hash_size=10000):\n",
    "    \"\"\"\n",
    "    Create a feature extraction pipeline for text data.\n",
    "    \n",
    "    Args:\n",
    "        input_col (str): Input column name\n",
    "        output_col (str): Output column name\n",
    "        hash_size (int): Size of the feature vectors\n",
    "        \n",
    "    Returns:\n",
    "        list: List of pipeline stages\n",
    "    \"\"\"\n",
    "    # Create pipeline stages\n",
    "    tokenizer = Tokenizer(inputCol=input_col, outputCol=\"words\")\n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "    hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=hash_size)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=output_col)\n",
    "    \n",
    "    return [tokenizer, remover, hashingTF, idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae00368",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def split_data(df, train_ratio=0.8, test_ratio=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Split data into training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        train_ratio (float): Ratio of training data\n",
    "        test_ratio (float): Ratio of testing data\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_data, test_data)\n",
    "    \"\"\"\n",
    "    # Split data\n",
    "    train_data, test_data = df.randomSplit([train_ratio, test_ratio], seed=seed)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8640cc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    \"\"\"\n",
    "    Save a trained model to disk.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        path (str): Path to save the model\n",
    "    \"\"\"\n",
    "    model.write().overwrite().save(path)\n",
    "    print(f\"Model saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43efff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(spark, path):\n",
    "    \"\"\"\n",
    "    Load a trained model from disk.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): Spark session\n",
    "        path (str): Path to the saved model\n",
    "        \n",
    "    Returns:\n",
    "        Model: Loaded model\n",
    "    \"\"\"\n",
    "    from pyspark.ml.pipeline import PipelineModel\n",
    "    \n",
    "    model = PipelineModel.load(path)\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Last modified: May 29, 2025
