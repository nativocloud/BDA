{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset preparation and augmentation utility functions for fake news detection project.\n",
    "This module contains functions for preparing and supplementing datasets while preventing data leakage.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, regexp_replace, lower, udf, concat\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca18656",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ba86c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_and_clean_datasets(spark, fake_path, true_path):\n",
    "    \"\"\"\n",
    "    Load and clean fake and true news datasets.\n",
    "    \n",
    "    Args:\n",
    "        spark (SparkSession): Spark session\n",
    "        fake_path (str): Path to fake news CSV file\n",
    "        true_path (str): Path to true news CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Combined and cleaned dataset with labels\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    df_fake = spark.read.csv(fake_path, header=True, inferSchema=True)\n",
    "    df_real = spark.read.csv(true_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Add labels (0 for fake, 1 for real)\n",
    "    df_fake = df_fake.withColumn(\"label\", lit(0))\n",
    "    df_real = df_real.withColumn(\"label\", lit(1))\n",
    "    \n",
    "    # Select relevant columns and handle missing values\n",
    "    if \"title\" in df_fake.columns and \"text\" in df_fake.columns:\n",
    "        df_fake = df_fake.select(\"title\", \"text\", \"label\").na.drop()\n",
    "        df_real = df_real.select(\"title\", \"text\", \"label\").na.drop()\n",
    "        \n",
    "        # Combine title and text for better context\n",
    "        df_fake = df_fake.withColumn(\"full_text\", \n",
    "                                    concat(col(\"title\"), lit(\". \"), col(\"text\")))\n",
    "        df_real = df_real.withColumn(\"full_text\", \n",
    "                                    concat(col(\"title\"), lit(\". \"), col(\"text\")))\n",
    "        \n",
    "        # Select final columns\n",
    "        df_fake = df_fake.select(\"full_text\", \"label\")\n",
    "        df_real = df_real.select(\"full_text\", \"label\")\n",
    "        \n",
    "        # Rename column\n",
    "        df_fake = df_fake.withColumnRenamed(\"full_text\", \"text\")\n",
    "        df_real = df_real.withColumnRenamed(\"full_text\", \"text\")\n",
    "    else:\n",
    "        df_fake = df_fake.select(\"text\", \"label\").na.drop()\n",
    "        df_real = df_real.select(\"text\", \"label\").na.drop()\n",
    "    \n",
    "    # Combine datasets\n",
    "    df = df_fake.unionByName(df_real)\n",
    "    \n",
    "    # Clean text\n",
    "    df = df.withColumn(\"text\", lower(col(\"text\")))\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), \"[^a-zA-Z0-9\\\\s]\", \" \"))\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), \"\\\\s+\", \" \"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133042e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def advanced_text_preprocessing(df):\n",
    "    \"\"\"\n",
    "    Perform advanced text preprocessing including lemmatization and stopword removal.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with preprocessed text\n",
    "    \"\"\"\n",
    "    # Convert to pandas for more efficient text processing\n",
    "    pandas_df = df.toPandas()\n",
    "    \n",
    "    # Initialize lemmatizer and stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocess_text(text):\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "        \n",
    "        # Join tokens back into text\n",
    "        processed_text = ' '.join(processed_tokens)\n",
    "        \n",
    "        return processed_text\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    pandas_df['processed_text'] = pandas_df['text'].apply(preprocess_text)\n",
    "    \n",
    "    # Convert back to Spark DataFrame\n",
    "    processed_df = df.sparkSession.createDataFrame(pandas_df)\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560c152d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def augment_dataset_with_synonyms(df, augmentation_factor=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Augment dataset by replacing words with synonyms.\n",
    "    This does not introduce future data as it only uses existing vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text column\n",
    "        augmentation_factor (float): Fraction of data to augment\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Augmented DataFrame\n",
    "    \"\"\"\n",
    "    from nltk.corpus import wordnet\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    \n",
    "    # Convert to pandas for more efficient text processing\n",
    "    pandas_df = df.toPandas()\n",
    "    \n",
    "    # Set random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Select subset for augmentation\n",
    "    n_samples = int(len(pandas_df) * augmentation_factor)\n",
    "    augment_indices = np.random.choice(len(pandas_df), n_samples, replace=False)\n",
    "    \n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for idx in augment_indices:\n",
    "        text = pandas_df.iloc[idx]['text']\n",
    "        label = pandas_df.iloc[idx]['label']\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Replace some words with synonyms\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            # 30% chance to replace with synonym if word is alphabetic\n",
    "            if token.isalpha() and np.random.random() < 0.3:\n",
    "                synonyms = []\n",
    "                for syn in wordnet.synsets(token):\n",
    "                    for lemma in syn.lemmas():\n",
    "                        synonyms.append(lemma.name())\n",
    "                \n",
    "                if synonyms:\n",
    "                    # Remove duplicates and original word\n",
    "                    synonyms = list(set(synonyms))\n",
    "                    if token in synonyms:\n",
    "                        synonyms.remove(token)\n",
    "                    \n",
    "                    if synonyms:\n",
    "                        # Replace with random synonym\n",
    "                        replacement = np.random.choice(synonyms)\n",
    "                        new_tokens.append(replacement)\n",
    "                    else:\n",
    "                        new_tokens.append(token)\n",
    "                else:\n",
    "                    new_tokens.append(token)\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "        \n",
    "        # Join tokens back into text\n",
    "        augmented_text = ' '.join(new_tokens)\n",
    "        \n",
    "        augmented_texts.append(augmented_text)\n",
    "        augmented_labels.append(label)\n",
    "    \n",
    "    # Create DataFrame with augmented data\n",
    "    augmented_df = pd.DataFrame({\n",
    "        'text': augmented_texts,\n",
    "        'label': augmented_labels\n",
    "    })\n",
    "    \n",
    "    # Combine original and augmented data\n",
    "    combined_df = pd.concat([pandas_df, augmented_df], ignore_index=True)\n",
    "    \n",
    "    # Convert back to Spark DataFrame\n",
    "    result_df = df.sparkSession.createDataFrame(combined_df)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbac5fb8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_balanced_dataset(df, balance_method='undersample', seed=42):\n",
    "    \"\"\"\n",
    "    Create a balanced dataset by undersampling or oversampling.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text and label columns\n",
    "        balance_method (str): Method to balance dataset ('undersample' or 'oversample')\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Balanced DataFrame\n",
    "    \"\"\"\n",
    "    # Get counts by label\n",
    "    label_counts = df.groupBy(\"label\").count().collect()\n",
    "    label_counts_dict = {row['label']: row['count'] for row in label_counts}\n",
    "    \n",
    "    # Determine minority and majority classes\n",
    "    min_label = min(label_counts_dict.items(), key=lambda x: x[1])[0]\n",
    "    max_label = max(label_counts_dict.items(), key=lambda x: x[1])[0]\n",
    "    min_count = label_counts_dict[min_label]\n",
    "    max_count = label_counts_dict[max_label]\n",
    "    \n",
    "    if balance_method == 'undersample':\n",
    "        # Undersample majority class\n",
    "        df_majority = df.filter(col(\"label\") == max_label)\n",
    "        df_minority = df.filter(col(\"label\") == min_label)\n",
    "        \n",
    "        # Sample majority class to match minority class size\n",
    "        df_majority_sampled = df_majority.sample(fraction=min_count/max_count, seed=seed)\n",
    "        \n",
    "        # Combine minority class with sampled majority class\n",
    "        balanced_df = df_majority_sampled.unionByName(df_minority)\n",
    "        \n",
    "    elif balance_method == 'oversample':\n",
    "        # Oversample minority class\n",
    "        df_majority = df.filter(col(\"label\") == max_label)\n",
    "        df_minority = df.filter(col(\"label\") == min_label)\n",
    "        \n",
    "        # Calculate oversampling ratio (with replacement)\n",
    "        ratio = max_count / min_count\n",
    "        \n",
    "        # Oversample minority class\n",
    "        df_minority_oversampled = df_minority.sample(fraction=ratio, withReplacement=True, seed=seed)\n",
    "        \n",
    "        # Combine majority class with oversampled minority class\n",
    "        balanced_df = df_majority.unionByName(df_minority_oversampled)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"balance_method must be 'undersample' or 'oversample'\")\n",
    "    \n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40405194",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def split_dataset_by_time(df, date_col, train_ratio=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Split dataset by time to prevent data leakage.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text and label columns\n",
    "        date_col (str): Column containing date information\n",
    "        train_ratio (float): Ratio of training data\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_df, test_df)\n",
    "    \"\"\"\n",
    "    # Sort by date\n",
    "    sorted_df = df.orderBy(date_col)\n",
    "    \n",
    "    # Calculate split point\n",
    "    count = sorted_df.count()\n",
    "    split_point = int(count * train_ratio)\n",
    "    \n",
    "    # Split data\n",
    "    train_df = sorted_df.limit(split_point)\n",
    "    test_df = sorted_df.subtract(train_df)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddac42b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_prepared_datasets(train_df, test_df, output_dir):\n",
    "    \"\"\"\n",
    "    Save prepared datasets to disk.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training DataFrame\n",
    "        test_df: Testing DataFrame\n",
    "        output_dir (str): Output directory\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_path, test_path)\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Define output paths\n",
    "    train_path = os.path.join(output_dir, \"train_data.parquet\")\n",
    "    test_path = os.path.join(output_dir, \"test_data.parquet\")\n",
    "    \n",
    "    # Save datasets\n",
    "    train_df.write.mode(\"overwrite\").parquet(train_path)\n",
    "    test_df.write.mode(\"overwrite\").parquet(test_path)\n",
    "    \n",
    "    print(f\"Training data saved to {train_path}\")\n",
    "    print(f\"Testing data saved to {test_path}\")\n",
    "    \n",
    "    return train_path, test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d36c080",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_streaming_simulation_data(df, output_path, batch_size=10, num_batches=5, seed=42):\n",
    "    \"\"\"\n",
    "    Create data for streaming simulation.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text and label columns\n",
    "        output_path (str): Output path for streaming data\n",
    "        batch_size (int): Number of records per batch\n",
    "        num_batches (int): Number of batches to create\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        list: List of batch file paths\n",
    "    \"\"\"\n",
    "    # Sample data for streaming simulation\n",
    "    sample_size = batch_size * num_batches\n",
    "    streaming_df = df.sample(fraction=min(1.0, sample_size/df.count()), seed=seed)\n",
    "    \n",
    "    # Convert to pandas for easier batch processing\n",
    "    pandas_df = streaming_df.toPandas()\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Create batches\n",
    "    batch_files = []\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(pandas_df))\n",
    "        \n",
    "        if start_idx >= len(pandas_df):\n",
    "            break\n",
    "        \n",
    "        batch = pandas_df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Add ID and timestamp columns\n",
    "        batch['id'] = [f\"doc_{i}_{j}\" for j in range(len(batch))]\n",
    "        batch['timestamp'] = pd.Timestamp.now()\n",
    "        \n",
    "        # Save batch\n",
    "        batch_file = f\"{output_path}_batch_{i}.csv\"\n",
    "        batch.to_csv(batch_file, index=False)\n",
    "        batch_files.append(batch_file)\n",
    "    \n",
    "    print(f\"Created {len(batch_files)} streaming data batches\")\n",
    "    \n",
    "    return batch_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(df, output_path=None):\n",
    "    \"\"\"\n",
    "    Analyze dataset and generate statistics.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text and label columns\n",
    "        output_path (str): Path to save analysis results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of dataset statistics\n",
    "    \"\"\"\n",
    "    # Count records by label\n",
    "    label_counts = df.groupBy(\"label\").count().collect()\n",
    "    label_counts_dict = {row['label']: row['count'] for row in label_counts}\n",
    "    \n",
    "    # Calculate text length statistics\n",
    "    from pyspark.sql.functions import length, mean, min, max, stddev\n",
    "    \n",
    "    text_length_stats = df.select(\n",
    "        mean(length(col(\"text\"))).alias(\"mean_length\"),\n",
    "        min(length(col(\"text\"))).alias(\"min_length\"),\n",
    "        max(length(col(\"text\"))).alias(\"max_length\"),\n",
    "        stddev(length(col(\"text\"))).alias(\"stddev_length\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    # Convert to dictionary\n",
    "    stats = {\n",
    "        \"total_records\": df.count(),\n",
    "        \"label_distribution\": label_counts_dict,\n",
    "        \"text_length_stats\": {\n",
    "            \"mean\": text_length_stats[\"mean_length\"],\n",
    "            \"min\": text_length_stats[\"min_length\"],\n",
    "            \"max\": text_length_stats[\"max_length\"],\n",
    "            \"stddev\": text_length_stats[\"stddev_length\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save statistics if output path is provided\n",
    "    if output_path:\n",
    "        import json\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "        print(f\"Dataset analysis saved to {output_path}\")\n",
    "    \n",
    "    return stats"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Last modified: May 29, 2025
