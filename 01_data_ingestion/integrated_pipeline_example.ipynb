{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd4c093",
   "metadata": {},
   "source": [
    "# Pipeline Integrado Completo: Ingestão, Pré-processamento e Persistência\n",
    "\n",
    "Este notebook demonstra o pipeline integrado completo para detecção de fake news, incluindo:\n",
    "1. Ingestão de dados\n",
    "2. Pré-processamento de texto\n",
    "3. Tokenização\n",
    "4. Remoção de stopwords\n",
    "5. Persistência em tabelas Hive e arquivos Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62641bb8",
   "metadata": {},
   "source": [
    "## Configuração e Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0236190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necessárias\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, lower, regexp_replace, regexp_extract, trim, when\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# Importar funções do pipeline integrado\n",
    "from integrated_text_processing import (\n",
    "    preprocess_text,\n",
    "    tokenize_text,\n",
    "    remove_stopwords,\n",
    "    complete_text_processing\n",
    ")\n",
    "\n",
    "# Importar funções de persistência\n",
    "from integrated_save_functions import (\n",
    "    save_to_hive_table,\n",
    "    save_to_parquet,\n",
    "    complete_pipeline_with_persistence\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520f1e47",
   "metadata": {},
   "source": [
    "## Criar Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e728a7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Criar uma sessão Spark com configuração otimizada para Databricks Community Edition\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection_IntegratedPipeline\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Exibir configuração do Spark\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf0b290",
   "metadata": {},
   "source": [
    "## Criar Estrutura de Diretórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee4d21",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_directory_structure(base_dir=\"/dbfs/FileStore/fake_news_detection\"):\n",
    "    \"\"\"\n",
    "    Cria a estrutura de diretórios necessária para o projeto de detecção de fake news.\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): Diretório base para o projeto\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dicionário com caminhos para todos os diretórios criados\n",
    "    \"\"\"\n",
    "    print(f\"Criando estrutura de diretórios em {base_dir}...\")\n",
    "    \n",
    "    # Definir caminhos de diretórios\n",
    "    directories = {\n",
    "        \"data\": f\"{base_dir}/data\",\n",
    "        \"raw_data\": f\"{base_dir}/data/raw\",\n",
    "        \"processed_data\": f\"{base_dir}/data/processed\",\n",
    "        \"sample_data\": f\"{base_dir}/data/sample\",\n",
    "        \"models\": f\"{base_dir}/models\",\n",
    "        \"logs\": f\"{base_dir}/logs\",\n",
    "        \"visualizations\": f\"{base_dir}/visualizations\",\n",
    "        \"temp\": f\"{base_dir}/temp\"\n",
    "    }\n",
    "    \n",
    "    # Criar diretórios\n",
    "    for dir_name, dir_path in directories.items():\n",
    "        # Usar dbutils no ambiente Databricks\n",
    "        try:\n",
    "            dbutils.fs.mkdirs(dir_path)\n",
    "            print(f\"Diretório criado: {dir_path}\")\n",
    "        except NameError:\n",
    "            # Fallback para ambientes não-Databricks\n",
    "            os.makedirs(dir_path.replace(\"/dbfs\", \"\"), exist_ok=True)\n",
    "            print(f\"Diretório criado: {dir_path} (modo local)\")\n",
    "    \n",
    "    print(\"Estrutura de diretórios criada com sucesso\")\n",
    "    return directories\n",
    "\n",
    "# Criar diretórios\n",
    "directories = create_directory_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538459cd",
   "metadata": {},
   "source": [
    "## Carregar Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e0a672",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_csv_files(fake_path, true_path, cache=True):\n",
    "    \"\"\"\n",
    "    Carrega arquivos CSV contendo notícias falsas e verdadeiras.\n",
    "    \n",
    "    Args:\n",
    "        fake_path (str): Caminho para o arquivo CSV com notícias falsas\n",
    "        true_path (str): Caminho para o arquivo CSV com notícias verdadeiras\n",
    "        cache (bool): Se deve usar cache durante o carregamento\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (fake_df, true_df) DataFrames com dados carregados\n",
    "    \"\"\"\n",
    "    print(f\"Carregando arquivos CSV de {fake_path} e {true_path}...\")\n",
    "    \n",
    "    # Carregar arquivos CSV\n",
    "    fake_df = spark.read.csv(fake_path, header=True, inferSchema=True)\n",
    "    true_df = spark.read.csv(true_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Adicionar rótulos (0 para falso, 1 para verdadeiro)\n",
    "    fake_df = fake_df.withColumn(\"label\", lit(0))\n",
    "    true_df = true_df.withColumn(\"label\", lit(1))\n",
    "    \n",
    "    # Cache DataFrames se solicitado\n",
    "    if cache:\n",
    "        fake_df.cache()\n",
    "        true_df.cache()\n",
    "        # Forçar materialização\n",
    "        fake_count = fake_df.count()\n",
    "        true_count = true_df.count()\n",
    "    \n",
    "    # Mostrar informações sobre os DataFrames\n",
    "    print(f\"Notícias falsas carregadas: {fake_df.count()} registros\")\n",
    "    print(f\"Notícias verdadeiras carregadas: {true_df.count()} registros\")\n",
    "    \n",
    "    return fake_df, true_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cc07b",
   "metadata": {},
   "source": [
    "## Combinar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2b6ae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def combine_datasets(fake_df, true_df, cache=True):\n",
    "    \"\"\"\n",
    "    Combina datasets de notícias falsas e verdadeiras em um único DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        fake_df: DataFrame com notícias falsas\n",
    "        true_df: DataFrame com notícias verdadeiras\n",
    "        cache (bool): Se deve usar cache durante a combinação\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame combinado com notícias falsas e verdadeiras\n",
    "    \"\"\"\n",
    "    print(\"Combinando datasets de notícias falsas e verdadeiras...\")\n",
    "    \n",
    "    # Combinar datasets\n",
    "    combined_df = fake_df.union(true_df)\n",
    "    \n",
    "    # Cache o DataFrame combinado se solicitado\n",
    "    if cache:\n",
    "        combined_df.cache()\n",
    "        # Forçar materialização\n",
    "        combined_count = combined_df.count()\n",
    "    \n",
    "    print(f\"Dataset combinado criado com {combined_df.count()} registros\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addce206",
   "metadata": {},
   "source": [
    "## Exemplo 1: Pipeline Passo a Passo\n",
    "\n",
    "Este exemplo mostra como executar o pipeline passo a passo, com controle explícito sobre cada etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33553ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminhos (atualize com seus caminhos reais)\n",
    "fake_path = \"/path/to/Fake.csv\"\n",
    "true_path = \"/path/to/True.csv\"\n",
    "\n",
    "# Etapa 1: Carregar dados\n",
    "# fake_df, true_df = load_csv_files(fake_path, true_path, cache=True)\n",
    "\n",
    "# Etapa 2: Combinar datasets\n",
    "# combined_df = combine_datasets(fake_df, true_df, cache=True)\n",
    "\n",
    "# Etapa 3: Pré-processamento de texto\n",
    "# preprocessed_df = preprocess_text(combined_df, cache=True)\n",
    "\n",
    "# Etapa 4: Tokenização\n",
    "# tokenized_df = tokenize_text(preprocessed_df, text_column=\"text\", output_column=\"tokens\")\n",
    "\n",
    "# Etapa 5: Remoção de stopwords\n",
    "# processed_df = remove_stopwords(tokenized_df, tokens_column=\"tokens\", output_column=\"filtered_tokens\")\n",
    "\n",
    "# Etapa 6: Salvar em tabela Hive\n",
    "# save_to_hive_table(processed_df, \"processed_news\", partition_by=\"label\")\n",
    "\n",
    "# Etapa 7: Salvar em Parquet\n",
    "# parquet_path = f\"{directories['processed_data']}/processed_news.parquet\"\n",
    "# save_to_parquet(processed_df, parquet_path, partition_by=\"label\")\n",
    "\n",
    "# Etapa 8: Liberar memória\n",
    "# fake_df.unpersist()\n",
    "# true_df.unpersist()\n",
    "# combined_df.unpersist()\n",
    "# preprocessed_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d8805",
   "metadata": {},
   "source": [
    "## Exemplo 2: Pipeline Integrado com Processamento de Texto\n",
    "\n",
    "Este exemplo usa a função `complete_text_processing` para executar o pré-processamento, tokenização e remoção de stopwords em uma única chamada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0216998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminhos (atualize com seus caminhos reais)\n",
    "fake_path = \"/path/to/Fake.csv\"\n",
    "true_path = \"/path/to/True.csv\"\n",
    "\n",
    "# Etapa 1: Carregar dados\n",
    "# fake_df, true_df = load_csv_files(fake_path, true_path, cache=True)\n",
    "\n",
    "# Etapa 2: Combinar datasets\n",
    "# combined_df = combine_datasets(fake_df, true_df, cache=True)\n",
    "\n",
    "# Etapa 3: Processamento completo de texto (pré-processamento, tokenização, remoção de stopwords)\n",
    "# processed_df = complete_text_processing(combined_df, cache=True)\n",
    "\n",
    "# Etapa 4: Salvar em tabela Hive\n",
    "# save_to_hive_table(processed_df, \"processed_news\", partition_by=\"label\")\n",
    "\n",
    "# Etapa 5: Salvar em Parquet\n",
    "# parquet_path = f\"{directories['processed_data']}/processed_news.parquet\"\n",
    "# save_to_parquet(processed_df, parquet_path, partition_by=\"label\")\n",
    "\n",
    "# Etapa 6: Liberar memória\n",
    "# fake_df.unpersist()\n",
    "# true_df.unpersist()\n",
    "# combined_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303d04a",
   "metadata": {},
   "source": [
    "## Exemplo 3: Pipeline Completo com Persistência\n",
    "\n",
    "Este exemplo usa a função `complete_pipeline_with_persistence` para executar todo o pipeline em uma única chamada, incluindo persistência em Hive e Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminhos (atualize com seus caminhos reais)\n",
    "fake_path = \"/path/to/Fake.csv\"\n",
    "true_path = \"/path/to/True.csv\"\n",
    "\n",
    "# Executar o pipeline completo com persistência\n",
    "# processed_df = complete_pipeline_with_persistence(fake_path, true_path, directories, cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b2d4a",
   "metadata": {},
   "source": [
    "## Examinar os Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f777c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibir schema\n",
    "# processed_df.printSchema()\n",
    "\n",
    "# Mostrar dados de exemplo\n",
    "# display(processed_df.select(\"text\", \"tokens\", \"filtered_tokens\", \"label\", \"location\", \"news_source\").limit(5))\n",
    "\n",
    "# Contar registros por rótulo\n",
    "# display(processed_df.groupBy(\"label\").count().orderBy(\"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d1a63c",
   "metadata": {},
   "source": [
    "## Abordagem com Pipeline API\n",
    "\n",
    "Uma abordagem alternativa é usar a API Pipeline do Spark ML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40fa857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "def create_pipeline_api_approach(include_features=True):\n",
    "    \"\"\"\n",
    "    Cria um pipeline de processamento de texto usando a API Pipeline do Spark ML.\n",
    "    \n",
    "    Args:\n",
    "        include_features (bool): Se deve incluir etapas de extração de características\n",
    "        \n",
    "    Returns:\n",
    "        Pipeline: Pipeline Spark ML para processamento de texto\n",
    "    \"\"\"\n",
    "    # Definir transformadores\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "    \n",
    "    # Criar estágios do pipeline\n",
    "    stages = [tokenizer, remover]\n",
    "    \n",
    "    # Opcionalmente adicionar extração de características\n",
    "    if include_features:\n",
    "        hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "        idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "        stages.extend([hashingTF, idf])\n",
    "    \n",
    "    # Criar e retornar o pipeline\n",
    "    return Pipeline(stages=stages)\n",
    "\n",
    "# Exemplo de uso (comentado)\n",
    "# pipeline = create_pipeline_api_approach(include_features=True)\n",
    "# model = pipeline.fit(preprocessed_df)\n",
    "# processed_df = model.transform(preprocessed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9863e",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Este notebook demonstra o pipeline integrado completo para detecção de fake news, combinando ingestão de dados, pré-processamento, tokenização, remoção de stopwords e persistência em uma única fase.\n",
    "\n",
    "Benefícios desta abordagem:\n",
    "1. Redução de computação ao eliminar processamento redundante\n",
    "2. Melhoria na eficiência de memória através de cache estratégico\n",
    "3. Fluxo de trabalho simplificado com menos etapas\n",
    "4. Desempenho aprimorado em ambientes com recursos limitados como Databricks Community Edition\n",
    "5. Persistência eficiente em múltiplos formatos (Hive e Parquet)\n",
    "\n",
    "Os dados processados estão agora prontos para engenharia de características e treinamento de modelos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
