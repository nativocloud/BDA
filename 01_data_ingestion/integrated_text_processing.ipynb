{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b889e1",
   "metadata": {},
   "source": [
    "# Integrated Text Processing for Fake News Detection\n",
    "\n",
    "This notebook demonstrates the integrated approach to text processing, combining data ingestion and preprocessing in a single phase. This optimized pipeline improves efficiency and reduces redundancy in the fake news detection workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943e2901",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe57e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, lower, regexp_replace, regexp_extract, trim, when, rand, concat\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import NLTK for text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK resources\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4654ec",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4f90e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create a Spark session with configuration optimized for Databricks Community Edition\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection_IntegratedProcessing\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be61ed82",
   "metadata": {},
   "source": [
    "## Create Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7662fe4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_directory_structure(base_dir=\"/dbfs/FileStore/fake_news_detection\"):\n",
    "    \"\"\"\n",
    "    Creates the necessary directory structure for the fake news detection project.\n",
    "    \n",
    "    This function ensures all required directories exist in the Databricks environment.\n",
    "    It's essential to run this function before executing the rest of the pipeline.\n",
    "    \n",
    "    Args:\n",
    "        base_dir (str): Base directory for the project\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with paths to all created directories\n",
    "    \"\"\"\n",
    "    print(f\"Creating directory structure in {base_dir}...\")\n",
    "    \n",
    "    # Define directory paths\n",
    "    directories = {\n",
    "        \"data\": f\"{base_dir}/data\",\n",
    "        \"raw_data\": f\"{base_dir}/data/raw\",\n",
    "        \"processed_data\": f\"{base_dir}/data/processed\",\n",
    "        \"sample_data\": f\"{base_dir}/data/sample\",\n",
    "        \"models\": f\"{base_dir}/models\",\n",
    "        \"logs\": f\"{base_dir}/logs\",\n",
    "        \"visualizations\": f\"{base_dir}/visualizations\",\n",
    "        \"temp\": f\"{base_dir}/temp\"\n",
    "    }\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_name, dir_path in directories.items():\n",
    "        # Use dbutils in Databricks environment\n",
    "        try:\n",
    "            dbutils.fs.mkdirs(dir_path)\n",
    "            print(f\"Created directory: {dir_path}\")\n",
    "        except NameError:\n",
    "            # Fallback for non-Databricks environments\n",
    "            os.makedirs(dir_path.replace(\"/dbfs\", \"\"), exist_ok=True)\n",
    "            print(f\"Created directory: {dir_path} (local mode)\")\n",
    "    \n",
    "    print(\"Directory structure created successfully\")\n",
    "    return directories\n",
    "\n",
    "# Create directories\n",
    "directories = create_directory_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db324ee9",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b85eb6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_csv_files(fake_path, true_path, cache=True):\n",
    "    \"\"\"\n",
    "    Loads CSV files containing fake and true news articles.\n",
    "    \n",
    "    Args:\n",
    "        fake_path (str): Path to the CSV file with fake news\n",
    "        true_path (str): Path to the CSV file with true news\n",
    "        cache (bool): Whether to cache the DataFrames in memory\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (fake_df, true_df) DataFrames with loaded data\n",
    "    \"\"\"\n",
    "    print(f\"Loading CSV files from {fake_path} and {true_path}...\")\n",
    "    \n",
    "    # Load CSV files\n",
    "    fake_df = spark.read.csv(fake_path, header=True, inferSchema=True)\n",
    "    true_df = spark.read.csv(true_path, header=True, inferSchema=True)\n",
    "    \n",
    "    # Add labels (0 for fake, 1 for true)\n",
    "    fake_df = fake_df.withColumn(\"label\", lit(0))\n",
    "    true_df = true_df.withColumn(\"label\", lit(1))\n",
    "    \n",
    "    # Cache DataFrames if requested (improves performance for multiple operations)\n",
    "    if cache:\n",
    "        fake_df.cache()\n",
    "        true_df.cache()\n",
    "        # Force materialization\n",
    "        fake_count = fake_df.count()\n",
    "        true_count = true_df.count()\n",
    "    \n",
    "    # Show information about the DataFrames\n",
    "    print(f\"Fake news loaded: {fake_df.count()} records\")\n",
    "    print(f\"True news loaded: {true_df.count()} records\")\n",
    "    \n",
    "    return fake_df, true_df\n",
    "\n",
    "def combine_datasets(fake_df, true_df, cache=True):\n",
    "    \"\"\"\n",
    "    Combines fake and true news datasets into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        fake_df: DataFrame with fake news\n",
    "        true_df: DataFrame with true news\n",
    "        cache (bool): Whether to cache the combined DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Combined DataFrame with both fake and true news\n",
    "    \"\"\"\n",
    "    print(\"Combining fake and true news datasets...\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_df = fake_df.union(true_df)\n",
    "    \n",
    "    # Cache the combined DataFrame if requested\n",
    "    if cache:\n",
    "        combined_df.cache()\n",
    "        # Force materialization\n",
    "        combined_count = combined_df.count()\n",
    "    \n",
    "    print(f\"Combined dataset created with {combined_df.count()} records\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b06df8",
   "metadata": {},
   "source": [
    "## Integrated Text Processing Functions\n",
    "\n",
    "These functions combine preprocessing, tokenization, and stopword removal in a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b2ad8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, regexp_extract, trim, when, lit, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "def preprocess_text(df, cache=True):\n",
    "    \"\"\"\n",
    "    Optimized text preprocessing function for Spark performance with fixed acronym handling.\n",
    "    Preprocesses text by extracting optional location(s) and news source,\n",
    "    normalizing acronyms, converting to lowercase, removing special characters,\n",
    "    and handling multiple spaces.\n",
    "    \n",
    "    This version fixes the issue with acronyms like \"U.S.\" being incorrectly converted to \"u s\".\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text and potentially title columns.\n",
    "        cache (bool): Whether to cache the preprocessed DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame with preprocessed text (and title if applicable),\n",
    "                   plus new 'location' and 'news_source' columns.\n",
    "    \"\"\"\n",
    "    print(\"Starting text preprocessing...\")\n",
    "    \n",
    "    # Create a list to track columns that need preprocessing\n",
    "    columns_to_preprocess = []\n",
    "    \n",
    "    # Check for text and title columns upfront to minimize schema lookups\n",
    "    has_text = \"text\" in df.columns\n",
    "    has_title = \"title\" in df.columns\n",
    "    \n",
    "    # Get column types once to avoid repeated schema lookups\n",
    "    if has_text:\n",
    "        text_is_string = isinstance(df.schema[\"text\"].dataType, StringType)\n",
    "        if text_is_string:\n",
    "            columns_to_preprocess.append(\"text\")\n",
    "    \n",
    "    if has_title:\n",
    "        title_is_string = isinstance(df.schema[\"title\"].dataType, StringType)\n",
    "        if title_is_string:\n",
    "            columns_to_preprocess.append(\"title\")\n",
    "    \n",
    "    # --- 1. Extract Optional Location(s) and News Source from 'text' column ---\n",
    "    if has_text and text_is_string:\n",
    "        print(\"â€¢ Extracting 'location' and 'news_source' from 'text' column...\")\n",
    "        \n",
    "        # Optimize regex pattern with non-capturing groups where possible\n",
    "        news_header_pattern = r\"^(?:([A-Z][a-zA-Z\\s\\./,]*)\\s*)?\\(([^)]+)\\)\\s*-\\s*(.*)\"\n",
    "        \n",
    "        # Apply all extractions in a single transformation to minimize passes over the data\n",
    "        df = df.withColumn(\"location\", regexp_extract(col(\"text\"), news_header_pattern, 1)) \\\n",
    "               .withColumn(\"news_source\", regexp_extract(col(\"text\"), news_header_pattern, 2)) \\\n",
    "               .withColumn(\"text_cleaned\", regexp_extract(col(\"text\"), news_header_pattern, 3))\n",
    "        \n",
    "        # Update text column and handle empty extractions in a single transformation\n",
    "        df = df.withColumn(\"text\", \n",
    "                          when(col(\"text_cleaned\") != \"\", col(\"text_cleaned\"))\n",
    "                          .otherwise(col(\"text\"))) \\\n",
    "               .withColumn(\"location\", \n",
    "                          when(col(\"location\") == \"\", lit(None))\n",
    "                          .otherwise(trim(col(\"location\")))) \\\n",
    "               .withColumn(\"news_source\", \n",
    "                          when(col(\"news_source\") == \"\", lit(None))\n",
    "                          .otherwise(trim(col(\"news_source\")))) \\\n",
    "               .drop(\"text_cleaned\")\n",
    "        \n",
    "        print(\"â€¢ 'location' and 'news_source' columns added (if pattern found).\")\n",
    "    else:\n",
    "        if has_text:\n",
    "            print(f\"â€¢ Skipping location/source extraction: 'text' column is not a string type.\")\n",
    "        else:\n",
    "            print(\"â€¢ 'text' column not found, skipping location/source extraction.\")\n",
    "    \n",
    "    # --- 2. Apply acronym normalization BEFORE any other text processing ---\n",
    "    if columns_to_preprocess:\n",
    "        print(f\"â€¢ Applying acronym normalization to {len(columns_to_preprocess)} column(s): {', '.join(columns_to_preprocess)}\")\n",
    "        \n",
    "        # Define a function to normalize acronyms\n",
    "        def normalize_acronyms(text):\n",
    "            if text is None:\n",
    "                return None\n",
    "                \n",
    "            # Replace common acronyms with their normalized forms\n",
    "            # The order is important - longer patterns first\n",
    "            replacements = [\n",
    "                (\"U.S.A.\", \"USA\"),\n",
    "                (\"U.S.\", \"US\"),\n",
    "                (\"U.N.\", \"UN\"),\n",
    "                (\"F.B.I.\", \"FBI\"),\n",
    "                (\"C.I.A.\", \"CIA\"),\n",
    "                (\"D.C.\", \"DC\"),\n",
    "                (\"U.K.\", \"UK\"),\n",
    "                (\"E.U.\", \"EU\"),\n",
    "                (\"N.Y.\", \"NY\"),\n",
    "                (\"L.A.\", \"LA\"),\n",
    "                (\"N.A.T.O.\", \"NATO\"),\n",
    "                (\"W.H.O.\", \"WHO\")\n",
    "            ]\n",
    "            \n",
    "            for pattern, replacement in replacements:\n",
    "                # Use Python's replace method which is more reliable for exact string replacement\n",
    "                text = text.replace(pattern, replacement)\n",
    "                \n",
    "            return text\n",
    "        \n",
    "        # Register the UDF\n",
    "        normalize_acronyms_udf = udf(normalize_acronyms, StringType())\n",
    "        \n",
    "        # Apply the UDF to each column that needs preprocessing\n",
    "        for col_name in columns_to_preprocess:\n",
    "            # First normalize acronyms using the UDF\n",
    "            df = df.withColumn(col_name, normalize_acronyms_udf(col(col_name)))\n",
    "            print(f\"  - Applied acronym normalization to '{col_name}'.\")\n",
    "        \n",
    "        # --- 3. Now apply the rest of the text preprocessing ---\n",
    "        print(f\"â€¢ Applying general text preprocessing to {len(columns_to_preprocess)} column(s)\")\n",
    "        \n",
    "        for col_name in columns_to_preprocess:\n",
    "            # Apply remaining transformations in a single chain\n",
    "            df = df.withColumn(\n",
    "                col_name,\n",
    "                # Step 3: Trim and normalize spaces\n",
    "                trim(\n",
    "                    regexp_replace(\n",
    "                        # Step 2: Remove special characters (keep #@)\n",
    "                        regexp_replace(\n",
    "                            # Step 1: Convert to lowercase\n",
    "                            lower(col(col_name)),\n",
    "                            \"[^a-z0-9\\\\s#@]\", \" \"  # Remove special chars\n",
    "                        ),\n",
    "                        \"\\\\s+\", \" \"  # Normalize spaces\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            print(f\"  - Applied full preprocessing chain to '{col_name}'.\")\n",
    "    else:\n",
    "        print(\"â€¢ No suitable text columns found for preprocessing.\")\n",
    "    \n",
    "    # --- 4. Data Leakage Check and Removal ('subject' column) ---\n",
    "    has_subject = \"subject\" in df.columns\n",
    "    if has_subject:\n",
    "        print(\"\\nWARNING: Removing 'subject' column to prevent data leakage.\")\n",
    "        df = df.drop(\"subject\")\n",
    "        print(\"'subject' column successfully removed.\")\n",
    "    else:\n",
    "        print(\"\\n'subject' column not found, no data leakage prevention needed for this column.\")\n",
    "    \n",
    "    # --- 5. Cache the preprocessed DataFrame if requested ---\n",
    "    if cache:\n",
    "        print(\"â€¢ Caching the preprocessed DataFrame for optimized downstream operations.\")\n",
    "        df.cache()\n",
    "        # Force materialization of the cache to ensure transformations are computed\n",
    "        df.count()\n",
    "    else:\n",
    "        print(\"â€¢ Caching of the preprocessed DataFrame is disabled.\")\n",
    "    \n",
    "    print(\"Text preprocessing complete.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787db190",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize_text(df, text_column=\"text\", output_column=\"tokens\"):\n",
    "    \"\"\"\n",
    "    Tokenize text into words.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text column\n",
    "        text_column (str): Name of the text column\n",
    "        output_column (str): Name of the output column for tokens\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with tokenized text\n",
    "    \"\"\"\n",
    "    print(\"Tokenizing text...\")\n",
    "    \n",
    "    # Create a tokenizer\n",
    "    tokenizer = Tokenizer(inputCol=text_column, outputCol=output_column)\n",
    "    \n",
    "    # Apply tokenization\n",
    "    tokenized_df = tokenizer.transform(df)\n",
    "    \n",
    "    return tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05165671",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(df, tokens_column=\"tokens\", output_column=\"filtered_tokens\"):\n",
    "    \"\"\"\n",
    "    Remove stopwords from tokenized text.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with tokens column\n",
    "        tokens_column (str): Name of the tokens column\n",
    "        output_column (str): Name of the output column for filtered tokens\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with stopwords removed\n",
    "    \"\"\"\n",
    "    print(\"Removing stopwords...\")\n",
    "    \n",
    "    # Create a stopwords remover\n",
    "    remover = StopWordsRemover(inputCol=tokens_column, outputCol=output_column)\n",
    "    \n",
    "    # Apply stopwords removal\n",
    "    filtered_df = remover.transform(df)\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8170445",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def complete_text_processing(df, cache=True):\n",
    "    \"\"\"\n",
    "    Performs complete text processing in a single pass:\n",
    "    1. Text preprocessing (acronym normalization, lowercase, etc.)\n",
    "    2. Tokenization\n",
    "    3. Stopword removal\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text column\n",
    "        cache (bool): Whether to cache intermediate DataFrames\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Fully processed DataFrame with tokens and filtered tokens\n",
    "    \"\"\"\n",
    "    print(\"Starting complete text processing pipeline...\")\n",
    "    \n",
    "    # Step 1: Preprocess text\n",
    "    preprocessed_df = preprocess_text(df, cache=cache)\n",
    "    \n",
    "    # Step 2: Tokenize text\n",
    "    tokenized_df = tokenize_text(preprocessed_df, text_column=\"text\", output_column=\"tokens\")\n",
    "    \n",
    "    # Step 3: Remove stopwords\n",
    "    processed_df = remove_stopwords(tokenized_df, tokens_column=\"tokens\", output_column=\"filtered_tokens\")\n",
    "    \n",
    "    # Unpersist intermediate DataFrame if it was cached\n",
    "    if cache and preprocessed_df != df:  # Only if it's a different DataFrame\n",
    "        try:\n",
    "            preprocessed_df.unpersist()\n",
    "            print(\"Unpersisted intermediate preprocessed DataFrame to free memory.\")\n",
    "        except:\n",
    "            print(\"Note: Could not unpersist intermediate DataFrame.\")\n",
    "    \n",
    "    print(\"Complete text processing pipeline finished.\")\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3e272",
   "metadata": {},
   "source": [
    "## Data Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0d1b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_subject_distribution(fake_df, true_df):\n",
    "    \"\"\"\n",
    "    Analyzes the distribution of subjects in fake and true news datasets to detect potential data leakage.\n",
    "    Provides essential checks using native PySpark functionality.\n",
    "    Optimized for Databricks environment with display() visualizations by minimizing collect() calls.\n",
    "\n",
    "    Args:\n",
    "        fake_df: DataFrame with fake news.\n",
    "        true_df: DataFrame with true news.\n",
    "\n",
    "    Returns:\n",
    "        None (This function prints analysis directly to the Databricks notebook output).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š SUBJECT DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # --- Initial Checks for Robustness ---\n",
    "    # Calculate total counts upfront (this is a necessary action)\n",
    "    fake_total = fake_df.count()\n",
    "    true_total = true_df.count()\n",
    "\n",
    "    if fake_total == 0:\n",
    "        print(\"\\nâš ï¸ Fake news DataFrame is empty. Analysis cannot proceed.\")\n",
    "        return\n",
    "    if true_total == 0:\n",
    "        print(\"\\nâš ï¸ True news DataFrame is empty. Analysis cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    # Check if 'subject' column exists in both DataFrames\n",
    "    if \"subject\" not in fake_df.columns or \"subject\" not in true_df.columns:\n",
    "        print(\"\\nâš ï¸ 'subject' column not found in one or both datasets. Analysis cannot proceed.\")\n",
    "        print(f\"  Fake DF columns: {fake_df.columns}\")\n",
    "        print(f\"  True DF columns: {true_df.columns}\")\n",
    "        return\n",
    "    \n",
    "    # Check if 'subject' column is of a string type for proper analysis\n",
    "    fake_subject_type = fake_df.schema[\"subject\"].dataType\n",
    "    true_subject_type = true_df.schema[\"subject\"].dataType\n",
    "    if not isinstance(fake_subject_type, StringType) or not isinstance(true_subject_type, StringType):\n",
    "        print(f\"\\nâš ï¸ 'subject' column expected to be 'string' type for distribution analysis, but found '{fake_subject_type.typeName()}' in fake_df and '{true_subject_type.typeName()}' in true_df. Analysis cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    # --- Step 1 & 2: Get and Display Subject Distributions ---\n",
    "   \n",
    "    print(\"\\n1ï¸âƒ£ FAKE NEWS SUBJECT DISTRIBUTION\")\n",
    "    fake_subjects_df = fake_df.groupBy(\"subject\").count().orderBy(col(\"count\").desc())\n",
    "    print(\"â€¢ Subject distribution in fake news:\")\n",
    "    \n",
    "    # Use display() in Databricks, otherwise print\n",
    "    try:\n",
    "        display(fake_subjects_df)\n",
    "    except NameError:\n",
    "        print(fake_subjects_df.toPandas())\n",
    "\n",
    "    print(\"\\n2ï¸âƒ£ TRUE NEWS SUBJECT DISTRIBUTION\")\n",
    "    true_subjects_df = true_df.groupBy(\"subject\").count().orderBy(col(\"count\").desc())\n",
    "    print(\"â€¢ Subject distribution in true news:\")\n",
    "    \n",
    "    # Use display() in Databricks, otherwise print\n",
    "    try:\n",
    "        display(true_subjects_df)\n",
    "    except NameError:\n",
    "        print(true_subjects_df.toPandas())\n",
    "\n",
    "    # --- Step 3: Subject Overlap Analysis ---\n",
    "    print(\"\\n3ï¸âƒ£ SUBJECT OVERLAP ANALYSIS\")\n",
    "\n",
    "    # Get total unique subjects in each dataset directly in Spark\n",
    "    num_fake_unique_subjects = fake_subjects_df.count()\n",
    "    num_true_unique_subjects = true_subjects_df.count()\n",
    "\n",
    "    # Find common subjects \n",
    "    common_subjects_df = fake_subjects_df.join(true_subjects_df, on=\"subject\", how=\"inner\")\n",
    "    num_common_subjects = common_subjects_df.count()\n",
    "\n",
    "    # Find subjects exclusive to fake news using left_anti join\n",
    "    fake_exclusive_df = fake_subjects_df.join(true_subjects_df, on=\"subject\", how=\"left_anti\")\n",
    "    num_fake_exclusive = fake_exclusive_df.count()\n",
    "\n",
    "    # Find subjects exclusive to true news using right_anti join (or left_anti with roles swapped)\n",
    "    true_exclusive_df = true_subjects_df.join(fake_subjects_df, on=\"subject\", how=\"left_anti\") \n",
    "    # returns only the rows from the left DataFrame that have no match in the right DataFrame.\n",
    "    num_true_exclusive = true_exclusive_df.count()\n",
    "\n",
    "    print(f\"â€¢ Total unique subjects in fake news: {num_fake_unique_subjects}\")\n",
    "    print(f\"â€¢ Total unique subjects in true news: {num_true_unique_subjects}\")\n",
    "    print(f\"â€¢ Subjects common to both datasets: {num_common_subjects}\")\n",
    "    print(f\"â€¢ Subjects exclusive to fake news: {num_fake_exclusive}\")\n",
    "    print(f\"â€¢ Subjects exclusive to true news: {num_true_exclusive}\")\n",
    "\n",
    "    # Create a comparison view for common subjects\n",
    "    if num_common_subjects > 0:\n",
    "        print(\"\\nâ€¢ Distribution of common subjects (count and percentage):\")\n",
    "\n",
    "        # Create temporary views for SQL query \n",
    "        # Note: Using distinct temp view names to avoid conflicts if the notebook runs multiple times\n",
    "        fake_df.createOrReplaceTempView(\"fake_news_temp_view_subject_analysis\")\n",
    "        true_df.createOrReplaceTempView(\"true_news_temp_view_subject_analysis\")\n",
    "\n",
    "        # SQL query to compare subject distributions and their percentages\n",
    "        comparison_query = f\"\"\"\n",
    "        SELECT\n",
    "            f.subject,\n",
    "            f.count AS fake_count,\n",
    "            t.count AS true_count,\n",
    "            CAST(f.count AS DOUBLE) / {fake_total} * 100 AS fake_percentage,\n",
    "            CAST(t.count AS DOUBLE) / {true_total} * 100 AS true_percentage\n",
    "        FROM\n",
    "            (SELECT subject, COUNT(*) AS count FROM fake_news_temp_view_subject_analysis GROUP BY subject) f\n",
    "        JOIN\n",
    "            (SELECT subject, COUNT(*) AS count FROM true_news_temp_view_subject_analysis GROUP BY subject) t\n",
    "        ON\n",
    "            f.subject = t.subject\n",
    "        ORDER BY\n",
    "            ABS((CAST(f.count AS DOUBLE) / {fake_total} * 100) - (CAST(t.count AS DOUBLE) / {true_total} * 100)) DESC\n",
    "        \"\"\"\n",
    "\n",
    "        comparison_df = spark.sql(comparison_query)\n",
    "        \n",
    "        # Use display() in Databricks, otherwise print\n",
    "        try:\n",
    "            display(comparison_df)\n",
    "        except NameError:\n",
    "            print(comparison_df.toPandas())\n",
    "    else:\n",
    "        print(\"\\nâ€¢ No common subjects found between fake and true news datasets, skipping detailed comparison table.\")\n",
    "\n",
    "    # --- Step 4: Data Leakage Assessment ---\n",
    "    print(\"\\n4ï¸âƒ£ DATA LEAKAGE ASSESSMENT\")\n",
    "    \n",
    "    # Check if there's a perfect separation by subject\n",
    "    if num_common_subjects == 0 and num_fake_unique_subjects > 0 and num_true_unique_subjects > 0:\n",
    "        print(\"\\nðŸš¨ HIGH RISK OF DATA LEAKAGE DETECTED!\")\n",
    "        print(\"â€¢ The 'subject' column perfectly separates fake and true news articles.\")\n",
    "        print(\"â€¢ This is a clear case of data leakage that would artificially inflate model performance.\")\n",
    "        print(\"â€¢ RECOMMENDATION: Remove the 'subject' column before model training.\")\n",
    "    elif num_common_subjects > 0:\n",
    "        # Get the most biased subjects (those that appear predominantly in one class)\n",
    "        if comparison_df.count() > 0:\n",
    "            # Calculate the absolute difference between fake and true percentages\n",
    "            comparison_df = comparison_df.withColumn(\n",
    "                \"percentage_difference\", \n",
    "                abs(col(\"fake_percentage\") - col(\"true_percentage\"))\n",
    "            )\n",
    "            \n",
    "            # Find subjects with high bias (>80% difference)\n",
    "            highly_biased = comparison_df.filter(col(\"percentage_difference\") > 80).count()\n",
    "            \n",
    "            if highly_biased > 0:\n",
    "                print(\"\\nðŸ”¶ MODERATE RISK OF DATA LEAKAGE DETECTED!\")\n",
    "                print(f\"â€¢ Found {highly_biased} subjects with >80% difference in distribution between classes.\")\n",
    "                print(\"â€¢ These subjects may create partial data leakage.\")\n",
    "                print(\"â€¢ RECOMMENDATION: Consider removing the 'subject' column or perform careful cross-validation.\")\n",
    "            else:\n",
    "                print(\"\\nâœ… LOW RISK OF DATA LEAKAGE\")\n",
    "                print(\"â€¢ Subject distributions do not show strong bias toward either class.\")\n",
    "                print(\"â€¢ The 'subject' column may be used as a feature with caution.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf395e1d",
   "metadata": {},
   "source": [
    "## Complete Integrated Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade04d04",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_integrated_pipeline(fake_path, true_path, output_path=None, cache=True):\n",
    "    \"\"\"\n",
    "    Runs the complete integrated pipeline for fake news detection.\n",
    "    This combines data ingestion and preprocessing in a single phase.\n",
    "    \n",
    "    Args:\n",
    "        fake_path (str): Path to the CSV file with fake news\n",
    "        true_path (str): Path to the CSV file with true news\n",
    "        output_path (str): Path to save the processed data (optional)\n",
    "        cache (bool): Whether to cache DataFrames during processing\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Fully processed DataFrame ready for feature engineering\n",
    "    \"\"\"\n",
    "    print(\"Starting integrated pipeline for fake news detection...\")\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    fake_df, true_df = load_csv_files(fake_path, true_path, cache=cache)\n",
    "    \n",
    "    # Step 2: Analyze subject distribution to detect data leakage\n",
    "    analyze_subject_distribution(fake_df, true_df)\n",
    "    \n",
    "    # Step 3: Combine datasets\n",
    "    combined_df = combine_datasets(fake_df, true_df, cache=cache)\n",
    "    \n",
    "    # Step 4: Complete text processing (preprocessing + tokenization + stopword removal)\n",
    "    processed_df = complete_text_processing(combined_df, cache=cache)\n",
    "    \n",
    "    # Step 5: Save processed data if output path is provided\n",
    "    if output_path:\n",
    "        print(f\"Saving processed data to {output_path}...\")\n",
    "        processed_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        print(\"Data saved successfully.\")\n",
    "    \n",
    "    # Step 6: Unpersist DataFrames that are no longer needed\n",
    "    if cache:\n",
    "        print(\"Cleaning up memory...\")\n",
    "        try:\n",
    "            fake_df.unpersist()\n",
    "            true_df.unpersist()\n",
    "            combined_df.unpersist()\n",
    "            print(\"Memory cleanup complete.\")\n",
    "        except:\n",
    "            print(\"Note: Could not unpersist some DataFrames.\")\n",
    "    \n",
    "    print(\"Integrated pipeline completed successfully.\")\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a669ee8",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Here's how to use the integrated pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174794ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "fake_path = \"/path/to/Fake.csv\"  # Update with your actual path\n",
    "true_path = \"/path/to/True.csv\"  # Update with your actual path\n",
    "output_path = \"/path/to/processed_data\"  # Update with your desired output path\n",
    "\n",
    "# Run the integrated pipeline\n",
    "# Uncomment the following lines to execute\n",
    "# processed_df = run_integrated_pipeline(\n",
    "#     fake_path=fake_path,\n",
    "#     true_path=true_path,\n",
    "#     output_path=output_path,\n",
    "#     cache=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef22bc",
   "metadata": {},
   "source": [
    "## Examine the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157774ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema\n",
    "# processed_df.printSchema()\n",
    "\n",
    "# Show sample data\n",
    "# display(processed_df.select(\"text\", \"tokens\", \"filtered_tokens\", \"label\", \"location\", \"news_source\").limit(5))\n",
    "\n",
    "# Count records by label\n",
    "# display(processed_df.groupBy(\"label\").count().orderBy(\"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee232deb",
   "metadata": {},
   "source": [
    "## Pipeline API Approach\n",
    "\n",
    "An alternative approach is to use the Spark ML Pipeline API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac4be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def create_pipeline_api_approach(include_features=True):\n",
    "    \"\"\"\n",
    "    Creates a text processing pipeline using Spark ML Pipeline API.\n",
    "    \n",
    "    Args:\n",
    "        include_features (bool): Whether to include feature extraction steps\n",
    "        \n",
    "    Returns:\n",
    "        Pipeline: Spark ML Pipeline for text processing\n",
    "    \"\"\"\n",
    "    # Define transformers\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "    \n",
    "    # Create pipeline stages\n",
    "    stages = [tokenizer, remover]\n",
    "    \n",
    "    # Optionally add feature extraction\n",
    "    if include_features:\n",
    "        hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "        idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "        stages.extend([hashingTF, idf])\n",
    "    \n",
    "    # Create and return the pipeline\n",
    "    return Pipeline(stages=stages)\n",
    "\n",
    "# Example usage (commented out)\n",
    "# pipeline = create_pipeline_api_approach(include_features=True)\n",
    "# model = pipeline.fit(preprocessed_df)\n",
    "# processed_df = model.transform(preprocessed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc76e0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the integrated approach to text processing for fake news detection, combining data ingestion and preprocessing in a single phase. This optimized pipeline improves efficiency and reduces redundancy in the workflow.\n",
    "\n",
    "Key benefits of this approach:\n",
    "1. Reduced computation by eliminating redundant processing\n",
    "2. Improved memory efficiency through strategic caching and unpersisting\n",
    "3. Simplified workflow with fewer steps\n",
    "4. Enhanced performance in resource-constrained environments like Databricks Community Edition\n",
    "\n",
    "The processed data is now ready for feature engineering and model training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
