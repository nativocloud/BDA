{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b3b765",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Validation Utilities for Fake News Detection\n",
    "\n",
    "This module provides utilities for validating and cleaning all columns in the fake news dataset.\n",
    "It handles null values, blank fields, and malformed data, ensuring high data quality for downstream analysis.\n",
    "\n",
    "The implementation uses Spark's distributed processing capabilities to ensure scalability.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469d6f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, length, trim, when, lit, regexp_replace, udf, \n",
    "    lower, upper, initcap, isnan, count, sum, avg\n",
    ")\n",
    "from pyspark.sql.types import StringType, BooleanType, IntegerType, ArrayType\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d0e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    \"\"\"\n",
    "    A class for validating and cleaning data in a Spark DataFrame.\n",
    "    \n",
    "    This class provides methods for checking data quality, standardizing formats,\n",
    "    and handling problematic values across all columns in the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_title_length: int = 5, min_text_length: int = 50):\n",
    "        \"\"\"\n",
    "        Initialize the DataValidator.\n",
    "        \n",
    "        Args:\n",
    "            min_title_length (int): Minimum acceptable length for the title field.\n",
    "            min_text_length (int): Minimum acceptable length for the text field.\n",
    "        \"\"\"\n",
    "        self.min_title_length = min_title_length\n",
    "        self.min_text_length = min_text_length\n",
    "        \n",
    "        # Common patterns for problematic content\n",
    "        self.suspicious_patterns = [\n",
    "            r'<\\s*script.*?>.*?<\\s*/\\s*script\\s*>', # Script tags\n",
    "            r'<\\s*style.*?>.*?<\\s*/\\s*style\\s*>',   # Style tags\n",
    "            r'<\\s*iframe.*?>.*?<\\s*/\\s*iframe\\s*>', # iFrame tags\n",
    "            r'javascript:',                         # JavaScript protocol\n",
    "            r'data:',                               # Data URI scheme\n",
    "            r'[\\u0080-\\u00ff]{10,}',                # Long sequences of non-ASCII chars\n",
    "            r'(\\w)\\1{10,}'                          # Repeated characters (e.g., \"aaaaaaaaaa\")\n",
    "        ]\n",
    "    \n",
    "    def validate_text_fields(self, df: DataFrame, text_columns: List[str]) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Validate text fields in the DataFrame.\n",
    "        \n",
    "        This method checks for null values, minimum length, and suspicious patterns\n",
    "        in text fields, adding validation flag columns.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The input DataFrame.\n",
    "            text_columns (List[str]): List of text column names to validate.\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: The DataFrame with additional validation flag columns.\n",
    "        \"\"\"\n",
    "        result_df = df\n",
    "        \n",
    "        for column in text_columns:\n",
    "            # Check for null or empty values\n",
    "            result_df = result_df.withColumn(\n",
    "                f\"{column}_present\", \n",
    "                when(\n",
    "                    (col(column).isNull()) | (trim(col(column)) == \"\"),\n",
    "                    lit(False)\n",
    "                ).otherwise(lit(True))\n",
    "            )\n",
    "            \n",
    "            # Check for minimum length\n",
    "            min_length = self.min_title_length if \"title\" in column else self.min_text_length\n",
    "            result_df = result_df.withColumn(\n",
    "                f\"{column}_length_valid\",\n",
    "                when(\n",
    "                    col(f\"{column}_present\") & (length(trim(col(column))) >= min_length),\n",
    "                    lit(True)\n",
    "                ).otherwise(lit(False))\n",
    "            )\n",
    "            \n",
    "            # Check for suspicious patterns\n",
    "            has_suspicious_pattern_udf = udf(self._has_suspicious_pattern, BooleanType())\n",
    "            result_df = result_df.withColumn(\n",
    "                f\"{column}_content_valid\",\n",
    "                when(\n",
    "                    col(f\"{column}_present\") & (~has_suspicious_pattern_udf(col(column))),\n",
    "                    lit(True)\n",
    "                ).otherwise(lit(False))\n",
    "            )\n",
    "            \n",
    "            # Overall validation flag for this column\n",
    "            result_df = result_df.withColumn(\n",
    "                f\"{column}_valid\",\n",
    "                col(f\"{column}_present\") & \n",
    "                col(f\"{column}_length_valid\") & \n",
    "                col(f\"{column}_content_valid\")\n",
    "            )\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def standardize_text_fields(self, df: DataFrame, text_columns: List[str]) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Standardize text fields in the DataFrame.\n",
    "        \n",
    "        This method cleans and standardizes text fields by removing extra whitespace,\n",
    "        normalizing case, and removing problematic characters.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The input DataFrame.\n",
    "            text_columns (List[str]): List of text column names to standardize.\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: The DataFrame with standardized text columns.\n",
    "        \"\"\"\n",
    "        result_df = df\n",
    "        \n",
    "        for column in text_columns:\n",
    "            # Create standardized version of the column\n",
    "            std_column = f\"std_{column}\"\n",
    "            \n",
    "            # Clean whitespace and normalize case\n",
    "            result_df = result_df.withColumn(\n",
    "                std_column,\n",
    "                when(\n",
    "                    col(column).isNull(),\n",
    "                    None\n",
    "                ).otherwise(\n",
    "                    trim(regexp_replace(col(column), r'\\s+', ' '))\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Remove HTML tags\n",
    "            result_df = result_df.withColumn(\n",
    "                std_column,\n",
    "                regexp_replace(col(std_column), r'<[^>]+>', ' ')\n",
    "            )\n",
    "            \n",
    "            # Remove URLs\n",
    "            result_df = result_df.withColumn(\n",
    "                std_column,\n",
    "                regexp_replace(col(std_column), r'https?://\\S+', '[URL]')\n",
    "            )\n",
    "            \n",
    "            # Normalize case based on column type\n",
    "            if \"title\" in column:\n",
    "                # For titles, use title case\n",
    "                result_df = result_df.withColumn(\n",
    "                    std_column,\n",
    "                    initcap(col(std_column))\n",
    "                )\n",
    "            elif \"author\" in column or \"source\" in column:\n",
    "                # For author and source, use title case\n",
    "                result_df = result_df.withColumn(\n",
    "                    std_column,\n",
    "                    initcap(col(std_column))\n",
    "                )\n",
    "            else:\n",
    "                # For other text, leave as is\n",
    "                pass\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def validate_categorical_fields(self, df: DataFrame, categorical_columns: List[str]) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Validate categorical fields in the DataFrame.\n",
    "        \n",
    "        This method checks for null values and standardizes categorical values,\n",
    "        adding validation flag columns.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The input DataFrame.\n",
    "            categorical_columns (List[str]): List of categorical column names to validate.\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: The DataFrame with additional validation flag columns.\n",
    "        \"\"\"\n",
    "        result_df = df\n",
    "        \n",
    "        for column in categorical_columns:\n",
    "            # Check for null or empty values\n",
    "            result_df = result_df.withColumn(\n",
    "                f\"{column}_present\", \n",
    "                when(\n",
    "                    (col(column).isNull()) | (trim(col(column)) == \"\"),\n",
    "                    lit(False)\n",
    "                ).otherwise(lit(True))\n",
    "            )\n",
    "            \n",
    "            # Create standardized version of the column\n",
    "            std_column = f\"std_{column}\"\n",
    "            result_df = result_df.withColumn(\n",
    "                std_column,\n",
    "                when(\n",
    "                    col(column).isNull(),\n",
    "                    None\n",
    "                ).otherwise(\n",
    "                    trim(initcap(col(column)))\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Overall validation flag for this column\n",
    "            result_df = result_df.withColumn(\n",
    "                f\"{column}_valid\",\n",
    "                col(f\"{column}_present\")\n",
    "            )\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def calculate_data_quality_metrics(self, df: DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Calculate data quality metrics for the DataFrame.\n",
    "        \n",
    "        This method computes various metrics to assess the overall quality of the dataset,\n",
    "        including completeness, validity, and consistency.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The input DataFrame with validation columns.\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary of data quality metrics.\n",
    "        \"\"\"\n",
    "        # Get total number of rows\n",
    "        total_rows = df.count()\n",
    "        \n",
    "        # Initialize metrics dictionary\n",
    "        metrics = {\n",
    "            \"total_rows\": total_rows,\n",
    "            \"completeness\": {},\n",
    "            \"validity\": {}\n",
    "        }\n",
    "        \n",
    "        # Calculate completeness metrics for each column\n",
    "        for column in df.columns:\n",
    "            if f\"{column}_present\" in df.columns:\n",
    "                present_count = df.filter(col(f\"{column}_present\") == True).count()\n",
    "                completeness = present_count / total_rows if total_rows > 0 else 0\n",
    "                metrics[\"completeness\"][column] = completeness\n",
    "            \n",
    "            if f\"{column}_valid\" in df.columns:\n",
    "                valid_count = df.filter(col(f\"{column}_valid\") == True).count()\n",
    "                validity = valid_count / total_rows if total_rows > 0 else 0\n",
    "                metrics[\"validity\"][column] = validity\n",
    "        \n",
    "        # Calculate overall data quality score\n",
    "        if metrics[\"validity\"]:\n",
    "            metrics[\"overall_quality\"] = sum(metrics[\"validity\"].values()) / len(metrics[\"validity\"])\n",
    "        else:\n",
    "            metrics[\"overall_quality\"] = 0\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def filter_invalid_records(self, df: DataFrame, required_columns: List[str]) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Filter out invalid records from the DataFrame.\n",
    "        \n",
    "        This method removes records that have invalid values in required columns.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The input DataFrame with validation columns.\n",
    "            required_columns (List[str]): List of column names that must be valid.\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: The filtered DataFrame with only valid records.\n",
    "        \"\"\"\n",
    "        result_df = df\n",
    "        \n",
    "        for column in required_columns:\n",
    "            if f\"{column}_valid\" in df.columns:\n",
    "                result_df = result_df.filter(col(f\"{column}_valid\") == True)\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def process_dataframe(self, df: DataFrame) -> Tuple[DataFrame, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process the entire DataFrame for data quality.\n",
    "        \n",
    "        This method combines validation, standardization, and filtering\n",
    "        into a single pipeline.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): The input DataFrame.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[DataFrame, Dict[str, Any]]: The processed DataFrame and data quality metrics.\n",
    "        \"\"\"\n",
    "        # Define column groups\n",
    "        text_columns = [\"title\", \"text\"]\n",
    "        categorical_columns = [\"author\", \"source\"]\n",
    "        \n",
    "        # Validate text fields\n",
    "        result_df = self.validate_text_fields(df, text_columns)\n",
    "        \n",
    "        # Standardize text fields\n",
    "        result_df = self.standardize_text_fields(result_df, text_columns + categorical_columns)\n",
    "        \n",
    "        # Validate categorical fields\n",
    "        result_df = self.validate_categorical_fields(result_df, categorical_columns)\n",
    "        \n",
    "        # Calculate data quality metrics\n",
    "        metrics = self.calculate_data_quality_metrics(result_df)\n",
    "        \n",
    "        # Add overall record validity flag\n",
    "        result_df = result_df.withColumn(\n",
    "            \"record_valid\",\n",
    "            col(\"title_valid\") & col(\"text_valid\")\n",
    "        )\n",
    "        \n",
    "        return result_df, metrics\n",
    "    \n",
    "    def _has_suspicious_pattern(self, text: Optional[str]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if text contains suspicious patterns.\n",
    "        \n",
    "        This is a Python UDF that checks for potentially problematic content.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text to check.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if suspicious patterns are found, False otherwise.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return False\n",
    "        \n",
    "        for pattern in self.suspicious_patterns:\n",
    "            if re.search(pattern, text, re.IGNORECASE | re.DOTALL):\n",
    "                return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a609a4c7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def validate_and_clean_data(df: DataFrame) -> Tuple[DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Validate and clean all columns in a DataFrame.\n",
    "    \n",
    "    This function checks for data quality issues, standardizes formats,\n",
    "    and handles problematic values across all columns.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[DataFrame, Dict[str, Any]]: The processed DataFrame and data quality metrics.\n",
    "    \"\"\"\n",
    "    validator = DataValidator()\n",
    "    return validator.process_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afebe7b",
   "metadata": {},
   "source": [
    "Example usage:\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DataValidation\").getOrCreate()\n",
    "\n",
    "# Sample data with various issues\n",
    "data = [\n",
    "    (1, \"Valid Title\", \"This is a valid text content with sufficient length for analysis.\", \"John Doe\", \"Reliable Source\"),\n",
    "    (2, \"\", \"Text without a title\", \"Jane Smith\", \"Another Source\"),\n",
    "    (3, \"Title without text\", \"\", \"Unknown\", \"\"),\n",
    "    (4, \"Title with suspicious content\", \"<script>alert('XSS')</script>\", \"Hacker\", \"Suspicious Source\"),\n",
    "    (5, \"Very short\", \"Too short\", \"\", \"Source\"),\n",
    "    (6, None, None, None, None)\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"title\", \"text\", \"author\", \"source\"])\n",
    "\n",
    "# Validate and clean the data\n",
    "processed_df, metrics = validate_and_clean_data(df)\n",
    "\n",
    "# Show the results\n",
    "processed_df.select(\"id\", \"title\", \"std_title\", \"title_valid\", \"text\", \"std_text\", \"text_valid\", \"record_valid\").show(truncate=False)\n",
    "\n",
    "# Print data quality metrics\n",
    "print(\"Data Quality Metrics:\")\n",
    "print(f\"Total rows: {metrics['total_rows']}\")\n",
    "print(f\"Overall quality score: {metrics['overall_quality']:.2f}\")\n",
    "print(\"\\nCompleteness:\")\n",
    "for column, score in metrics['completeness'].items():\n",
    "    print(f\"  {column}: {score:.2f}\")\n",
    "print(\"\\nValidity:\")\n",
    "for column, score in metrics['validity'].items():\n",
    "    print(f\"  {column}: {score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
