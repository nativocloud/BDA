{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4805d12a",
   "metadata": {},
   "source": [
    "# Fake News Detection: Data Preprocessing\n",
    "\n",
    "This notebook contains all the necessary code for preprocessing data in the fake news detection project. The code is organized into independent functions, without dependencies on external modules or classes, to facilitate execution in Databricks Community Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee459a8",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd8ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, when, isnan, isnull, desc, expr, lit, trim, \n",
    "    length, lower, upper, regexp_replace, udf, to_date, year, \n",
    "    month, dayofmonth, dayofweek, date_format, concat\n",
    ")\n",
    "from pyspark.sql.types import StringType, BooleanType, IntegerType, DateType, ArrayType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# Import NLTK for text processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK resources\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4faff3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session optimized for Databricks Community Edition\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection_Preprocessing\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ce20a",
   "metadata": {},
   "source": [
    "## Reusable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147d966",
   "metadata": {},
   "source": [
    "### Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a23f5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_data_from_hive(fake_table_name=\"fake\", true_table_name=\"real\"):\n",
    "    \"\"\"\n",
    "    Load data from Hive tables.\n",
    "    \n",
    "    Args:\n",
    "        fake_table_name (str): Name of the Hive table with fake news\n",
    "        true_table_name (str): Name of the Hive table with real news\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (real_df, fake_df) DataFrames with loaded data\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from Hive tables '{true_table_name}' and '{fake_table_name}'...\")\n",
    "    \n",
    "    # Check if tables exist\n",
    "    tables = [row.tableName for row in spark.sql(\"SHOW TABLES\").collect()]\n",
    "    \n",
    "    if true_table_name not in tables or fake_table_name not in tables:\n",
    "        raise ValueError(f\"Hive tables '{true_table_name}' and/or '{fake_table_name}' do not exist\")\n",
    "    \n",
    "    # Load data from Hive tables\n",
    "    real_df = spark.table(true_table_name)\n",
    "    fake_df = spark.table(fake_table_name)\n",
    "    \n",
    "    # Register as temporary views for SQL queries\n",
    "    real_df.createOrReplaceTempView(\"real_news\")\n",
    "    fake_df.createOrReplaceTempView(\"fake_news\")\n",
    "    \n",
    "    # Display information about DataFrames\n",
    "    print(f\"Real news loaded: {real_df.count()} records\")\n",
    "    print(f\"Fake news loaded: {fake_df.count()} records\")\n",
    "    \n",
    "    return real_df, fake_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6e71b",
   "metadata": {},
   "source": [
    "### Data Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6600fc1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    \"\"\"\n",
    "    Check for missing values in all columns of a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to check\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with missing value counts for each column\n",
    "    \"\"\"\n",
    "    # Create expressions for each column to count nulls and empty strings\n",
    "    exprs = []\n",
    "    for col_name in df.columns:\n",
    "        # Count nulls\n",
    "        exprs.append(count(when(col(col_name).isNull() | \n",
    "                               (col(col_name) == \"\") | \n",
    "                               isnan(col_name), \n",
    "                               col_name)).alias(col_name))\n",
    "    \n",
    "    # Apply expressions to get missing value counts\n",
    "    missing_values = df.select(*exprs)\n",
    "    \n",
    "    return missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7214cf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_duplicates(df):\n",
    "    \"\"\"\n",
    "    Check for duplicate records in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to check\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of duplicate records\n",
    "    \"\"\"\n",
    "    # Count total records\n",
    "    total_records = df.count()\n",
    "    \n",
    "    # Count distinct records\n",
    "    distinct_records = df.distinct().count()\n",
    "    \n",
    "    # Calculate duplicates\n",
    "    duplicates = total_records - distinct_records\n",
    "    \n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e75fe",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def has_suspicious_pattern(text):\n",
    "    \"\"\"\n",
    "    Check if text contains suspicious patterns.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to check\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if suspicious patterns are found, False otherwise\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    # Common patterns for problematic content\n",
    "    suspicious_patterns = [\n",
    "        r'<\\s*script.*?>.*?<\\s*/\\s*script\\s*>', # Script tags\n",
    "        r'<\\s*style.*?>.*?<\\s*/\\s*style\\s*>',   # Style tags\n",
    "        r'<\\s*iframe.*?>.*?<\\s*/\\s*iframe\\s*>', # iFrame tags\n",
    "        r'javascript:',                         # JavaScript protocol\n",
    "        r'data:',                               # Data URI scheme\n",
    "        r'[\\u0080-\\u00ff]{10,}',                # Long sequences of non-ASCII chars\n",
    "        r'(\\w)\\1{10,}'                          # Repeated characters (e.g., \"aaaaaaaaaa\")\n",
    "    ]\n",
    "    \n",
    "    for pattern in suspicious_patterns:\n",
    "        if re.search(pattern, text, re.IGNORECASE | re.DOTALL):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cdb10",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def validate_text_fields(df, text_columns, min_title_length=5, min_text_length=50):\n",
    "    \"\"\"\n",
    "    Validate text fields in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame\n",
    "        text_columns (list): List of text column names to validate\n",
    "        min_title_length (int): Minimum acceptable length for title fields\n",
    "        min_text_length (int): Minimum acceptable length for text fields\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with additional validation flag columns\n",
    "    \"\"\"\n",
    "    result_df = df\n",
    "    \n",
    "    for column in text_columns:\n",
    "        # Check for null or empty values\n",
    "        result_df = result_df.withColumn(\n",
    "            f\"{column}_present\", \n",
    "            when(\n",
    "                (col(column).isNull()) | (trim(col(column)) == \"\"),\n",
    "                lit(False)\n",
    "            ).otherwise(lit(True))\n",
    "        )\n",
    "        \n",
    "        # Check for minimum length\n",
    "        min_length = min_title_length if \"title\" in column else min_text_length\n",
    "        result_df = result_df.withColumn(\n",
    "            f\"{column}_length_valid\",\n",
    "            when(\n",
    "                col(f\"{column}_present\") & (length(trim(col(column))) >= min_length),\n",
    "                lit(True)\n",
    "            ).otherwise(lit(False))\n",
    "        )\n",
    "        \n",
    "        # Check for suspicious patterns\n",
    "        has_suspicious_pattern_udf = udf(has_suspicious_pattern, BooleanType())\n",
    "        result_df = result_df.withColumn(\n",
    "            f\"{column}_content_valid\",\n",
    "            when(\n",
    "                col(f\"{column}_present\") & (~has_suspicious_pattern_udf(col(column))),\n",
    "                lit(True)\n",
    "            ).otherwise(lit(False))\n",
    "        )\n",
    "        \n",
    "        # Overall validation flag for this column\n",
    "        result_df = result_df.withColumn(\n",
    "            f\"{column}_valid\",\n",
    "            col(f\"{column}_present\") & \n",
    "            col(f\"{column}_length_valid\") & \n",
    "            col(f\"{column}_content_valid\")\n",
    "        )\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6749ca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def standardize_text_fields(df, text_columns):\n",
    "    \"\"\"\n",
    "    Standardize text fields in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame\n",
    "        text_columns (list): List of text column names to standardize\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with standardized text columns\n",
    "    \"\"\"\n",
    "    result_df = df\n",
    "    \n",
    "    for column in text_columns:\n",
    "        # Create standardized version of the column\n",
    "        std_column = f\"std_{column}\"\n",
    "        \n",
    "        # Clean whitespace and normalize case\n",
    "        result_df = result_df.withColumn(\n",
    "            std_column,\n",
    "            when(\n",
    "                col(column).isNull(),\n",
    "                None\n",
    "            ).otherwise(\n",
    "                trim(regexp_replace(col(column), r'\\s+', ' '))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        result_df = result_df.withColumn(\n",
    "            std_column,\n",
    "            regexp_replace(col(std_column), r'<[^>]+>', ' ')\n",
    "        )\n",
    "        \n",
    "        # Remove URLs\n",
    "        result_df = result_df.withColumn(\n",
    "            std_column,\n",
    "            regexp_replace(col(std_column), r'https?://\\S+', '[URL]')\n",
    "        )\n",
    "        \n",
    "        # Normalize case based on column type\n",
    "        if \"title\" in column:\n",
    "            # For titles, use title case\n",
    "            result_df = result_df.withColumn(\n",
    "                std_column,\n",
    "                expr(\"initcap(\" + std_column + \")\")\n",
    "            )\n",
    "        elif \"author\" in column or \"source\" in column:\n",
    "            # For author and source, use title case\n",
    "            result_df = result_df.withColumn(\n",
    "                std_column,\n",
    "                expr(\"initcap(\" + std_column + \")\")\n",
    "            )\n",
    "        else:\n",
    "            # For other text, leave as is\n",
    "            pass\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ad99f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_data_quality_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate data quality metrics for the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame with validation columns\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary of data quality metrics\n",
    "    \"\"\"\n",
    "    # Get total number of rows\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    # Initialize metrics dictionary\n",
    "    metrics = {\n",
    "        \"total_rows\": total_rows,\n",
    "        \"completeness\": {},\n",
    "        \"validity\": {}\n",
    "    }\n",
    "    \n",
    "    # Calculate completeness metrics for each column\n",
    "    for column in df.columns:\n",
    "        if f\"{column}_present\" in df.columns:\n",
    "            present_count = df.filter(col(f\"{column}_present\") == True).count()\n",
    "            completeness = present_count / total_rows if total_rows > 0 else 0\n",
    "            metrics[\"completeness\"][column] = completeness\n",
    "        \n",
    "        if f\"{column}_valid\" in df.columns:\n",
    "            valid_count = df.filter(col(f\"{column}_valid\") == True).count()\n",
    "            validity = valid_count / total_rows if total_rows > 0 else 0\n",
    "            metrics[\"validity\"][column] = validity\n",
    "    \n",
    "    # Calculate overall data quality score\n",
    "    if metrics[\"validity\"]:\n",
    "        metrics[\"overall_quality\"] = sum(metrics[\"validity\"].values()) / len(metrics[\"validity\"])\n",
    "    else:\n",
    "        metrics[\"overall_quality\"] = 0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569de7c5",
   "metadata": {},
   "source": [
    "### Date Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496767b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def parse_complex_date(date_str):\n",
    "    \"\"\"\n",
    "    Parse complex date formats that aren't handled by standard Spark functions.\n",
    "    \n",
    "    Args:\n",
    "        date_str (str): The date string to parse\n",
    "        \n",
    "    Returns:\n",
    "        datetime: The parsed date as a datetime object, or None if parsing fails\n",
    "    \"\"\"\n",
    "    if not date_str:\n",
    "        return None\n",
    "    \n",
    "    # Clean the date string\n",
    "    date_str = date_str.strip()\n",
    "    \n",
    "    # Try common Python datetime formats\n",
    "    formats = [\n",
    "        \"%B %d, %Y\",      # December 25, 2017\n",
    "        \"%b %d, %Y\",      # Dec 25, 2017\n",
    "        \"%Y-%m-%d\",       # 2017-12-25\n",
    "        \"%m/%d/%Y\",       # 12/25/2017\n",
    "        \"%d/%m/%Y\",       # 25/12/2017\n",
    "        \"%Y/%m/%d\",       # 2017/12/25\n",
    "        \"%m-%d-%Y\",       # 12-25-2017\n",
    "        \"%d-%m-%Y\"        # 25-12-2017\n",
    "    ]\n",
    "    \n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # Try to extract date components using regex\n",
    "    # This handles formats like \"25th December 2017\" or \"December 2017\"\n",
    "    try:\n",
    "        # Extract year\n",
    "        year_match = re.search(r'\\b(19|20)\\d{2}\\b', date_str)\n",
    "        if not year_match:\n",
    "            return None\n",
    "        year = int(year_match.group(0))\n",
    "        \n",
    "        # Extract month\n",
    "        month = None\n",
    "        month_names = {\n",
    "            \"january\": 1, \"february\": 2, \"march\": 3, \"april\": 4,\n",
    "            \"may\": 5, \"june\": 6, \"july\": 7, \"august\": 8,\n",
    "            \"september\": 9, \"october\": 10, \"november\": 11, \"december\": 12,\n",
    "            \"jan\": 1, \"feb\": 2, \"mar\": 3, \"apr\": 4, \"jun\": 6,\n",
    "            \"jul\": 7, \"aug\": 8, \"sep\": 9, \"oct\": 10, \"nov\": 11, \"dec\": 12\n",
    "        }\n",
    "        \n",
    "        for name, num in month_names.items():\n",
    "            if name in date_str.lower():\n",
    "                month = num\n",
    "                break\n",
    "        \n",
    "        if not month:\n",
    "            # Try to find numeric month\n",
    "            month_match = re.search(r'\\b(0?[1-9]|1[0-2])\\b', date_str)\n",
    "            if month_match:\n",
    "                month = int(month_match.group(0))\n",
    "            else:\n",
    "                month = 1  # Default to January if no month found\n",
    "        \n",
    "        # Extract day\n",
    "        day = None\n",
    "        day_match = re.search(r'\\b(0?[1-9]|[12][0-9]|3[01])(st|nd|rd|th)?\\b', date_str)\n",
    "        if day_match:\n",
    "            day = int(re.sub(r'(st|nd|rd|th)', '', day_match.group(0)))\n",
    "        else:\n",
    "            day = 1  # Default to 1st if no day found\n",
    "        \n",
    "        return datetime(year, month, day)\n",
    "    \n",
    "    except (ValueError, TypeError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70f37c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def standardize_date(df, date_column=\"publish_date\"):\n",
    "    \"\"\"\n",
    "    Standardize the date format in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame with a date column\n",
    "        date_column (str): The name of the date column to process\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with a standardized date column\n",
    "    \"\"\"\n",
    "    # Common date formats to try when parsing dates\n",
    "    date_formats = [\n",
    "        \"MMMM d, yyyy\",       # December 25, 2017\n",
    "        \"MMM d, yyyy\",        # Dec 25, 2017\n",
    "        \"yyyy-MM-dd\",         # 2017-12-25\n",
    "        \"MM/dd/yyyy\",         # 12/25/2017\n",
    "        \"dd/MM/yyyy\",         # 25/12/2017\n",
    "        \"yyyy/MM/dd\",         # 2017/12/25\n",
    "        \"MM-dd-yyyy\",         # 12-25-2017\n",
    "        \"dd-MM-yyyy\"          # 25-12-2017\n",
    "    ]\n",
    "    \n",
    "    # Start with the original DataFrame\n",
    "    result_df = df.withColumn(\"std_date\", lit(None).cast(DateType()))\n",
    "    \n",
    "    # Try to parse the date using each format\n",
    "    for date_format in date_formats:\n",
    "        result_df = result_df.withColumn(\n",
    "            \"std_date\",\n",
    "            when(\n",
    "                col(\"std_date\").isNull(),\n",
    "                to_date(col(date_column), date_format)\n",
    "            ).otherwise(col(\"std_date\"))\n",
    "        )\n",
    "    \n",
    "    # For any remaining null values, use a more complex approach with UDFs\n",
    "    # This is a fallback for unusual formats\n",
    "    parse_complex_date_udf = udf(parse_complex_date, DateType())\n",
    "    \n",
    "    result_df = result_df.withColumn(\n",
    "        \"std_date\",\n",
    "        when(\n",
    "            col(\"std_date\").isNull() & col(date_column).isNotNull(),\n",
    "            parse_complex_date_udf(col(date_column))\n",
    "        ).otherwise(col(\"std_date\"))\n",
    "    )\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576ade4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_date_features(df):\n",
    "    \"\"\"\n",
    "    Extract useful date features from the standardized date column.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame with a standardized date column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with additional date feature columns\n",
    "    \"\"\"\n",
    "    # Extract date components\n",
    "    result_df = df.withColumn(\"year\", year(col(\"std_date\")))\n",
    "    result_df = result_df.withColumn(\"month\", month(col(\"std_date\")))\n",
    "    result_df = result_df.withColumn(\"day\", dayofmonth(col(\"std_date\")))\n",
    "    result_df = result_df.withColumn(\"day_of_week\", dayofweek(col(\"std_date\")))\n",
    "    \n",
    "    # Create YYYYMMDD format\n",
    "    result_df = result_df.withColumn(\n",
    "        \"date_yyyymmdd\", \n",
    "        date_format(col(\"std_date\"), \"yyyyMMdd\").cast(IntegerType())\n",
    "    )\n",
    "    \n",
    "    # Add validation flag\n",
    "    result_df = result_df.withColumn(\n",
    "        \"date_valid\",\n",
    "        col(\"std_date\").isNotNull()\n",
    "    )\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262da13",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_date_column(df, date_column=\"publish_date\"):\n",
    "    \"\"\"\n",
    "    Process the date column in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame with a date column\n",
    "        date_column (str): The name of the date column to process\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: The DataFrame with processed date columns\n",
    "    \"\"\"\n",
    "    # Standardize the date\n",
    "    result_df = standardize_date(df, date_column)\n",
    "    \n",
    "    # Extract date features\n",
    "    result_df = extract_date_features(result_df)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45492e1",
   "metadata": {},
   "source": [
    "### Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7c41a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text, config=None):\n",
    "    \"\"\"\n",
    "    Apply all configured preprocessing steps to a text string.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to preprocess\n",
    "        config (dict): Configuration parameters for preprocessing\n",
    "            \n",
    "    Returns:\n",
    "        str: The preprocessed text\n",
    "    \"\"\"\n",
    "    # Default configuration\n",
    "    default_config = {\n",
    "        'remove_stopwords': True,\n",
    "        'stemming': False,\n",
    "        'lemmatization': True,\n",
    "        'lowercase': True,\n",
    "        'remove_punctuation': True,\n",
    "        'remove_numbers': False,\n",
    "        'remove_urls': True,\n",
    "        'remove_twitter_handles': True,\n",
    "        'remove_hashtags': False,\n",
    "        'convert_hashtags': True,\n",
    "        'remove_special_chars': True,\n",
    "        'normalize_whitespace': True,\n",
    "        'max_text_length': 500,  # 0 means no limit\n",
    "        'language': 'english'\n",
    "    }\n",
    "    \n",
    "    # Use provided config or default\n",
    "    if config is None:\n",
    "        config = default_config\n",
    "    \n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Initialize tools based on configuration\n",
    "    if config['remove_stopwords']:\n",
    "        stop_words = set(stopwords.words(config['language']))\n",
    "    \n",
    "    if config['stemming']:\n",
    "        stemmer = PorterStemmer()\n",
    "        \n",
    "    if config['lemmatization']:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    if config['normalize_whitespace']:\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove URLs (enhanced pattern for Twitter URLs)\n",
    "    if config['remove_urls']:\n",
    "        text = re.sub(r'https?://t\\.co/\\w+|http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove Twitter handles\n",
    "    if config['remove_twitter_handles']:\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Handle hashtags\n",
    "    if config['remove_hashtags']:\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "    elif config['convert_hashtags']:\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Remove special characters and emojis\n",
    "    if config['remove_special_chars']:\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    if config['lowercase']:\n",
    "        text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    if config['remove_punctuation']:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove numbers\n",
    "    if config['remove_numbers']:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Limit text length if configured\n",
    "    if config['max_text_length'] > 0 and len(tokens) > config['max_text_length']:\n",
    "        tokens = tokens[:config['max_text_length']]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if config['remove_stopwords']:\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming\n",
    "    if config['stemming']:\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    if config['lemmatization']:\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301c5b9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_spark_df(spark_df, text_column, title_column=None, combine_title_text=False, config=None):\n",
    "    \"\"\"\n",
    "    Preprocess text in a Spark DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        spark_df (DataFrame): The input Spark DataFrame\n",
    "        text_column (str): The name of the column containing text to preprocess\n",
    "        title_column (str, optional): The name of the column containing title text\n",
    "        combine_title_text (bool): Whether to combine title and text for preprocessing\n",
    "        config (dict, optional): Configuration parameters for preprocessing\n",
    "            \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with preprocessed text in a new column named 'processed_text'\n",
    "    \"\"\"\n",
    "    # Register UDF for preprocessing\n",
    "    preprocess_udf = udf(lambda text: preprocess_text(text, config), StringType())\n",
    "    \n",
    "    # Create a copy to work with\n",
    "    result_df = spark_df\n",
    "    \n",
    "    # Trim whitespace from string columns\n",
    "    for column in result_df.schema.fields:\n",
    "        if isinstance(column.dataType, StringType):\n",
    "            result_df = result_df.withColumn(column.name, trim(col(column.name)))\n",
    "    \n",
    "    # Drop the subject column if it exists (with explanation in the notebook)\n",
    "    if 'subject' in result_df.columns:\n",
    "        result_df = result_df.drop('subject')\n",
    "    \n",
    "    # Combine title and text if requested\n",
    "    if combine_title_text and title_column and title_column in result_df.columns:\n",
    "        result_df = result_df.withColumn(\n",
    "            'combined_text', \n",
    "            concat(col(title_column).cast(StringType()).fillna(''), lit(' '), col(text_column).cast(StringType()).fillna(''))\n",
    "        )\n",
    "        result_df = result_df.withColumn('processed_text', preprocess_udf(col('combined_text')))\n",
    "    else:\n",
    "        result_df = result_df.withColumn('processed_text', preprocess_udf(col(text_column)))\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5555cd21",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text, config=None):\n",
    "    \"\"\"\n",
    "    Tokenize text into words.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to tokenize\n",
    "        config (dict, optional): Configuration parameters for preprocessing\n",
    "            \n",
    "    Returns:\n",
    "        list: List of tokens\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # First apply preprocessing to get clean text\n",
    "    clean_text = preprocess_text(text, config)\n",
    "    \n",
    "    # Tokenize the clean text\n",
    "    tokens = word_tokenize(clean_text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be805e15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def tokenize_spark_df(spark_df, text_column, config=None):\n",
    "    \"\"\"\n",
    "    Tokenize text in a Spark DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        spark_df (DataFrame): The input Spark DataFrame\n",
    "        text_column (str): The name of the column containing text to tokenize\n",
    "        config (dict, optional): Configuration parameters for preprocessing\n",
    "            \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with tokenized text in a new column named 'tokens'\n",
    "    \"\"\"\n",
    "    # Register UDF for tokenization\n",
    "    tokenize_udf = udf(lambda text: tokenize_text(text, config), ArrayType(StringType()))\n",
    "    \n",
    "    # Apply tokenization to the specified column\n",
    "    tokenized_df = spark_df.withColumn('tokens', tokenize_udf(col(text_column)))\n",
    "    \n",
    "    return tokenized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d445b944",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_quoted_content(text):\n",
    "    \"\"\"\n",
    "    Extract content within quotation marks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text\n",
    "        \n",
    "    Returns:\n",
    "        list: List of quoted content\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Find all quoted content\n",
    "    quoted_content = re.findall(r'\"([^\"]*)\"', text)\n",
    "    \n",
    "    return quoted_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc38cf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_quoted_content_spark_df(spark_df, text_column):\n",
    "    \"\"\"\n",
    "    Extract quoted content from text in a Spark DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        spark_df (DataFrame): The input Spark DataFrame\n",
    "        text_column (str): The name of the column containing text\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with quoted content in a new column named 'quoted_content'\n",
    "    \"\"\"\n",
    "    # Register UDF for extracting quoted content\n",
    "    extract_quoted_udf = udf(extract_quoted_content, ArrayType(StringType()))\n",
    "    \n",
    "    # Apply extraction to the specified column\n",
    "    result_df = spark_df.withColumn('quoted_content', extract_quoted_udf(col(text_column)))\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7558fc",
   "metadata": {},
   "source": [
    "### Dataset Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f0f475",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_dataset_characteristics(df, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Analyze dataset characteristics to inform preprocessing decisions.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame\n",
    "        sample_size (int): Number of rows to sample\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of dataset characteristics\n",
    "    \"\"\"\n",
    "    # Sample the data\n",
    "    sample_df = df.limit(sample_size)\n",
    "    \n",
    "    # Collect basic statistics\n",
    "    stats = {\n",
    "        \"total_rows\": df.count(),\n",
    "        \"columns\": df.columns,\n",
    "        \"sample_size\": sample_size\n",
    "    }\n",
    "    \n",
    "    # Check for subject column distribution\n",
    "    if \"subject\" in df.columns:\n",
    "        subject_counts = df.groupBy(\"subject\").count().collect()\n",
    "        stats[\"subject_distribution\"] = {row[\"subject\"]: row[\"count\"] for row in subject_counts}\n",
    "    \n",
    "    # Check for URL presence in text\n",
    "    if \"text\" in df.columns:\n",
    "        # Register UDF to check for URLs\n",
    "        has_url_udf = udf(lambda text: 1 if re.search(r'http\\S+|www\\S+|https\\S+', text) else 0, StringType())\n",
    "        url_count = sample_df.withColumn(\"has_url\", has_url_udf(col(\"text\"))).filter(col(\"has_url\") == 1).count()\n",
    "        stats[\"url_presence_percentage\"] = (url_count / sample_size) * 100\n",
    "    \n",
    "    # Check for Twitter handles\n",
    "    if \"text\" in df.columns:\n",
    "        has_handle_udf = udf(lambda text: 1 if re.search(r'@\\w+', text) else 0, StringType())\n",
    "        handle_count = sample_df.withColumn(\"has_handle\", has_handle_udf(col(\"text\"))).filter(col(\"has_handle\") == 1).count()\n",
    "        stats[\"twitter_handle_percentage\"] = (handle_count / sample_size) * 100\n",
    "    \n",
    "    # Check for hashtags\n",
    "    if \"text\" in df.columns:\n",
    "        has_hashtag_udf = udf(lambda text: 1 if re.search(r'#\\w+', text) else 0, StringType())\n",
    "        hashtag_count = sample_df.withColumn(\"has_hashtag\", has_hashtag_udf(col(\"text\"))).filter(col(\"has_hashtag\") == 1).count()\n",
    "        stats[\"hashtag_percentage\"] = (hashtag_count / sample_size) * 100\n",
    "    \n",
    "    # Check text length distribution\n",
    "    if \"text\" in df.columns:\n",
    "        text_length_df = sample_df.withColumn(\"text_length\", length(col(\"text\")))\n",
    "        text_length_stats = text_length_df.select(\"text_length\").summary(\"min\", \"25%\", \"mean\", \"75%\", \"max\").collect()\n",
    "        stats[\"text_length\"] = {row[\"summary\"]: float(row[\"text_length\"]) for row in text_length_stats}\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"Dataset Characteristics:\")\n",
    "    print(f\"Total rows: {stats['total_rows']}\")\n",
    "    print(f\"Columns: {stats['columns']}\")\n",
    "    \n",
    "    if \"subject_distribution\" in stats:\n",
    "        print(\"\\nSubject Distribution:\")\n",
    "        for subject, count in stats[\"subject_distribution\"].items():\n",
    "            print(f\"  {subject}: {count}\")\n",
    "    \n",
    "    if \"url_presence_percentage\" in stats:\n",
    "        print(f\"\\nURL presence: {stats['url_presence_percentage']:.2f}%\")\n",
    "    \n",
    "    if \"twitter_handle_percentage\" in stats:\n",
    "        print(f\"Twitter handle presence: {stats['twitter_handle_percentage']:.2f}%\")\n",
    "    \n",
    "    if \"hashtag_percentage\" in stats:\n",
    "        print(f\"Hashtag presence: {stats['hashtag_percentage']:.2f}%\")\n",
    "    \n",
    "    if \"text_length\" in stats:\n",
    "        print(\"\\nText Length Statistics:\")\n",
    "        for stat, value in stats[\"text_length\"].items():\n",
    "            print(f\"  {stat}: {value}\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8c5e9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def contains_url(text):\n",
    "    \"\"\"\n",
    "    Check if text contains URLs.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to check\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if URLs are found, False otherwise\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return False\n",
    "    url_pattern = r'https?://t\\.co/\\w+|http\\S+|www\\S+|https\\S+'\n",
    "    return bool(re.search(url_pattern, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b5d5b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def contains_twitter_handle(text):\n",
    "    \"\"\"\n",
    "    Check if text contains Twitter handles.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to check\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if Twitter handles are found, False otherwise\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return False\n",
    "    pattern = r'@\\w+'\n",
    "    return bool(re.search(pattern, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d2694",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def contains_hashtag(text):\n",
    "    \"\"\"\n",
    "    Check if text contains hashtags.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to check\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if hashtags are found, False otherwise\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return False\n",
    "    pattern = r'#\\w+'\n",
    "    return bool(re.search(pattern, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419f823",
   "metadata": {},
   "source": [
    "### Data Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a7b14",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_to_parquet(df, path, partition_by=None):\n",
    "    \"\"\"\n",
    "    Save a DataFrame in Parquet format.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        path (str): Path where to save the DataFrame\n",
    "        partition_by (str): Column to partition by (optional)\n",
    "    \"\"\"\n",
    "    print(f\"Saving DataFrame to {path}...\")\n",
    "    \n",
    "    writer = df.write.mode(\"overwrite\")\n",
    "    \n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(partition_by)\n",
    "    \n",
    "    writer.parquet(path)\n",
    "    print(f\"DataFrame saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b383b26",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_to_hive_table(df, table_name, partition_by=None):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a Hive table.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        table_name (str): Name of the Hive table to create or replace\n",
    "        partition_by (str): Column to partition by (optional)\n",
    "    \"\"\"\n",
    "    print(f\"Saving DataFrame to Hive table {table_name}...\")\n",
    "    \n",
    "    writer = df.write.mode(\"overwrite\").format(\"parquet\")\n",
    "    \n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(partition_by)\n",
    "    \n",
    "    writer.saveAsTable(table_name)\n",
    "    print(f\"DataFrame saved to Hive table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b943a90",
   "metadata": {},
   "source": [
    "## Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984509c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_and_save_data(output_dir=\"dbfs:/FileStore/fake_news_detection/preprocessed_data\",\n",
    "                            fake_table_name=\"fake\", \n",
    "                            true_table_name=\"real\",\n",
    "                            create_tables=True):\n",
    "    \"\"\"\n",
    "    Preprocess and save fake and real news data.\n",
    "    \n",
    "    This complete pipeline loads data from Hive tables, validates and cleans the data,\n",
    "    preprocesses text, and saves the results in Parquet format and as Hive tables.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Directory to save processed data\n",
    "        fake_table_name (str): Name of the Hive table with fake news\n",
    "        true_table_name (str): Name of the Hive table with real news\n",
    "        create_tables (bool): Whether to create Hive tables\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with references to processed DataFrames\n",
    "    \"\"\"\n",
    "    print(\"Starting data preprocessing pipeline...\")\n",
    "    \n",
    "    # 1. Load data from Hive tables\n",
    "    real_df, fake_df = load_data_from_hive(fake_table_name, true_table_name)\n",
    "    \n",
    "    # 2. Combine datasets with labels\n",
    "    real_with_label = real_df.withColumn(\"label\", lit(1))\n",
    "    fake_with_label = fake_df.withColumn(\"label\", lit(0))\n",
    "    combined_df = real_with_label.union(fake_with_label)\n",
    "    \n",
    "    # 3. Analyze dataset characteristics\n",
    "    analyze_dataset_characteristics(combined_df)\n",
    "    \n",
    "    # 4. Validate text fields\n",
    "    text_columns = [\"title\", \"text\"]\n",
    "    validated_df = validate_text_fields(combined_df, text_columns)\n",
    "    \n",
    "    # 5. Process date column if it exists\n",
    "    if \"date\" in combined_df.columns or \"publish_date\" in combined_df.columns:\n",
    "        date_column = \"date\" if \"date\" in combined_df.columns else \"publish_date\"\n",
    "        validated_df = process_date_column(validated_df, date_column)\n",
    "    \n",
    "    # 6. Preprocess text\n",
    "    preprocessed_df = preprocess_spark_df(validated_df, \"text\", \"title\", combine_title_text=True)\n",
    "    \n",
    "    # 7. Tokenize text\n",
    "    tokenized_df = tokenize_spark_df(preprocessed_df, \"processed_text\")\n",
    "    \n",
    "    # 8. Extract quoted content\n",
    "    final_df = extract_quoted_content_spark_df(tokenized_df, \"text\")\n",
    "    \n",
    "    # 9. Save preprocessed data\n",
    "    preprocessed_path = f\"{output_dir}/preprocessed_news.parquet\"\n",
    "    save_to_parquet(final_df, preprocessed_path, partition_by=\"label\")\n",
    "    \n",
    "    # 10. Save to Hive table for easier access\n",
    "    if create_tables:\n",
    "        save_to_hive_table(final_df, \"preprocessed_news\", partition_by=\"label\")\n",
    "    \n",
    "    print(\"\\nData preprocessing pipeline completed successfully!\")\n",
    "    \n",
    "    return {\n",
    "        \"real_df\": real_df,\n",
    "        \"fake_df\": fake_df,\n",
    "        \"combined_df\": combined_df,\n",
    "        \"preprocessed_df\": preprocessed_df,\n",
    "        \"final_df\": final_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248967a2",
   "metadata": {},
   "source": [
    "## Step-by-Step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8389aac1",
   "metadata": {},
   "source": [
    "### 1. Load Data from Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f811c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Hive tables\n",
    "real_df, fake_df = load_data_from_hive()\n",
    "\n",
    "# Display sample data\n",
    "print(\"Real news sample:\")\n",
    "real_df.limit(3).show(truncate=False)\n",
    "\n",
    "print(\"\\nFake news sample:\")\n",
    "fake_df.limit(3).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1221894e",
   "metadata": {},
   "source": [
    "### 2. Data Exploration and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in both datasets\n",
    "print(\"Missing values in real news dataset:\")\n",
    "check_missing_values(real_df).show()\n",
    "\n",
    "print(\"\\nMissing values in fake news dataset:\")\n",
    "check_missing_values(fake_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(f\"Duplicates in real news dataset: {check_duplicates(real_df)}\")\n",
    "print(f\"Duplicates in fake news dataset: {check_duplicates(fake_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c9697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check text length distribution\n",
    "real_text_lengths = real_df.select(expr(\"size(split(text, ' '))\").alias(\"length\"))\n",
    "fake_text_lengths = fake_df.select(expr(\"size(split(text, ' '))\").alias(\"length\"))\n",
    "\n",
    "# Convert to pandas for visualization\n",
    "real_lengths_pd = real_text_lengths.toPandas()\n",
    "fake_lengths_pd = fake_text_lengths.toPandas()\n",
    "\n",
    "# Plot text length distributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(real_lengths_pd['length'], bins=50, alpha=0.5, label='Real News')\n",
    "plt.hist(fake_lengths_pd['length'], bins=50, alpha=0.5, label='Fake News')\n",
    "plt.xlabel('Text Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Text Length Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Real news text length statistics:\")\n",
    "real_text_lengths.summary().show()\n",
    "\n",
    "print(\"\\nFake news text length statistics:\")\n",
    "fake_text_lengths.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f35cd2c",
   "metadata": {},
   "source": [
    "### 3. Analyzing the 'subject' Column - Data Leakage Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdbd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check subject distribution in real news\n",
    "print(\"Subject distribution in real news:\")\n",
    "if \"subject\" in real_df.columns:\n",
    "    real_subjects = real_df.groupBy(\"subject\").count().orderBy(desc(\"count\"))\n",
    "    real_subjects.show()\n",
    "else:\n",
    "    print(\"No 'subject' column in real news dataset\")\n",
    "\n",
    "# Check subject distribution in fake news\n",
    "print(\"\\nSubject distribution in fake news:\")\n",
    "if \"subject\" in fake_df.columns:\n",
    "    fake_subjects = fake_df.groupBy(\"subject\").count().orderBy(desc(\"count\"))\n",
    "    fake_subjects.show()\n",
    "else:\n",
    "    print(\"No 'subject' column in fake news dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2938d72",
   "metadata": {},
   "source": [
    "### Data Leakage Visualization\n",
    "\n",
    "The analysis above reveals a critical issue: the 'subject' column perfectly discriminates between real and fake news. This creates a data leakage problem that would invalidate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f51792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined dataset with labels\n",
    "real_with_label = real_df.withColumn(\"label\", lit(1))\n",
    "fake_with_label = fake_df.withColumn(\"label\", lit(0))\n",
    "combined_df = real_with_label.union(fake_with_label)\n",
    "\n",
    "# Check if subject column exists\n",
    "if \"subject\" in combined_df.columns:\n",
    "    # Get subject distribution by label\n",
    "    subject_by_label = combined_df.groupBy(\"subject\", \"label\").count().orderBy(\"subject\", \"label\")\n",
    "    subject_by_label.show()\n",
    "    \n",
    "    # Convert to pandas for visualization\n",
    "    subject_label_pd = subject_by_label.toPandas()\n",
    "    \n",
    "    # Create a pivot table\n",
    "    pivot_data = subject_label_pd.pivot(index='subject', columns='label', values='count').fillna(0)\n",
    "    pivot_data.columns = ['Fake News', 'Real News']\n",
    "    \n",
    "    # Plot the distribution\n",
    "    ax = pivot_data.plot(kind='bar', figsize=(10, 6), color=['#FF6B6B', '#4ECDC4'])\n",
    "    plt.title('Subject Distribution by News Type')\n",
    "    plt.xlabel('Subject')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add text labels\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%d')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No 'subject' column in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fa40a5",
   "metadata": {},
   "source": [
    "### Why We Must Drop the 'subject' Column\n",
    "\n",
    "As demonstrated above, the 'subject' column creates a perfect data leakage problem:\n",
    "\n",
    "1. **Perfect Correlation with Labels**: \n",
    "   - Real news articles are all labeled as \"politicsNews\"\n",
    "   - Fake news articles are all labeled as \"News\"\n",
    "\n",
    "2. **Model Impact**: If we include this column:\n",
    "   - The model would achieve nearly 100% accuracy in training and validation\n",
    "   - But this accuracy would be misleading and wouldn't generalize to real-world data\n",
    "   - The model would simply learn \"if subject='politicsNews' then real, else fake\"\n",
    "\n",
    "3. **Real-World Failure**: In production, new articles wouldn't follow this artificial pattern, causing the model to make incorrect predictions based on an irrelevant feature.\n",
    "\n",
    "4. **Scientific Integrity**: Including this column would invalidate any research findings since the model wouldn't be learning meaningful patterns in the text content.\n",
    "\n",
    "Therefore, we must drop the 'subject' column from our preprocessing pipeline to ensure our model learns from the actual content of the news articles rather than this artificial distinction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02db7040",
   "metadata": {},
   "source": [
    "### 4. Analyzing URL Presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the function as a UDF\n",
    "contains_url_udf = udf(contains_url, BooleanType())\n",
    "\n",
    "# Check URL presence in real news\n",
    "real_with_url = real_df.withColumn(\"contains_url\", contains_url_udf(col(\"text\")))\n",
    "real_url_count = real_with_url.filter(col(\"contains_url\") == True).count()\n",
    "real_total = real_df.count()\n",
    "real_url_percentage = (real_url_count / real_total) * 100\n",
    "\n",
    "# Check URL presence in fake news\n",
    "fake_with_url = fake_df.withColumn(\"contains_url\", contains_url_udf(col(\"text\")))\n",
    "fake_url_count = fake_with_url.filter(col(\"contains_url\") == True).count()\n",
    "fake_total = fake_df.count()\n",
    "fake_url_percentage = (fake_url_count / fake_total) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Real news articles containing URLs: {real_url_count} out of {real_total} ({real_url_percentage:.2f}%)\")\n",
    "print(f\"Fake news articles containing URLs: {fake_url_count} out of {fake_total} ({fake_url_percentage:.2f}%)\")\n",
    "\n",
    "# Plot the results\n",
    "labels = ['Real News', 'Fake News']\n",
    "url_percentages = [real_url_percentage, fake_url_percentage]\n",
    "no_url_percentages = [100 - real_url_percentage, 100 - fake_url_percentage]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "width = 0.35\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "ax.bar(x, url_percentages, width, label='Contains URLs', color='#FF6B6B')\n",
    "ax.bar(x, no_url_percentages, width, bottom=url_percentages, label='No URLs', color='#4ECDC4')\n",
    "\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_title('URL Presence in News Articles')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(url_percentages):\n",
    "    ax.text(i, v/2, f\"{v:.1f}%\", ha='center', va='center', color='white', fontweight='bold')\n",
    "    ax.text(i, v + (no_url_percentages[i]/2), f\"{no_url_percentages[i]:.1f}%\", ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b6e1b",
   "metadata": {},
   "source": [
    "### 5. Analyzing Social Media Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the functions as UDFs\n",
    "contains_handle_udf = udf(contains_twitter_handle, BooleanType())\n",
    "contains_hashtag_udf = udf(contains_hashtag, BooleanType())\n",
    "\n",
    "# Check Twitter handle presence\n",
    "real_with_handle = real_df.withColumn(\"contains_handle\", contains_handle_udf(col(\"text\")))\n",
    "real_handle_count = real_with_handle.filter(col(\"contains_handle\") == True).count()\n",
    "real_handle_percentage = (real_handle_count / real_total) * 100\n",
    "\n",
    "fake_with_handle = fake_df.withColumn(\"contains_handle\", contains_handle_udf(col(\"text\")))\n",
    "fake_handle_count = fake_with_handle.filter(col(\"contains_handle\") == True).count()\n",
    "fake_handle_percentage = (fake_handle_count / fake_total) * 100\n",
    "\n",
    "# Check hashtag presence\n",
    "real_with_hashtag = real_df.withColumn(\"contains_hashtag\", contains_hashtag_udf(col(\"text\")))\n",
    "real_hashtag_count = real_with_hashtag.filter(col(\"contains_hashtag\") == True).count()\n",
    "real_hashtag_percentage = (real_hashtag_count / real_total) * 100\n",
    "\n",
    "fake_with_hashtag = fake_df.withColumn(\"contains_hashtag\", contains_hashtag_udf(col(\"text\")))\n",
    "fake_hashtag_count = fake_with_hashtag.filter(col(\"contains_hashtag\") == True).count()\n",
    "fake_hashtag_percentage = (fake_hashtag_count / fake_total) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Real news articles containing Twitter handles: {real_handle_count} out of {real_total} ({real_handle_percentage:.2f}%)\")\n",
    "print(f\"Fake news articles containing Twitter handles: {fake_handle_count} out of {fake_total} ({fake_handle_percentage:.2f}%)\")\n",
    "print(f\"\\nReal news articles containing hashtags: {real_hashtag_count} out of {real_total} ({real_hashtag_percentage:.2f}%)\")\n",
    "print(f\"Fake news articles containing hashtags: {fake_hashtag_count} out of {fake_total} ({fake_hashtag_percentage:.2f}%)\")\n",
    "\n",
    "# Plot the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Twitter handles plot\n",
    "handle_percentages = [real_handle_percentage, fake_handle_percentage]\n",
    "no_handle_percentages = [100 - real_handle_percentage, 100 - fake_handle_percentage]\n",
    "\n",
    "ax1.bar(x, handle_percentages, width, label='Contains Twitter Handles', color='#FF6B6B')\n",
    "ax1.bar(x, no_handle_percentages, width, bottom=handle_percentages, label='No Twitter Handles', color='#4ECDC4')\n",
    "ax1.set_ylabel('Percentage')\n",
    "ax1.set_title('Twitter Handle Presence in News Articles')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels)\n",
    "ax1.legend()\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(handle_percentages):\n",
    "    ax1.text(i, v/2, f\"{v:.1f}%\", ha='center', va='center', color='white', fontweight='bold')\n",
    "    ax1.text(i, v + (no_handle_percentages[i]/2), f\"{no_handle_percentages[i]:.1f}%\", ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Hashtags plot\n",
    "hashtag_percentages = [real_hashtag_percentage, fake_hashtag_percentage]\n",
    "no_hashtag_percentages = [100 - real_hashtag_percentage, 100 - fake_hashtag_percentage]\n",
    "\n",
    "ax2.bar(x, hashtag_percentages, width, label='Contains Hashtags', color='#FF6B6B')\n",
    "ax2.bar(x, no_hashtag_percentages, width, bottom=hashtag_percentages, label='No Hashtags', color='#4ECDC4')\n",
    "ax2.set_ylabel('Percentage')\n",
    "ax2.set_title('Hashtag Presence in News Articles')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(labels)\n",
    "ax2.legend()\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(hashtag_percentages):\n",
    "    ax2.text(i, v/2, f\"{v:.1f}%\", ha='center', va='center', color='white', fontweight='bold')\n",
    "    ax2.text(i, v + (no_hashtag_percentages[i]/2), f\"{no_hashtag_percentages[i]:.1f}%\", ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60058e1",
   "metadata": {},
   "source": [
    "### 6. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9af04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing configuration\n",
    "preprocessing_config = {\n",
    "    'remove_stopwords': True,\n",
    "    'stemming': False,\n",
    "    'lemmatization': True,\n",
    "    'lowercase': True,\n",
    "    'remove_punctuation': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_urls': True,\n",
    "    'remove_twitter_handles': True,\n",
    "    'remove_hashtags': False,\n",
    "    'convert_hashtags': True,\n",
    "    'remove_special_chars': True,\n",
    "    'normalize_whitespace': True,\n",
    "    'max_text_length': 500,\n",
    "    'language': 'english'\n",
    "}\n",
    "\n",
    "# Preprocess a sample text to demonstrate\n",
    "sample_text = real_df.select(\"text\").limit(1).collect()[0][0]\n",
    "print(\"Original text:\")\n",
    "print(sample_text[:500] + \"...\" if len(sample_text) > 500 else sample_text)\n",
    "\n",
    "# Preprocess the text\n",
    "preprocessed_text = preprocess_text(sample_text, preprocessing_config)\n",
    "print(\"\\nPreprocessed text:\")\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd15e7",
   "metadata": {},
   "source": [
    "### 7. Apply Preprocessing to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess both datasets\n",
    "real_preprocessed = preprocess_spark_df(real_df, \"text\", \"title\", combine_title_text=True, config=preprocessing_config)\n",
    "fake_preprocessed = preprocess_spark_df(fake_df, \"text\", \"title\", combine_title_text=True, config=preprocessing_config)\n",
    "\n",
    "# Display sample of preprocessed data\n",
    "print(\"Sample of preprocessed real news:\")\n",
    "real_preprocessed.select(\"title\", \"text\", \"processed_text\").limit(2).show(truncate=50)\n",
    "\n",
    "print(\"\\nSample of preprocessed fake news:\")\n",
    "fake_preprocessed.select(\"title\", \"text\", \"processed_text\").limit(2).show(truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393a036",
   "metadata": {},
   "source": [
    "### 8. Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize preprocessed text\n",
    "real_tokenized = tokenize_spark_df(real_preprocessed, \"processed_text\", config=preprocessing_config)\n",
    "fake_tokenized = tokenize_spark_df(fake_preprocessed, \"processed_text\", config=preprocessing_config)\n",
    "\n",
    "# Display sample of tokenized data\n",
    "print(\"Sample of tokenized real news:\")\n",
    "real_tokenized.select(\"processed_text\", \"tokens\").limit(2).show(truncate=50)\n",
    "\n",
    "print(\"\\nSample of tokenized fake news:\")\n",
    "fake_tokenized.select(\"processed_text\", \"tokens\").limit(2).show(truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915fe1cc",
   "metadata": {},
   "source": [
    "### 9. Extract Quoted Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ccb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract quoted content\n",
    "real_with_quotes = extract_quoted_content_spark_df(real_tokenized, \"text\")\n",
    "fake_with_quotes = extract_quoted_content_spark_df(fake_tokenized, \"text\")\n",
    "\n",
    "# Display sample of data with quoted content\n",
    "print(\"Sample of real news with quoted content:\")\n",
    "real_with_quotes.select(\"text\", \"quoted_content\").limit(2).show(truncate=50)\n",
    "\n",
    "print(\"\\nSample of fake news with quoted content:\")\n",
    "fake_with_quotes.select(\"text\", \"quoted_content\").limit(2).show(truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff56d86a",
   "metadata": {},
   "source": [
    "### 10. Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete preprocessing pipeline\n",
    "results = preprocess_and_save_data(\n",
    "    output_dir=\"dbfs:/FileStore/fake_news_detection/preprocessed_data\",\n",
    "    fake_table_name=\"fake\", \n",
    "    true_table_name=\"real\",\n",
    "    create_tables=True\n",
    ")\n",
    "\n",
    "# Display sample of final preprocessed data\n",
    "print(\"Sample of final preprocessed data:\")\n",
    "results[\"final_df\"].select(\"label\", \"processed_text\", \"tokens\").limit(3).show(truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9a1e3",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "1. **Data Leakage**: We identified and removed the 'subject' column to prevent data leakage, as it perfectly discriminates between real and fake news in our dataset.\n",
    "\n",
    "2. **Text Preprocessing**: Our preprocessing pipeline includes multiple steps:\n",
    "   - Removing URLs, Twitter handles, and special characters\n",
    "   - Converting text to lowercase\n",
    "   - Removing punctuation\n",
    "   - Removing stopwords\n",
    "   - Lemmatization\n",
    "   - Tokenization\n",
    "\n",
    "3. **Social Media Content**: We found significant differences in the presence of URLs, Twitter handles, and hashtags between real and fake news, which can be useful features for classification.\n",
    "\n",
    "4. **Date Processing**: If date information is available, we standardize it and extract useful features like year, month, day, and day of week.\n",
    "\n",
    "5. **Quoted Content**: We extract content within quotation marks, which can be useful for analyzing quoted sources in news articles.\n",
    "\n",
    "6. **Configuration**: All preprocessing steps are configurable through a dictionary, allowing for easy experimentation with different preprocessing strategies.\n",
    "\n",
    "7. **Spark Optimization**: The code is optimized for Spark's distributed processing capabilities, making it suitable for large datasets."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
