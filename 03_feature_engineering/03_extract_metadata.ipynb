{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80089019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to extract and analyze topic and source metadata from news articles.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7fdab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fcc74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_dir = \"/home/ubuntu/fake_news_detection/data\"\n",
    "results_dir = \"/home/ubuntu/fake_news_detection/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23608a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e20387",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "# Load the sampled data\n",
    "df = pd.read_csv(f\"{data_dir}/news_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfbe48c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to extract source and location from text\n",
    "def extract_metadata(text):\n",
    "    \"\"\"Extract source and location information from text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None, None\n",
    "    \n",
    "    # Common news sources\n",
    "    sources = [\n",
    "        'Reuters', 'AP', 'Associated Press', 'CNN', 'Fox News', 'MSNBC', 'BBC', \n",
    "        'New York Times', 'Washington Post', 'USA Today', 'NPR', 'CBS', 'NBC', \n",
    "        'ABC News', 'The Guardian', 'Bloomberg', 'Wall Street Journal', 'WSJ',\n",
    "        'Huffington Post', 'Breitbart', 'BuzzFeed', 'Daily Mail', 'The Hill'\n",
    "    ]\n",
    "    \n",
    "    # Try to find source at the end of the text with pattern \"- Source\"\n",
    "    source_match = re.search(r'-\\s*([^-\\n]+?)$', text)\n",
    "    if source_match:\n",
    "        potential_source = source_match.group(1).strip()\n",
    "        # Check if the extracted text contains a known source\n",
    "        for known_source in sources:\n",
    "            if known_source.lower() in potential_source.lower():\n",
    "                return known_source, None\n",
    "    \n",
    "    # Try to find source in the text\n",
    "    for source in sources:\n",
    "        if source.lower() in text.lower():\n",
    "            return source, None\n",
    "    \n",
    "    # If no source found\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply extraction to the dataset\n",
    "print(\"Extracting metadata...\")\n",
    "metadata = df['text'].apply(extract_metadata)\n",
    "df['source'] = metadata.apply(lambda x: x[0])\n",
    "df['location'] = metadata.apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e422a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count sources\n",
    "print(\"Analyzing sources...\")\n",
    "source_counts = df['source'].value_counts()\n",
    "print(f\"Found {len(source_counts)} unique sources\")\n",
    "print(source_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc396f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze source distribution by label\n",
    "source_by_label = pd.crosstab(df['source'], df['label'])\n",
    "source_by_label.columns = ['Fake', 'Real']\n",
    "source_by_label['Total'] = source_by_label['Fake'] + source_by_label['Real']\n",
    "source_by_label['Fake_Ratio'] = source_by_label['Fake'] / source_by_label['Total']\n",
    "source_by_label['Real_Ratio'] = source_by_label['Real'] / source_by_label['Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by total count\n",
    "source_by_label = source_by_label.sort_values('Total', ascending=False)\n",
    "print(\"\\nSource distribution by label:\")\n",
    "print(source_by_label.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract topics using NLP techniques\n",
    "print(\"\\nExtracting topics...\")\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    \n",
    "    # Use CountVectorizer for topic modeling\n",
    "    count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english', max_features=1000)\n",
    "    doc_term_matrix = count_vectorizer.fit_transform(df['text'].fillna(''))\n",
    "    \n",
    "    # LDA for topic modeling\n",
    "    lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "    lda.fit(doc_term_matrix)\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = count_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Extract top words for each topic\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words_idx = topic.argsort()[:-11:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics[f\"Topic {topic_idx}\"] = top_words\n",
    "        print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n",
    "    \n",
    "    # Get dominant topic for each document\n",
    "    doc_topics = lda.transform(doc_term_matrix)\n",
    "    df['dominant_topic'] = np.argmax(doc_topics, axis=1)\n",
    "    \n",
    "    # Analyze topic distribution by label\n",
    "    topic_by_label = pd.crosstab(df['dominant_topic'], df['label'])\n",
    "    topic_by_label.columns = ['Fake', 'Real']\n",
    "    topic_by_label['Total'] = topic_by_label['Fake'] + topic_by_label['Real']\n",
    "    topic_by_label['Fake_Ratio'] = topic_by_label['Fake'] / topic_by_label['Total']\n",
    "    topic_by_label['Real_Ratio'] = topic_by_label['Real'] / topic_by_label['Total']\n",
    "    \n",
    "    print(\"\\nTopic distribution by label:\")\n",
    "    print(topic_by_label)\n",
    "    \n",
    "    # Create topic names based on top words\n",
    "    topic_names = {i: f\"Topic {i}: {', '.join(topics[f'Topic {i}'][:3])}\" for i in range(len(topics))}\n",
    "    \n",
    "    # Replace topic numbers with names\n",
    "    df['topic_name'] = df['dominant_topic'].map(topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "except Exception as e:\n",
    "    print(f\"Error in topic extraction: {e}\")\n",
    "    df['dominant_topic'] = None\n",
    "    df['topic_name'] = None\n",
    "    topics = {}\n",
    "    topic_by_label = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize source distribution\n",
    "print(\"\\nCreating visualizations...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_sources = source_counts.head(10).index\n",
    "source_data = source_by_label.loc[source_by_label.index.isin(top_sources)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8df03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacked bar chart\n",
    "source_data[['Fake', 'Real']].plot(kind='bar', stacked=True, figsize=(12, 8))\n",
    "plt.title('Distribution of Fake and Real News by Source')\n",
    "plt.xlabel('Source')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dir}/source_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860fb93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic distribution\n",
    "if not topic_by_label.empty:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    topic_by_label[['Fake', 'Real']].plot(kind='bar', stacked=True, figsize=(12, 8))\n",
    "    plt.title('Distribution of Fake and Real News by Topic')\n",
    "    plt.xlabel('Topic')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{results_dir}/topic_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of source vs topic\n",
    "if 'dominant_topic' in df.columns and df['dominant_topic'].notna().any():\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    # Filter to include only top sources\n",
    "    filtered_df = df[df['source'].isin(top_sources)]\n",
    "    if not filtered_df.empty:\n",
    "        heatmap_data = pd.crosstab(filtered_df['source'], filtered_df['dominant_topic'])\n",
    "        sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='d')\n",
    "        plt.title('Source vs Topic Distribution')\n",
    "        plt.xlabel('Topic')\n",
    "        plt.ylabel('Source')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{results_dir}/source_topic_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dashboard visualization\n",
    "plt.figure(figsize=(15, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa1603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "source_counts.head(5).plot(kind='bar')\n",
    "plt.title('Top 5 Sources')\n",
    "plt.xlabel('Source')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic distribution\n",
    "if 'dominant_topic' in df.columns and df['dominant_topic'].notna().any():\n",
    "    plt.subplot(2, 2, 2)\n",
    "    df['dominant_topic'].value_counts().sort_index().plot(kind='bar')\n",
    "    plt.title('Documents per Topic')\n",
    "    plt.xlabel('Topic')\n",
    "    plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7ef98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake vs Real by source\n",
    "plt.subplot(2, 2, 3)\n",
    "if not source_data.empty:\n",
    "    source_data['Fake_Ratio'].plot(kind='bar', color='red', alpha=0.7)\n",
    "    plt.title('Fake News Ratio by Source')\n",
    "    plt.xlabel('Source')\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687802a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake vs Real by topic\n",
    "if not topic_by_label.empty:\n",
    "    plt.subplot(2, 2, 4)\n",
    "    topic_by_label['Fake_Ratio'].plot(kind='bar', color='red', alpha=0.7)\n",
    "    plt.title('Fake News Ratio by Topic')\n",
    "    plt.xlabel('Topic')\n",
    "    plt.ylabel('Ratio')\n",
    "    plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ab567",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dir}/metadata_dashboard.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3fee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced dataset\n",
    "df.to_csv(f\"{data_dir}/news_sample_enhanced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cffe8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata analysis results\n",
    "metadata_results = {\n",
    "    \"sources\": {\n",
    "        \"unique_count\": len(source_counts),\n",
    "        \"top_sources\": source_counts.head(10).to_dict(),\n",
    "        \"source_by_label\": source_by_label.head(10).to_dict()\n",
    "    },\n",
    "    \"topics\": {\n",
    "        \"topic_words\": topics,\n",
    "        \"topic_by_label\": topic_by_label.to_dict() if not topic_by_label.empty else {}\n",
    "    },\n",
    "    \"execution_time\": time.time() - start_time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{results_dir}/metadata_analysis.json\", \"w\") as f:\n",
    "    json.dump(metadata_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nMetadata extraction and analysis completed in {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"Enhanced dataset saved to {data_dir}/news_sample_enhanced.csv\")\n",
    "print(f\"Results saved to {results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Last modified: May 29, 2025
