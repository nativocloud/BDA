{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "131a5117",
   "metadata": {},
   "source": [
    "# Fake News Detection: Feature Engineering\n",
    "\n",
    "This notebook contains all the necessary code for feature engineering in the fake news detection project. The code is organized into independent functions, without dependencies on external modules or classes, to facilitate execution in Databricks Community Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc1c38",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fdcc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, udf, lower, regexp_extract, when, count, desc, lit, array, \n",
    "    size, split, explode, collect_list, struct, expr\n",
    ")\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField, IntegerType, FloatType\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer, StopWordsRemover, LDA\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3be95c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session optimized for Databricks Community Edition\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsFeatureEngineering\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a53ec5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Start timer for performance tracking\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6c134f",
   "metadata": {},
   "source": [
    "## Reusable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a679124",
   "metadata": {},
   "source": [
    "### Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf15d0b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_preprocessed_data(path=\"dbfs:/FileStore/fake_news_detection/preprocessed_data/preprocessed_news.parquet\"):\n",
    "    \"\"\"\n",
    "    Load preprocessed data from Parquet file.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the preprocessed data Parquet file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Spark DataFrame with preprocessed data\n",
    "    \"\"\"\n",
    "    print(f\"Loading preprocessed data from {path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load data from Parquet file\n",
    "        df = spark.read.parquet(path)\n",
    "        \n",
    "        # Display basic information\n",
    "        print(f\"Successfully loaded {df.count()} records.\")\n",
    "        df.printSchema()\n",
    "        \n",
    "        # Cache the DataFrame for better performance\n",
    "        df.cache()\n",
    "        print(\"Preprocessed DataFrame cached.\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading preprocessed data: {e}\")\n",
    "        print(\"Please ensure the preprocessing notebook ran successfully and saved data to the correct path.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9eae8",
   "metadata": {},
   "source": [
    "### Source Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbac050",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_source_from_text(text):\n",
    "    \"\"\"\n",
    "    Extract news source from text using regex patterns.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The news article text\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted source name or None if not found\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    \n",
    "    # Define list of common news sources\n",
    "    common_sources = [\n",
    "        \"Reuters\", \"AP\", \"Associated Press\", \"CNN\", \"Fox News\", \"MSNBC\", \"BBC\", \n",
    "        \"New York Times\", \"Washington Post\", \"USA Today\", \"NPR\", \"CBS\", \"NBC\", \n",
    "        \"ABC News\", \"The Guardian\", \"Bloomberg\", \"Wall Street Journal\", \"WSJ\",\n",
    "        \"Huffington Post\", \"Breitbart\", \"BuzzFeed\", \"Daily Mail\", \"The Hill\"\n",
    "    ]\n",
    "    \n",
    "    # Pattern: Optional Location (SOURCE) - Text\n",
    "    match = re.match(r\"^\\s*\\w*\\s*\\(([^)]+)\\)\\s*-\", text)\n",
    "    if match:\n",
    "        potential_source = match.group(1).strip()\n",
    "        # Check against common sources\n",
    "        for src in common_sources:\n",
    "            if src.lower() == potential_source.lower():\n",
    "                return src\n",
    "    \n",
    "    # Fallback: Check if text starts with a known source name\n",
    "    for src in common_sources:\n",
    "        if text and text.lower().startswith(src.lower()):\n",
    "            return src\n",
    "    \n",
    "    # Try to find source at the end of the text with pattern \"- Source\"\n",
    "    if text:\n",
    "        source_match = re.search(r'-\\s*([^-\\n]+?)$', text)\n",
    "        if source_match:\n",
    "            potential_source = source_match.group(1).strip()\n",
    "            # Check if the extracted text contains a known source\n",
    "            for known_source in common_sources:\n",
    "                if known_source.lower() in potential_source.lower():\n",
    "                    return known_source\n",
    "    \n",
    "    # Try to find source in the text\n",
    "    if text:\n",
    "        for source in common_sources:\n",
    "            if source.lower() in text.lower():\n",
    "                return source\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27d7a8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_source_feature(df, text_column=\"text\"):\n",
    "    \"\"\"\n",
    "    Extract source feature from text column in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with text column\n",
    "        text_column (str): Name of the column containing text\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with extracted source feature\n",
    "    \"\"\"\n",
    "    print(\"Extracting news source feature...\")\n",
    "    \n",
    "    # Register UDF for source extraction\n",
    "    extract_source_udf = udf(extract_source_from_text, StringType())\n",
    "    \n",
    "    # Apply UDF to extract source\n",
    "    result_df = df.withColumn(\"extracted_source\", extract_source_udf(col(text_column)))\n",
    "    \n",
    "    # Show some results\n",
    "    result_df.select(text_column, \"extracted_source\").show(10, truncate=80)\n",
    "    \n",
    "    # Analyze extracted sources\n",
    "    print(\"\\nDistribution of extracted sources:\")\n",
    "    source_counts = result_df.groupBy(\"extracted_source\").count().orderBy(desc(\"count\"))\n",
    "    source_counts.show()\n",
    "    \n",
    "    # Analyze source distribution by label\n",
    "    if \"label\" in df.columns:\n",
    "        print(\"\\nExtracted source distribution by label:\")\n",
    "        source_by_label = result_df.groupBy(\"extracted_source\", \"label\").count()\n",
    "        source_by_label_pivot = source_by_label.groupBy(\"extracted_source\")\\\n",
    "            .pivot(\"label\", [0, 1])\\\n",
    "            .agg(count(\"count\").alias(\"count\"))\\\n",
    "            .na.fill(0)\\\n",
    "            .withColumnRenamed(\"0\", \"fake_count\")\\\n",
    "            .withColumnRenamed(\"1\", \"real_count\")\\\n",
    "            .withColumn(\"total_count\", col(\"fake_count\") + col(\"real_count\"))\\\n",
    "            .orderBy(desc(\"total_count\"))\n",
    "            \n",
    "        source_by_label_pivot.show()\n",
    "    \n",
    "    return result_df, source_by_label_pivot if \"label\" in df.columns else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dface9cf",
   "metadata": {},
   "source": [
    "### Topic Modeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cdea5f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_topic_modeling_pipeline(num_topics=10, max_iterations=10, vocab_size=10000, min_doc_freq=5.0):\n",
    "    \"\"\"\n",
    "    Create a topic modeling pipeline using Spark MLlib.\n",
    "    \n",
    "    Args:\n",
    "        num_topics (int): Number of topics for LDA\n",
    "        max_iterations (int): Maximum iterations for LDA\n",
    "        vocab_size (int): Vocabulary size for CountVectorizer\n",
    "        min_doc_freq (float): Minimum document frequency for CountVectorizer\n",
    "        \n",
    "    Returns:\n",
    "        Pipeline: Spark ML Pipeline for topic modeling\n",
    "    \"\"\"\n",
    "    print(\"Setting up topic modeling pipeline...\")\n",
    "    \n",
    "    # 1. Tokenizer: Split processed text into words\n",
    "    tokenizer = Tokenizer(inputCol=\"processed_text\", outputCol=\"raw_tokens\")\n",
    "    \n",
    "    # 2. StopWordsRemover: Remove common English stop words\n",
    "    remover = StopWordsRemover(inputCol=\"raw_tokens\", outputCol=\"tokens\")\n",
    "    \n",
    "    # 3. CountVectorizer: Convert tokens into frequency vectors\n",
    "    cv = CountVectorizer(\n",
    "        inputCol=\"tokens\", \n",
    "        outputCol=\"rawFeatures\", \n",
    "        vocabSize=vocab_size, \n",
    "        minDF=min_doc_freq\n",
    "    )\n",
    "    \n",
    "    # 4. IDF: Down-weight common terms across documents\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    \n",
    "    # 5. LDA: Discover latent topics\n",
    "    lda = LDA(\n",
    "        k=num_topics, \n",
    "        maxIter=max_iterations, \n",
    "        featuresCol=\"features\", \n",
    "        topicDistributionCol=\"topicDistribution\"\n",
    "    )\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, lda])\n",
    "    \n",
    "    print(f\"Topic modeling pipeline created with {num_topics} topics.\")\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d14c8c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def fit_topic_model(df, pipeline, text_column=\"processed_text\"):\n",
    "    \"\"\"\n",
    "    Fit topic modeling pipeline to data.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with processed text\n",
    "        pipeline (Pipeline): Spark ML Pipeline for topic modeling\n",
    "        text_column (str): Name of the column containing processed text\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (pipeline_model, lda_results_df) - Fitted pipeline model and transformed DataFrame\n",
    "    \"\"\"\n",
    "    print(\"Fitting topic modeling pipeline... This may take some time on the full dataset.\")\n",
    "    start_lda_time = time.time()\n",
    "    \n",
    "    # Ensure the text column exists\n",
    "    if text_column not in df.columns:\n",
    "        print(f\"Error: Column '{text_column}' not found in DataFrame.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    \n",
    "    # Transform the data to get topic distributions\n",
    "    lda_results_df = pipeline_model.transform(df)\n",
    "    \n",
    "    print(f\"Pipeline fitting and transformation completed in {time.time() - start_lda_time:.2f} seconds.\")\n",
    "    \n",
    "    # Display schema with new columns\n",
    "    lda_results_df.printSchema()\n",
    "    \n",
    "    # Show sample results with topic distribution\n",
    "    if \"id\" in lda_results_df.columns:\n",
    "        lda_results_df.select(\"id\", \"label\", \"topicDistribution\").show(5, truncate=False)\n",
    "    else:\n",
    "        lda_results_df.select(\"label\", \"topicDistribution\").show(5, truncate=False)\n",
    "    \n",
    "    return pipeline_model, lda_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61a0bf9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_topics(pipeline_model, max_terms_per_topic=10):\n",
    "    \"\"\"\n",
    "    Analyze topics discovered by the LDA model.\n",
    "    \n",
    "    Args:\n",
    "        pipeline_model (PipelineModel): Fitted pipeline model containing LDA\n",
    "        max_terms_per_topic (int): Maximum number of terms to show per topic\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with topic descriptions\n",
    "    \"\"\"\n",
    "    print(\"Analyzing discovered topics...\")\n",
    "    \n",
    "    # Extract the LDA model and vocabulary from the pipeline\n",
    "    lda_model = pipeline_model.stages[-1]  # LDA is the last stage\n",
    "    cv_model = pipeline_model.stages[2]    # CountVectorizer is the third stage\n",
    "    vocabulary = cv_model.vocabulary\n",
    "    \n",
    "    # Get the topic descriptions (top words per topic)\n",
    "    topics = lda_model.describeTopics(maxTermsPerTopic=max_terms_per_topic)\n",
    "    \n",
    "    print(\"Top terms per topic:\")\n",
    "    topics_with_terms = []\n",
    "    for row in topics.collect():\n",
    "        topic_idx = row[0]\n",
    "        term_indices = row[1]\n",
    "        term_weights = row[2]\n",
    "        topic_terms = [vocabulary[i] for i in term_indices]\n",
    "        print(f\"Topic {topic_idx}: {topic_terms}\")\n",
    "        \n",
    "        # Create a row for the topics DataFrame\n",
    "        topics_with_terms.append((topic_idx, topic_terms))\n",
    "    \n",
    "    # Convert topics summary to DataFrame for easier analysis/saving\n",
    "    topics_schema = StructType([\n",
    "        StructField(\"topic_id\", StringType(), False),\n",
    "        StructField(\"top_terms\", ArrayType(StringType()), False)\n",
    "    ])\n",
    "    \n",
    "    # Create DataFrame using createDataFrame with explicit schema\n",
    "    topics_df = spark.createDataFrame(topics_with_terms, [\"topic_id\", \"top_terms\"])\n",
    "    topics_df.show(truncate=False)\n",
    "    \n",
    "    return topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e3f3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_topic_distribution_by_label(lda_results_df):\n",
    "    \"\"\"\n",
    "    Analyze how topics are distributed across fake and real news.\n",
    "    \n",
    "    Args:\n",
    "        lda_results_df (DataFrame): DataFrame with topic distributions and labels\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with topic distribution by label\n",
    "    \"\"\"\n",
    "    print(\"Analyzing topic distribution by label...\")\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    if \"topicDistribution\" not in lda_results_df.columns or \"label\" not in lda_results_df.columns:\n",
    "        print(\"Error: Required columns 'topicDistribution' or 'label' not found.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract dominant topic for each document\n",
    "    dominant_topic_udf = udf(lambda v: float(v.argmax()), FloatType())\n",
    "    with_dominant_topic = lda_results_df.withColumn(\"dominant_topic\", dominant_topic_udf(col(\"topicDistribution\")))\n",
    "    \n",
    "    # Analyze topic distribution by label\n",
    "    topic_by_label = with_dominant_topic.groupBy(\"dominant_topic\", \"label\").count()\n",
    "    \n",
    "    # Create pivot table\n",
    "    topic_by_label_pivot = topic_by_label.groupBy(\"dominant_topic\")\\\n",
    "        .pivot(\"label\", [0, 1])\\\n",
    "        .agg(count(\"count\").alias(\"count\"))\\\n",
    "        .na.fill(0)\\\n",
    "        .withColumnRenamed(\"0\", \"fake_count\")\\\n",
    "        .withColumnRenamed(\"1\", \"real_count\")\\\n",
    "        .withColumn(\"total_count\", col(\"fake_count\") + col(\"real_count\"))\\\n",
    "        .withColumn(\"fake_ratio\", col(\"fake_count\") / col(\"total_count\"))\\\n",
    "        .withColumn(\"real_ratio\", col(\"real_count\") / col(\"total_count\"))\\\n",
    "        .orderBy(\"dominant_topic\")\n",
    "    \n",
    "    # Show results\n",
    "    print(\"Topic distribution by label:\")\n",
    "    topic_by_label_pivot.show()\n",
    "    \n",
    "    return topic_by_label_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834486da",
   "metadata": {},
   "source": [
    "### Text Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4be2f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_text_features(df, text_column=\"text\"):\n",
    "    \"\"\"\n",
    "    Extract various text features from the text column.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with text column\n",
    "        text_column (str): Name of the column containing text\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with extracted text features\n",
    "    \"\"\"\n",
    "    print(\"Extracting text features...\")\n",
    "    \n",
    "    # Ensure text column exists\n",
    "    if text_column not in df.columns:\n",
    "        print(f\"Error: Column '{text_column}' not found in DataFrame.\")\n",
    "        return df\n",
    "    \n",
    "    # Text length (character count)\n",
    "    result_df = df.withColumn(\"text_length\", length(col(text_column)))\n",
    "    \n",
    "    # Word count\n",
    "    result_df = result_df.withColumn(\"word_count\", size(split(col(text_column), \" \")))\n",
    "    \n",
    "    # Average word length\n",
    "    result_df = result_df.withColumn(\n",
    "        \"avg_word_length\", \n",
    "        when(col(\"word_count\") > 0, col(\"text_length\") / col(\"word_count\")).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    # Count of special characters\n",
    "    special_chars_udf = udf(lambda text: len(re.findall(r'[^\\w\\s]', text)) if text else 0, IntegerType())\n",
    "    result_df = result_df.withColumn(\"special_char_count\", special_chars_udf(col(text_column)))\n",
    "    \n",
    "    # Count of uppercase words\n",
    "    uppercase_words_udf = udf(lambda text: len(re.findall(r'\\b[A-Z]{2,}\\b', text)) if text else 0, IntegerType())\n",
    "    result_df = result_df.withColumn(\"uppercase_word_count\", uppercase_words_udf(col(text_column)))\n",
    "    \n",
    "    # Show sample of extracted features\n",
    "    result_df.select(\n",
    "        text_column, \"text_length\", \"word_count\", \n",
    "        \"avg_word_length\", \"special_char_count\", \"uppercase_word_count\"\n",
    "    ).show(5, truncate=80)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e1bc5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_text_statistics(df):\n",
    "    \"\"\"\n",
    "    Calculate statistics for text features.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame with text features\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with text feature statistics by label\n",
    "    \"\"\"\n",
    "    print(\"Calculating text feature statistics...\")\n",
    "    \n",
    "    # Required text feature columns\n",
    "    text_features = [\"text_length\", \"word_count\", \"avg_word_length\", \"special_char_count\", \"uppercase_word_count\"]\n",
    "    \n",
    "    # Check if all required columns exist\n",
    "    missing_cols = [col for col in text_features if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Error: Missing columns: {missing_cols}\")\n",
    "        print(\"Please run extract_text_features first.\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate statistics for each feature\n",
    "    stats = []\n",
    "    for feature in text_features:\n",
    "        # Overall statistics\n",
    "        overall_stats = df.select(feature).summary(\"min\", \"25%\", \"mean\", \"75%\", \"max\").collect()\n",
    "        overall_dict = {row[\"summary\"]: float(row[feature]) for row in overall_stats}\n",
    "        \n",
    "        # Statistics by label if label column exists\n",
    "        if \"label\" in df.columns:\n",
    "            # Real news statistics (label=1)\n",
    "            real_stats = df.filter(col(\"label\") == 1).select(feature).summary(\"min\", \"25%\", \"mean\", \"75%\", \"max\").collect()\n",
    "            real_dict = {row[\"summary\"]: float(row[feature]) for row in real_stats}\n",
    "            \n",
    "            # Fake news statistics (label=0)\n",
    "            fake_stats = df.filter(col(\"label\") == 0).select(feature).summary(\"min\", \"25%\", \"mean\", \"75%\", \"max\").collect()\n",
    "            fake_dict = {row[\"summary\"]: float(row[feature]) for row in fake_stats}\n",
    "            \n",
    "            stats.append((feature, overall_dict, real_dict, fake_dict))\n",
    "        else:\n",
    "            stats.append((feature, overall_dict, None, None))\n",
    "    \n",
    "    # Print statistics\n",
    "    for feature, overall, real, fake in stats:\n",
    "        print(f\"\\nStatistics for {feature}:\")\n",
    "        print(f\"Overall: {overall}\")\n",
    "        if real and fake:\n",
    "            print(f\"Real news: {real}\")\n",
    "            print(f\"Fake news: {fake}\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb25ac1",
   "metadata": {},
   "source": [
    "### Data Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40637261",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_to_parquet(df, path, partition_by=None):\n",
    "    \"\"\"\n",
    "    Save a DataFrame in Parquet format.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to save\n",
    "        path (str): Path where to save the DataFrame\n",
    "        partition_by (str): Column to partition by (optional)\n",
    "    \"\"\"\n",
    "    print(f\"Saving DataFrame to {path}...\")\n",
    "    \n",
    "    writer = df.write.mode(\"overwrite\")\n",
    "    \n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(partition_by)\n",
    "    \n",
    "    writer.parquet(path)\n",
    "    print(f\"DataFrame saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4774cf8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_to_hive_table(df, table_name, partition_by=None):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a Hive table.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to save\n",
    "        table_name (str): Name of the Hive table to create or replace\n",
    "        partition_by (str): Column to partition by (optional)\n",
    "    \"\"\"\n",
    "    print(f\"Saving DataFrame to Hive table {table_name}...\")\n",
    "    \n",
    "    writer = df.write.mode(\"overwrite\").format(\"parquet\")\n",
    "    \n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(partition_by)\n",
    "    \n",
    "    writer.saveAsTable(table_name)\n",
    "    print(f\"DataFrame saved to Hive table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804d6dc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_pipeline_model(pipeline_model, path):\n",
    "    \"\"\"\n",
    "    Save a pipeline model to disk.\n",
    "    \n",
    "    Args:\n",
    "        pipeline_model (PipelineModel): Fitted pipeline model to save\n",
    "        path (str): Path where to save the model\n",
    "    \"\"\"\n",
    "    print(f\"Saving pipeline model to {path}...\")\n",
    "    \n",
    "    try:\n",
    "        pipeline_model.write().overwrite().save(path)\n",
    "        print(\"Pipeline model saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving pipeline model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ef76d",
   "metadata": {},
   "source": [
    "## Complete Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f534a7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def engineer_features_and_save(\n",
    "    input_path=\"dbfs:/FileStore/fake_news_detection/preprocessed_data/preprocessed_news.parquet\",\n",
    "    output_dir=\"dbfs:/FileStore/fake_news_detection/feature_data\",\n",
    "    model_save_dir=\"dbfs:/FileStore/fake_news_detection/models/feature_engineering\",\n",
    "    num_topics=10,\n",
    "    create_tables=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete feature engineering pipeline for fake news detection.\n",
    "    \n",
    "    This pipeline loads preprocessed data, extracts source and text features,\n",
    "    performs topic modeling, and saves the results.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): Path to preprocessed data\n",
    "        output_dir (str): Directory to save feature data\n",
    "        model_save_dir (str): Directory to save models\n",
    "        num_topics (int): Number of topics for LDA\n",
    "        create_tables (bool): Whether to create Hive tables\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with references to processed DataFrames\n",
    "    \"\"\"\n",
    "    print(\"Starting feature engineering pipeline...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create output directories\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(output_dir.replace(\"dbfs:\", \"\"))\n",
    "        dbutils.fs.mkdirs(model_save_dir.replace(\"dbfs:\", \"\"))\n",
    "    except:\n",
    "        print(\"Warning: Could not create directories. This is expected in local environments.\")\n",
    "    \n",
    "    # 1. Load preprocessed data\n",
    "    df = load_preprocessed_data(input_path)\n",
    "    if df is None:\n",
    "        print(\"Error: Could not load preprocessed data. Pipeline aborted.\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Extract source feature\n",
    "    df_with_source, source_by_label = extract_source_feature(df, \"text\")\n",
    "    \n",
    "    # 3. Extract text features\n",
    "    df_with_text_features = extract_text_features(df_with_source, \"text\")\n",
    "    \n",
    "    # 4. Calculate text statistics\n",
    "    text_stats = calculate_text_statistics(df_with_text_features)\n",
    "    \n",
    "    # 5. Create topic modeling pipeline\n",
    "    topic_pipeline = create_topic_modeling_pipeline(num_topics=num_topics)\n",
    "    \n",
    "    # 6. Fit topic model\n",
    "    pipeline_model, df_with_topics = fit_topic_model(df_with_text_features, topic_pipeline)\n",
    "    \n",
    "    # 7. Analyze topics\n",
    "    if pipeline_model:\n",
    "        topics_df = analyze_topics(pipeline_model)\n",
    "        \n",
    "        # 8. Analyze topic distribution by label\n",
    "        topic_by_label = analyze_topic_distribution_by_label(df_with_topics)\n",
    "        \n",
    "        # 9. Save pipeline model\n",
    "        model_path = f\"{model_save_dir}/lda_pipeline_model\"\n",
    "        save_pipeline_model(pipeline_model, model_path)\n",
    "    else:\n",
    "        print(\"Warning: Topic modeling failed. Continuing with other features.\")\n",
    "        df_with_topics = df_with_text_features\n",
    "        topics_df = None\n",
    "        topic_by_label = None\n",
    "    \n",
    "    # 10. Save feature data\n",
    "    features_path = f\"{output_dir}/features.parquet\"\n",
    "    save_to_parquet(df_with_topics, features_path, partition_by=\"label\")\n",
    "    \n",
    "    # 11. Save to Hive table for easier access\n",
    "    if create_tables:\n",
    "        save_to_hive_table(df_with_topics, \"news_features\", partition_by=\"label\")\n",
    "        \n",
    "        # Save source distribution\n",
    "        if source_by_label is not None:\n",
    "            save_to_hive_table(source_by_label, \"source_distribution\")\n",
    "        \n",
    "        # Save topic distribution\n",
    "        if topic_by_label is not None:\n",
    "            save_to_hive_table(topic_by_label, \"topic_distribution\")\n",
    "    \n",
    "    print(f\"\\nFeature engineering pipeline completed in {time.time() - start_time:.2f} seconds!\")\n",
    "    \n",
    "    return {\n",
    "        \"preprocessed_df\": df,\n",
    "        \"df_with_source\": df_with_source,\n",
    "        \"df_with_text_features\": df_with_text_features,\n",
    "        \"df_with_topics\": df_with_topics,\n",
    "        \"source_by_label\": source_by_label,\n",
    "        \"topic_by_label\": topic_by_label,\n",
    "        \"topics_df\": topics_df,\n",
    "        \"pipeline_model\": pipeline_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65af94",
   "metadata": {},
   "source": [
    "## Step-by-Step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb8228",
   "metadata": {},
   "source": [
    "### 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bccda84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "preprocessed_df = load_preprocessed_data()\n",
    "\n",
    "# Display sample data\n",
    "if preprocessed_df:\n",
    "    print(\"Preprocessed data sample:\")\n",
    "    preprocessed_df.show(5, truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d7888",
   "metadata": {},
   "source": [
    "### 2. Extract Source Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract source feature\n",
    "if preprocessed_df:\n",
    "    df_with_source, source_by_label = extract_source_feature(preprocessed_df)\n",
    "    \n",
    "    # Visualize source distribution (using Databricks display function)\n",
    "    if source_by_label:\n",
    "        print(\"Source distribution by label:\")\n",
    "        display(source_by_label.limit(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d9d5a",
   "metadata": {},
   "source": [
    "### 3. Extract Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec01747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text features\n",
    "if 'df_with_source' in locals():\n",
    "    df_with_text_features = extract_text_features(df_with_source)\n",
    "    \n",
    "    # Calculate text statistics\n",
    "    text_stats = calculate_text_statistics(df_with_text_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be268135",
   "metadata": {},
   "source": [
    "### 4. Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006adfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create topic modeling pipeline\n",
    "if 'df_with_text_features' in locals():\n",
    "    # Create pipeline with 10 topics\n",
    "    topic_pipeline = create_topic_modeling_pipeline(num_topics=10)\n",
    "    \n",
    "    # Fit topic model\n",
    "    pipeline_model, df_with_topics = fit_topic_model(df_with_text_features, topic_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e538a",
   "metadata": {},
   "source": [
    "### 5. Analyze Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze topics\n",
    "if 'pipeline_model' in locals() and pipeline_model:\n",
    "    # Get topic descriptions\n",
    "    topics_df = analyze_topics(pipeline_model)\n",
    "    \n",
    "    # Analyze topic distribution by label\n",
    "    topic_by_label = analyze_topic_distribution_by_label(df_with_topics)\n",
    "    \n",
    "    # Visualize topic distribution (using Databricks display function)\n",
    "    print(\"Topic distribution by label:\")\n",
    "    display(topic_by_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8916f8",
   "metadata": {},
   "source": [
    "### 6. Complete Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a492e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete feature engineering pipeline\n",
    "results = engineer_features_and_save(\n",
    "    input_path=\"dbfs:/FileStore/fake_news_detection/preprocessed_data/preprocessed_news.parquet\",\n",
    "    output_dir=\"dbfs:/FileStore/fake_news_detection/feature_data\",\n",
    "    model_save_dir=\"dbfs:/FileStore/fake_news_detection/models/feature_engineering\",\n",
    "    num_topics=10,\n",
    "    create_tables=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c4bcce",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "1. **Feature Engineering Importance**: Feature engineering is crucial for fake news detection as it helps extract meaningful signals from text and creates numerical representations suitable for machine learning models.\n",
    "\n",
    "2. **Source Extraction**: We extract the news source from the text, which can be a valuable feature as some sources may be more reliable than others.\n",
    "\n",
    "3. **Text Features**: We extract various text features like text length, word count, average word length, and counts of special characters and uppercase words, which can help identify patterns in fake vs. real news.\n",
    "\n",
    "4. **Topic Modeling**: We use Latent Dirichlet Allocation (LDA) to discover latent topics in the news articles, which can reveal thematic differences between fake and real news.\n",
    "\n",
    "5. **Spark Optimization**: The code is optimized for Spark's distributed processing capabilities, making it suitable for large datasets.\n",
    "\n",
    "6. **Databricks Integration**: The pipeline is designed to work seamlessly in Databricks, with appropriate configurations for the Community Edition.\n",
    "\n",
    "7. **Pipeline Model Saving**: We save the fitted pipeline model for later use in prediction or further analysis.\n",
    "\n",
    "8. **Feature Storage**: All extracted features are saved in Parquet format and as Hive tables for easy access in subsequent steps of the pipeline."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
