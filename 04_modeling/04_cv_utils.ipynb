{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cross-validation and data leakage prevention utility functions for fake news detection project.\n",
    "This module contains functions for ensuring proper cross-validation and preventing data leakage.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e7a1b3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2443d0c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_stratified_cv_folds(df, label_col=\"label\", n_splits=5, seed=42):\n",
    "    \"\"\"\n",
    "    Create stratified cross-validation folds for a Spark DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        label_col (str): Column containing labels\n",
    "        n_splits (int): Number of folds\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        list: List of (train_df, val_df) tuples\n",
    "    \"\"\"\n",
    "    # Convert to pandas for stratification\n",
    "    pandas_df = df.toPandas()\n",
    "    \n",
    "    # Create stratified k-fold\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Create folds\n",
    "    folds = []\n",
    "    for train_idx, val_idx in skf.split(pandas_df, pandas_df[label_col]):\n",
    "        train_pandas = pandas_df.iloc[train_idx]\n",
    "        val_pandas = pandas_df.iloc[val_idx]\n",
    "        \n",
    "        # Convert back to Spark DataFrames\n",
    "        train_spark = df.sparkSession.createDataFrame(train_pandas)\n",
    "        val_spark = df.sparkSession.createDataFrame(val_pandas)\n",
    "        \n",
    "        folds.append((train_spark, val_spark))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01af1f3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_stratified_cv(spark, pipeline, param_grid, df, label_col=\"label\", n_splits=5, seed=42):\n",
    "    \"\"\"\n",
    "    Perform stratified cross-validation for a Spark ML pipeline.\n",
    "    \n",
    "    Args:\n",
    "        spark: Spark session\n",
    "        pipeline: ML Pipeline\n",
    "        param_grid: Parameter grid for tuning\n",
    "        df: Spark DataFrame\n",
    "        label_col (str): Column containing labels\n",
    "        n_splits (int): Number of folds\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_model, cv_metrics)\n",
    "    \"\"\"\n",
    "    # Create folds\n",
    "    folds = create_stratified_cv_folds(df, label_col, n_splits, seed)\n",
    "    \n",
    "    # Initialize metrics\n",
    "    all_metrics = []\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    for i, (train_df, val_df) in enumerate(folds):\n",
    "        print(f\"Fold {i+1}/{n_splits}\")\n",
    "        \n",
    "        # Train model\n",
    "        model = pipeline.fit(train_df)\n",
    "        \n",
    "        # Evaluate model\n",
    "        predictions = model.transform(val_df)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        evaluator_auc = BinaryClassificationEvaluator(\n",
    "            labelCol=label_col, \n",
    "            rawPredictionCol=\"rawPrediction\", \n",
    "            metricName=\"areaUnderROC\"\n",
    "        )\n",
    "        \n",
    "        evaluator_acc = MulticlassClassificationEvaluator(\n",
    "            labelCol=label_col, \n",
    "            predictionCol=\"prediction\", \n",
    "            metricName=\"accuracy\"\n",
    "        )\n",
    "        \n",
    "        auc = evaluator_auc.evaluate(predictions)\n",
    "        accuracy = evaluator_acc.evaluate(predictions)\n",
    "        \n",
    "        # Store metrics\n",
    "        all_metrics.append({\n",
    "            \"fold\": i+1,\n",
    "            \"auc\": auc,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "        \n",
    "        print(f\"Fold {i+1} - AUC: {auc:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_auc = np.mean([m[\"auc\"] for m in all_metrics])\n",
    "    avg_accuracy = np.mean([m[\"accuracy\"] for m in all_metrics])\n",
    "    \n",
    "    print(f\"Average - AUC: {avg_auc:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
    "    \n",
    "    # Train final model on all data\n",
    "    final_model = pipeline.fit(df)\n",
    "    \n",
    "    return final_model, all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e434dd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def time_based_train_test_split(df, time_col, train_ratio=0.8, seed=42):\n",
    "    \"\"\"\n",
    "    Split data based on time to prevent data leakage.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        time_col (str): Column containing time/date information\n",
    "        train_ratio (float): Ratio of training data\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_df, test_df)\n",
    "    \"\"\"\n",
    "    # Sort by time\n",
    "    sorted_df = df.orderBy(time_col)\n",
    "    \n",
    "    # Calculate split point\n",
    "    count = sorted_df.count()\n",
    "    split_point = int(count * train_ratio)\n",
    "    \n",
    "    # Split data\n",
    "    train_df = sorted_df.limit(split_point)\n",
    "    test_df = sorted_df.subtract(train_df)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097e174b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def feature_leakage_check(train_df, test_df, feature_cols):\n",
    "    \"\"\"\n",
    "    Check for potential feature leakage between train and test sets.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training DataFrame\n",
    "        test_df: Testing DataFrame\n",
    "        feature_cols (list): List of feature column names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of leakage metrics\n",
    "    \"\"\"\n",
    "    leakage_metrics = {}\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        # Get unique values\n",
    "        train_values = set(train_df.select(col).distinct().rdd.flatMap(lambda x: x).collect())\n",
    "        test_values = set(test_df.select(col).distinct().rdd.flatMap(lambda x: x).collect())\n",
    "        \n",
    "        # Calculate overlap\n",
    "        overlap = train_values.intersection(test_values)\n",
    "        overlap_ratio = len(overlap) / len(test_values) if len(test_values) > 0 else 0\n",
    "        \n",
    "        leakage_metrics[col] = {\n",
    "            \"overlap_count\": len(overlap),\n",
    "            \"overlap_ratio\": overlap_ratio\n",
    "        }\n",
    "    \n",
    "    return leakage_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e41e9f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_cv_results(cv_metrics, metric_name=\"auc\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plot cross-validation results.\n",
    "    \n",
    "    Args:\n",
    "        cv_metrics (list): List of metrics dictionaries\n",
    "        metric_name (str): Name of the metric to plot\n",
    "        save_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    # Extract metrics\n",
    "    folds = [m[\"fold\"] for m in cv_metrics]\n",
    "    metrics = [m[metric_name] for m in cv_metrics]\n",
    "    avg_metric = np.mean(metrics)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(folds, metrics, color='skyblue')\n",
    "    plt.axhline(y=avg_metric, color='r', linestyle='-', label=f'Average: {avg_metric:.4f}')\n",
    "    \n",
    "    plt.title(f'Cross-Validation Results - {metric_name.upper()}')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel(metric_name.upper())\n",
    "    plt.xticks(folds)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save plot if path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9889ed",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_nested_cv(df, outer_splits=5, inner_splits=3, seed=42):\n",
    "    \"\"\"\n",
    "    Create nested cross-validation folds to prevent data leakage during hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        outer_splits (int): Number of outer folds for model evaluation\n",
    "        inner_splits (int): Number of inner folds for hyperparameter tuning\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        list: List of (train_idx, test_idx, inner_cv) tuples\n",
    "    \"\"\"\n",
    "    # Convert to pandas for stratification\n",
    "    pandas_df = df.toPandas()\n",
    "    X = pandas_df.drop('label', axis=1)\n",
    "    y = pandas_df['label']\n",
    "    \n",
    "    # Create outer folds\n",
    "    outer_cv = StratifiedKFold(n_splits=outer_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Create nested CV structure\n",
    "    nested_cv = []\n",
    "    \n",
    "    for train_idx, test_idx in outer_cv.split(X, y):\n",
    "        # Create inner CV\n",
    "        inner_cv = StratifiedKFold(n_splits=inner_splits, shuffle=True, random_state=seed)\n",
    "        inner_cv_splits = list(inner_cv.split(X.iloc[train_idx], y.iloc[train_idx]))\n",
    "        \n",
    "        nested_cv.append((train_idx, test_idx, inner_cv_splits))\n",
    "    \n",
    "    return nested_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b2ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_nested_cv(spark, pipeline_factory, param_grid_factory, df, nested_cv):\n",
    "    \"\"\"\n",
    "    Perform nested cross-validation to prevent data leakage during hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "        spark: Spark session\n",
    "        pipeline_factory: Function that creates a pipeline\n",
    "        param_grid_factory: Function that creates a parameter grid\n",
    "        df: DataFrame\n",
    "        nested_cv: Nested CV structure from create_nested_cv\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_models, outer_metrics, inner_metrics)\n",
    "    \"\"\"\n",
    "    pandas_df = df.toPandas()\n",
    "    \n",
    "    best_models = []\n",
    "    outer_metrics = []\n",
    "    inner_metrics = []\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx, inner_cv_splits) in enumerate(nested_cv):\n",
    "        print(f\"Outer Fold {fold_idx+1}/{len(nested_cv)}\")\n",
    "        \n",
    "        # Get outer train/test data\n",
    "        outer_train = pandas_df.iloc[train_idx]\n",
    "        outer_test = pandas_df.iloc[test_idx]\n",
    "        \n",
    "        # Convert to Spark DataFrames\n",
    "        spark_outer_train = spark.createDataFrame(outer_train)\n",
    "        spark_outer_test = spark.createDataFrame(outer_test)\n",
    "        \n",
    "        # Inner CV for hyperparameter tuning\n",
    "        inner_fold_metrics = []\n",
    "        \n",
    "        for inner_fold_idx, (inner_train_idx, inner_val_idx) in enumerate(inner_cv_splits):\n",
    "            print(f\"  Inner Fold {inner_fold_idx+1}/{len(inner_cv_splits)}\")\n",
    "            \n",
    "            # Get inner train/val data\n",
    "            inner_train = outer_train.iloc[inner_train_idx]\n",
    "            inner_val = outer_train.iloc[inner_val_idx]\n",
    "            \n",
    "            # Convert to Spark DataFrames\n",
    "            spark_inner_train = spark.createDataFrame(inner_train)\n",
    "            spark_inner_val = spark.createDataFrame(inner_val)\n",
    "            \n",
    "            # Create pipeline and param grid\n",
    "            pipeline = pipeline_factory()\n",
    "            param_grid = param_grid_factory()\n",
    "            \n",
    "            # Create cross-validator\n",
    "            evaluator = BinaryClassificationEvaluator(\n",
    "                labelCol=\"label\", \n",
    "                rawPredictionCol=\"rawPrediction\", \n",
    "                metricName=\"areaUnderROC\"\n",
    "            )\n",
    "            \n",
    "            cv = CrossValidator(\n",
    "                estimator=pipeline,\n",
    "                estimatorParamMaps=param_grid,\n",
    "                evaluator=evaluator,\n",
    "                numFolds=3  # This is just for hyperparameter selection within the inner fold\n",
    "            )\n",
    "            \n",
    "            # Fit cross-validator\n",
    "            cv_model = cv.fit(spark_inner_train)\n",
    "            \n",
    "            # Evaluate on inner validation set\n",
    "            predictions = cv_model.transform(spark_inner_val)\n",
    "            auc = evaluator.evaluate(predictions)\n",
    "            \n",
    "            inner_fold_metrics.append({\n",
    "                \"outer_fold\": fold_idx+1,\n",
    "                \"inner_fold\": inner_fold_idx+1,\n",
    "                \"auc\": auc\n",
    "            })\n",
    "            \n",
    "            print(f\"    Inner Fold {inner_fold_idx+1} - AUC: {auc:.4f}\")\n",
    "        \n",
    "        # Train final model for this outer fold using best hyperparameters\n",
    "        pipeline = pipeline_factory()\n",
    "        best_model = pipeline.fit(spark_outer_train)\n",
    "        \n",
    "        # Evaluate on outer test set\n",
    "        predictions = best_model.transform(spark_outer_test)\n",
    "        evaluator = BinaryClassificationEvaluator(\n",
    "            labelCol=\"label\", \n",
    "            rawPredictionCol=\"rawPrediction\", \n",
    "            metricName=\"areaUnderROC\"\n",
    "        )\n",
    "        auc = evaluator.evaluate(predictions)\n",
    "        \n",
    "        outer_metrics.append({\n",
    "            \"fold\": fold_idx+1,\n",
    "            \"auc\": auc\n",
    "        })\n",
    "        \n",
    "        best_models.append(best_model)\n",
    "        inner_metrics.append(inner_fold_metrics)\n",
    "        \n",
    "        print(f\"Outer Fold {fold_idx+1} - AUC: {auc:.4f}\")\n",
    "    \n",
    "    return best_models, outer_metrics, inner_metrics"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
