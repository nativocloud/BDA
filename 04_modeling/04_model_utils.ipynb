{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59278e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model utility functions for fake news detection project.\n",
    "This module contains functions for creating, training, and evaluating models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e596e5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0238d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_random_forest_model(num_trees=10, max_depth=5, seed=42):\n",
    "    \"\"\"\n",
    "    Create a Random Forest classifier model.\n",
    "    \n",
    "    Args:\n",
    "        num_trees (int): Number of trees in the forest\n",
    "        max_depth (int): Maximum depth of each tree\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        RandomForestClassifier: Configured model\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(\n",
    "        labelCol=\"label\", \n",
    "        featuresCol=\"features\", \n",
    "        numTrees=num_trees, \n",
    "        maxDepth=max_depth,\n",
    "        seed=seed\n",
    "    )\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b00c5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_naive_bayes_model():\n",
    "    \"\"\"\n",
    "    Create a Naive Bayes classifier model.\n",
    "    \n",
    "    Returns:\n",
    "        NaiveBayes: Configured model\n",
    "    \"\"\"\n",
    "    nb = NaiveBayes(\n",
    "        labelCol=\"label\", \n",
    "        featuresCol=\"features\", \n",
    "        modelType=\"multinomial\"\n",
    "    )\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d8de5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_logistic_regression_model(max_iter=100, reg_param=0.3, elastic_net_param=0.8):\n",
    "    \"\"\"\n",
    "    Create a Logistic Regression classifier model.\n",
    "    \n",
    "    Args:\n",
    "        max_iter (int): Maximum number of iterations\n",
    "        reg_param (float): Regularization parameter\n",
    "        elastic_net_param (float): ElasticNet mixing parameter\n",
    "        \n",
    "    Returns:\n",
    "        LogisticRegression: Configured model\n",
    "    \"\"\"\n",
    "    lr = LogisticRegression(\n",
    "        labelCol=\"label\", \n",
    "        featuresCol=\"features\", \n",
    "        maxIter=max_iter,\n",
    "        regParam=reg_param,\n",
    "        elasticNetParam=elastic_net_param\n",
    "    )\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd87a02",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data, log_dir=None):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on test data.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_data: Test dataset\n",
    "        log_dir (str): Directory to save evaluation logs\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    predictions = model.transform(test_data)\n",
    "    \n",
    "    # Binary classification evaluator\n",
    "    evaluator_auc = BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        rawPredictionCol=\"rawPrediction\", \n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    \n",
    "    # Multiclass classification evaluator for accuracy\n",
    "    evaluator_acc = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = evaluator_auc.evaluate(predictions)\n",
    "    accuracy = evaluator_acc.evaluate(predictions)\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    evaluator_precision = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    \n",
    "    evaluator_recall = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"weightedRecall\"\n",
    "    )\n",
    "    \n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"f1\"\n",
    "    )\n",
    "    \n",
    "    precision = evaluator_precision.evaluate(predictions)\n",
    "    recall = evaluator_recall.evaluate(predictions)\n",
    "    f1 = evaluator_f1.evaluate(predictions)\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        \"auc\": auc,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "    \n",
    "    # Log metrics\n",
    "    if log_dir:\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        with open(os.path.join(log_dir, \"evaluation_metrics.txt\"), \"a\") as f:\n",
    "            f.write(f\"Model: {type(model).__name__}\\n\")\n",
    "            f.write(f\"AUC: {auc:.4f}\\n\")\n",
    "            f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "            f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "            f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "            f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e95ae0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_cross_validation(pipeline, param_grid, train_data, num_folds=3, seed=42):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to find the best model parameters.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: ML Pipeline\n",
    "        param_grid: Parameter grid for tuning\n",
    "        train_data: Training dataset\n",
    "        num_folds (int): Number of cross-validation folds\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_model, cv_metrics)\n",
    "    \"\"\"\n",
    "    # Create evaluator\n",
    "    evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        rawPredictionCol=\"rawPrediction\", \n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    \n",
    "    # Create cross-validator\n",
    "    cv = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=num_folds,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Fit cross-validator\n",
    "    cv_model = cv.fit(train_data)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = cv_model.bestModel\n",
    "    \n",
    "    # Get metrics for all models\n",
    "    cv_metrics = {\n",
    "        \"avg_metrics\": cv_model.avgMetrics,\n",
    "        \"best_model_index\": np.argmax(cv_model.avgMetrics),\n",
    "        \"param_maps\": cv_model.getEstimatorParamMaps()\n",
    "    }\n",
    "    \n",
    "    return best_model, cv_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(metrics_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot comparison of model metrics.\n",
    "    \n",
    "    Args:\n",
    "        metrics_dict (dict): Dictionary of model metrics\n",
    "        save_path (str): Path to save the plot\n",
    "    \"\"\"\n",
    "    models = list(metrics_dict.keys())\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc']\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Set width of bars\n",
    "    bar_width = 0.15\n",
    "    index = np.arange(len(metrics))\n",
    "    \n",
    "    # Plot bars for each model\n",
    "    for i, model in enumerate(models):\n",
    "        values = [metrics_dict[model][metric] for metric in metrics]\n",
    "        ax.bar(index + i * bar_width, values, bar_width, label=model)\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('Metrics')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Model Comparison')\n",
    "    ax.set_xticks(index + bar_width * (len(models) - 1) / 2)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Save plot if path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "    \n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Last modified: May 29, 2025
