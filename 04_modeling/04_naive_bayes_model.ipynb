{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to implement and run a simplified Naive Bayes model for fake news detection.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e7821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b407b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_dir = \"/home/ubuntu/fake_news_detection/data\"\n",
    "models_dir = \"/home/ubuntu/fake_news_detection/models\"\n",
    "results_dir = \"/home/ubuntu/fake_news_detection/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9700fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "# Load the sampled data\n",
    "df = pd.read_csv(f\"{data_dir}/news_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic preprocessing\n",
    "print(\"Preprocessing text...\")\n",
    "# Fill NaN values\n",
    "df['text'] = df['text'].fillna('')\n",
    "if 'title' in df.columns:\n",
    "    df['title'] = df['title'].fillna('')\n",
    "    # Combine title and text for better context\n",
    "    df['content'] = df['title'] + \" \" + df['text']\n",
    "else:\n",
    "    df['content'] = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "df['content'] = df['content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f153f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Class distribution:\\n{df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad220102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "print(\"Splitting data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['content'], df['label'], test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2090c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction with TF-IDF\n",
    "print(\"Extracting features...\")\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab50e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier\n",
    "print(\"Training Naive Bayes model...\")\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825481d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "y_pred = nb_model.predict(X_test_tfidf)\n",
    "y_pred_prob = nb_model.predict_proba(X_test_tfidf)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d59652",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef88d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and vectorizer\n",
    "print(\"Saving model and vectorizer...\")\n",
    "with open(f\"{models_dir}/nb_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(nb_model, f)\n",
    "with open(f\"{models_dir}/nb_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a10bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results = {\n",
    "    \"model\": \"Naive Bayes\",\n",
    "    \"accuracy\": float(accuracy),\n",
    "    \"precision\": float(precision),\n",
    "    \"recall\": float(recall),\n",
    "    \"f1\": float(f1),\n",
    "    \"execution_time\": time.time() - start_time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6adba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results as text\n",
    "with open(f\"{results_dir}/nb_results.txt\", \"w\") as f:\n",
    "    f.write(f\"Model: {results['model']}\\n\")\n",
    "    f.write(f\"Accuracy: {results['accuracy']:.4f}\\n\")\n",
    "    f.write(f\"Precision: {results['precision']:.4f}\\n\")\n",
    "    f.write(f\"Recall: {results['recall']:.4f}\\n\")\n",
    "    f.write(f\"F1 Score: {results['f1']:.4f}\\n\")\n",
    "    f.write(f\"Execution Time: {results['execution_time']:.2f} seconds\\n\\n\")\n",
    "    f.write(\"Classification Report:\\n\")\n",
    "    f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results as JSON for Grafana\n",
    "with open(f\"{results_dir}/nb_results.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations of the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "values = [accuracy, precision, recall, f1]\n",
    "sns.barplot(x=metrics, y=values)\n",
    "plt.title('Naive Bayes Model Performance')\n",
    "plt.ylim(0, 1)\n",
    "plt.savefig(f\"{results_dir}/nb_performance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1972cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison with the Random Forest model\n",
    "# Load Random Forest results if available\n",
    "rf_results_path = f\"{results_dir}/baseline_results.txt\"\n",
    "if os.path.exists(rf_results_path):\n",
    "    with open(rf_results_path, 'r') as f:\n",
    "        rf_content = f.read()\n",
    "        rf_accuracy = float(rf_content.split('Accuracy: ')[1].split('\\n')[0])\n",
    "        rf_precision = float(rf_content.split('Precision: ')[1].split('\\n')[0])\n",
    "        rf_recall = float(rf_content.split('Recall: ')[1].split('\\n')[0])\n",
    "        rf_f1 = float(rf_content.split('F1 Score: ')[1].split('\\n')[0])\n",
    "    \n",
    "    # Create comparison plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Set up data for comparison\n",
    "    models = ['Random Forest', 'Naive Bayes']\n",
    "    accuracy_values = [rf_accuracy, accuracy]\n",
    "    precision_values = [rf_precision, precision]\n",
    "    recall_values = [rf_recall, recall]\n",
    "    f1_values = [rf_f1, f1]\n",
    "    \n",
    "    # Set width of bars\n",
    "    barWidth = 0.2\n",
    "    \n",
    "    # Set position of bars on X axis\n",
    "    r1 = np.arange(len(models))\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "    r3 = [x + barWidth for x in r2]\n",
    "    r4 = [x + barWidth for x in r3]\n",
    "    \n",
    "    # Create bars\n",
    "    plt.bar(r1, accuracy_values, width=barWidth, label='Accuracy')\n",
    "    plt.bar(r2, precision_values, width=barWidth, label='Precision')\n",
    "    plt.bar(r3, recall_values, width=barWidth, label='Recall')\n",
    "    plt.bar(r4, f1_values, width=barWidth, label='F1 Score')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title('Model Comparison')\n",
    "    plt.xticks([r + barWidth*1.5 for r in range(len(models))], models)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Save comparison plot\n",
    "    plt.savefig(f\"{results_dir}/model_comparison.png\")\n",
    "    \n",
    "    # Save comparison data for Grafana\n",
    "    comparison_data = {\n",
    "        \"models\": models,\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": accuracy_values,\n",
    "            \"precision\": precision_values,\n",
    "            \"recall\": recall_values,\n",
    "            \"f1\": f1_values\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{results_dir}/model_comparison.json\", \"w\") as f:\n",
    "        json.dump(comparison_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ccd12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Results saved to {results_dir}\")\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n",
    "print(\"Naive Bayes model implementation completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
