{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Machine Learning Models for Fake News Detection\n",
    "\n",
    "This notebook implements and evaluates traditional machine learning models (Naive Bayes and Random Forest) for fake news detection using PySpark MLlib. These models serve as strong baselines and are particularly well-suited for the Databricks Community Edition due to their efficiency.\n",
    "\n",
    "## Models Implemented\n",
    "1. TF-IDF + Naive Bayes\n",
    "2. TF-IDF + Random Forest\n",
    "\n",
    "## Key Features\n",
    "- Distributed processing with PySpark MLlib\n",
    "- Cross-validation for robust evaluation\n",
    "- Feature importance analysis\n",
    "- Optimized for Databricks Community Edition\n",
    "- Comparison with deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, we'll set up our Spark session with configurations optimized for the Databricks Community Edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, regexp_replace, length, udf\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, ArrayType\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Configure Spark session optimized for Databricks Community Edition\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection_TraditionalModels\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We'll load data directly from the Hive metastore tables ('fake' and 'real')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import the HiveDataIngestion class\n",
    "import sys\n",
    "sys.path.append('/dbfs/FileStore/tables')\n",
    "from hive_data_ingestion import HiveDataIngestion\n",
    "\n",
    "# Create an instance\n",
    "data_ingestion = HiveDataIngestion(spark)\n",
    "\n",
    "# Load data from Hive tables\n",
    "start_time = time.time()\n",
    "real_df, fake_df = data_ingestion.load_data_from_hive()\n",
    "print(f\"Data loading time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Combine datasets with labels\n",
    "combined_df = data_ingestion.combine_datasets(real_df, fake_df)\n",
    "print(f\"Combined dataset size: {combined_df.count()} records\")\n",
    "\n",
    "# Display class distribution\n",
    "combined_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management for Community Edition\n",
    "\n",
    "The Databricks Community Edition has limited resources (15.3 GB memory, 2 cores). We'll implement strategies to manage memory efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to create a stratified sample if needed\n",
    "def create_stratified_sample(df, sample_size_per_class=2000, seed=42):\n",
    "    \"\"\"\n",
    "    Create a balanced sample with equal representation from each class.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to sample from\n",
    "        sample_size_per_class: Number of samples per class\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with balanced samples\n",
    "    \"\"\"\n",
    "    # Get class counts\n",
    "    class_counts = df.groupBy(\"label\").count().collect()\n",
    "    \n",
    "    # Calculate sampling fractions\n",
    "    fractions = {}\n",
    "    for row in class_counts:\n",
    "        label = row[\"label\"]\n",
    "        count = row[\"count\"]\n",
    "        fraction = min(1.0, sample_size_per_class / count)\n",
    "        fractions[label] = fraction\n",
    "    \n",
    "    # Create stratified sample\n",
    "    sampled_df = df.sampleBy(\"label\", fractions, seed)\n",
    "    \n",
    "    print(f\"Original class distribution:\")\n",
    "    df.groupBy(\"label\").count().show()\n",
    "    \n",
    "    print(f\"Sampled class distribution:\")\n",
    "    sampled_df.groupBy(\"label\").count().show()\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "# Check if we need to sample based on available memory\n",
    "# For traditional models, we'll try to use the full dataset first\n",
    "use_full_dataset = True\n",
    "\n",
    "if use_full_dataset:\n",
    "    working_df = combined_df\n",
    "    print(\"Using full dataset\")\n",
    "else:\n",
    "    # Create a balanced sample for development/testing\n",
    "    working_df = create_stratified_sample(combined_df, sample_size_per_class=2000)\n",
    "    print(\"Using sampled dataset\")\n",
    "\n",
    "# Cache the working dataset for faster processing\n",
    "working_df.cache()\n",
    "print(f\"Working dataset size: {working_df.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "We'll preprocess the text data to prepare it for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import concat_ws, lower, regexp_replace\n",
    "\n",
    "# Combine title and text fields, and perform basic cleaning\n",
    "preprocessed_df = working_df.withColumn(\n",
    "    \"content\", \n",
    "    concat_ws(\" \", \n",
    "              when(col(\"title\").isNull(), \"\").otherwise(col(\"title\")),\n",
    "              when(col(\"text\").isNull(), \"\").otherwise(col(\"text\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert to lowercase and remove special characters\n",
    "preprocessed_df = preprocessed_df.withColumn(\"content\", lower(col(\"content\")))\n",
    "preprocessed_df = preprocessed_df.withColumn(\n",
    "    \"content\", \n",
    "    regexp_replace(col(\"content\"), \"[^a-zA-Z0-9\\s]\", \" \")\n",
    ")\n",
    "\n",
    "# Remove extra whitespace\n",
    "preprocessed_df = preprocessed_df.withColumn(\n",
    "    \"content\", \n",
    "    regexp_replace(col(\"content\"), \"\\s+\", \" \")\n",
    ")\n",
    "\n",
    "# Show sample of preprocessed content\n",
    "preprocessed_df.select(\"content\", \"label\").show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "We'll split our data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data into training and testing sets (70% train, 30% test)\n",
    "train_df, test_df = preprocessed_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Cache datasets for faster processing\n",
    "train_df.cache()\n",
    "test_df.cache()\n",
    "\n",
    "print(f\"Training set size: {train_df.count()} records\")\n",
    "print(f\"Testing set size: {test_df.count()} records\")\n",
    "\n",
    "# Check class distribution in training set\n",
    "print(\"Training set class distribution:\")\n",
    "train_df.groupBy(\"label\").count().show()\n",
    "\n",
    "# Check class distribution in testing set\n",
    "print(\"Testing set class distribution:\")\n",
    "test_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with TF-IDF\n",
    "\n",
    "We'll use TF-IDF (Term Frequency-Inverse Document Frequency) to convert text into numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define feature extraction pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Create feature extraction pipeline\n",
    "feature_pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "feature_model = feature_pipeline.fit(train_df)\n",
    "\n",
    "# Transform the training and testing data\n",
    "train_features = feature_model.transform(train_df)\n",
    "test_features = feature_model.transform(test_df)\n",
    "\n",
    "# Show sample of features\n",
    "train_features.select(\"content\", \"filtered\", \"features\", \"label\").show(2, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Naive Bayes\n",
    "\n",
    "We'll implement a Naive Bayes classifier using PySpark MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create Naive Bayes model\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Define parameter grid for cross-validation\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0.1, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Define evaluators\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Create cross-validator\n",
    "cv = CrossValidator(\n",
    "    estimator=nb,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=f1_evaluator,\n",
    "    numFolds=3  # Use 3 folds for Community Edition (less resource intensive)\n",
    ")\n",
    "\n",
    "# Train model with cross-validation\n",
    "start_time = time.time()\n",
    "cv_model = cv.fit(train_features)\n",
    "nb_training_time = time.time() - start_time\n",
    "print(f\"Naive Bayes training time: {nb_training_time:.2f} seconds\")\n",
    "\n",
    "# Get best model\n",
    "best_nb_model = cv_model.bestModel\n",
    "print(f\"Best smoothing parameter: {best_nb_model.getSmoothing()}\")\n",
    "\n",
    "# Make predictions on test data\n",
    "nb_predictions = best_nb_model.transform(test_features)\n",
    "\n",
    "# Evaluate model\n",
    "nb_accuracy = accuracy_evaluator.evaluate(nb_predictions)\n",
    "nb_f1 = f1_evaluator.evaluate(nb_predictions)\n",
    "nb_auc = auc_evaluator.evaluate(nb_predictions)\n",
    "\n",
    "print(f\"Naive Bayes Accuracy: {nb_accuracy:.4f}\")\n",
    "print(f\"Naive Bayes F1 Score: {nb_f1:.4f}\")\n",
    "print(f\"Naive Bayes AUC: {nb_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest\n",
    "\n",
    "We'll implement a Random Forest classifier using PySpark MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create Random Forest model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Define parameter grid for cross-validation\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Create cross-validator\n",
    "cv = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=f1_evaluator,\n",
    "    numFolds=3  # Use 3 folds for Community Edition (less resource intensive)\n",
    ")\n",
    "\n",
    "# Train model with cross-validation\n",
    "start_time = time.time()\n",
    "cv_model = cv.fit(train_features)\n",
    "rf_training_time = time.time() - start_time\n",
    "print(f\"Random Forest training time: {rf_training_time:.2f} seconds\")\n",
    "\n",
    "# Get best model\n",
    "best_rf_model = cv_model.bestModel\n",
    "print(f\"Best numTrees: {best_rf_model.getNumTrees}\")\n",
    "print(f\"Best maxDepth: {best_rf_model.getMaxDepth()}\")\n",
    "\n",
    "# Make predictions on test data\n",
    "rf_predictions = best_rf_model.transform(test_features)\n",
    "\n",
    "# Evaluate model\n",
    "rf_accuracy = accuracy_evaluator.evaluate(rf_predictions)\n",
    "rf_f1 = f1_evaluator.evaluate(rf_predictions)\n",
    "rf_auc = auc_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Random Forest F1 Score: {rf_f1:.4f}\")\n",
    "print(f\"Random Forest AUC: {rf_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "We'll analyze feature importance from the Random Forest model to understand what words are most predictive of fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get feature importances from Random Forest model\n",
    "feature_importances = best_rf_model.featureImportances.toArray()\n",
    "\n",
    "# Create a DataFrame with feature importances\n",
    "import pandas as pd\n",
    "\n",
    "# Get top 20 feature indices\n",
    "top_indices = np.argsort(-feature_importances)[:20]\n",
    "top_importances = feature_importances[top_indices]\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature Index': top_indices,\n",
    "    'Importance': top_importances\n",
    "})\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importance_df)), importance_df['Importance'])\n",
    "plt.yticks(range(len(importance_df)), [f\"Feature {idx}\" for idx in importance_df['Feature Index']])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "We'll compare the performance of Naive Bayes and Random Forest models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create comparison DataFrame\n",
    "models = ['Naive Bayes', 'Random Forest']\n",
    "accuracy = [nb_accuracy, rf_accuracy]\n",
    "f1_score = [nb_f1, rf_f1]\n",
    "auc_score = [nb_auc, rf_auc]\n",
    "training_time = [nb_training_time, rf_training_time]\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Accuracy': accuracy,\n",
    "    'F1 Score': f1_score,\n",
    "    'AUC': auc_score,\n",
    "    'Training Time (s)': training_time\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Set width of bars\n",
    "barWidth = 0.25\n",
    "\n",
    "# Set position of bars on X axis\n",
    "r1 = np.arange(len(models))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "\n",
    "# Create bars\n",
    "plt.bar(r1, accuracy, width=barWidth, label='Accuracy')\n",
    "plt.bar(r2, f1_score, width=barWidth, label='F1 Score')\n",
    "plt.bar(r3, auc_score, width=barWidth, label='AUC')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks([r + barWidth for r in range(len(models))], models)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot training time comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(models, training_time)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.title('Training Time Comparison')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models to DBFS\n",
    "\n",
    "We'll save our trained models to DBFS for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create directory for models\n",
    "dbutils.fs.mkdirs(\"/FileStore/fake_news_detection/models\")\n",
    "\n",
    "# Save feature pipeline model\n",
    "feature_model_path = \"/FileStore/fake_news_detection/models/feature_pipeline\"\n",
    "feature_model.write().overwrite().save(feature_model_path)\n",
    "\n",
    "# Save Naive Bayes model\n",
    "nb_model_path = \"/FileStore/fake_news_detection/models/naive_bayes\"\n",
    "best_nb_model.write().overwrite().save(nb_model_path)\n",
    "\n",
    "# Save Random Forest model\n",
    "rf_model_path = \"/FileStore/fake_news_detection/models/random_forest\"\n",
    "best_rf_model.write().overwrite().save(rf_model_path)\n",
    "\n",
    "print(f\"Models saved to DBFS:\")\n",
    "print(f\"- Feature Pipeline: {feature_model_path}\")\n",
    "print(f\"- Naive Bayes: {nb_model_path}\")\n",
    "print(f\"- Random Forest: {rf_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results as Hive Table\n",
    "\n",
    "We'll save our model comparison results as a Hive table for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert comparison DataFrame to Spark DataFrame\n",
    "comparison_spark_df = spark.createDataFrame(comparison_df)\n",
    "\n",
    "# Save as Hive table\n",
    "comparison_spark_df.write.mode(\"overwrite\").saveAsTable(\"model_comparison_results\")\n",
    "\n",
    "print(\"Model comparison results saved as Hive table: model_comparison_results\")\n",
    "\n",
    "# Query the table to verify\n",
    "spark.sql(\"SELECT * FROM model_comparison_results\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Deep Learning Models\n",
    "\n",
    "We'll compare our traditional models with deep learning models (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if LSTM results are available\n",
    "try:\n",
    "    lstm_results = spark.sql(\"SELECT * FROM lstm_model_results\")\n",
    "    \n",
    "    if lstm_results.count() > 0:\n",
    "        # Get LSTM metrics\n",
    "        lstm_metrics = lstm_results.collect()[0]\n",
    "        \n",
    "        # Add LSTM to comparison\n",
    "        models.append('LSTM')\n",
    "        accuracy.append(float(lstm_metrics['accuracy']))\n",
    "        f1_score.append(float(lstm_metrics['f1_score']))\n",
    "        auc_score.append(float(lstm_metrics['auc']))\n",
    "        training_time.append(float(lstm_metrics['training_time']))\n",
    "        \n",
    "        # Create updated comparison DataFrame\n",
    "        full_comparison_df = pd.DataFrame({\n",
    "            'Model': models,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1 Score': f1_score,\n",
    "            'AUC': auc_score,\n",
    "            'Training Time (s)': training_time\n",
    "        })\n",
    "        \n",
    "        print(\"Full Model Comparison (including Deep Learning):\")\n",
    "        display(full_comparison_df)\n",
    "        \n",
    "        # Plot updated comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Set width of bars\n",
    "        barWidth = 0.25\n",
    "        \n",
    "        # Set position of bars on X axis\n",
    "        r1 = np.arange(len(models))\n",
    "        r2 = [x + barWidth for x in r1]\n",
    "        r3 = [x + barWidth for x in r2]\n",
    "        \n",
    "        # Create bars\n",
    "        plt.bar(r1, accuracy, width=barWidth, label='Accuracy')\n",
    "        plt.bar(r2, f1_score, width=barWidth, label='F1 Score')\n",
    "        plt.bar(r3, auc_score, width=barWidth, label='AUC')\n",
    "        \n",
    "        # Add labels and legend\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Model Performance Comparison (including Deep Learning)')\n",
    "        plt.xticks([r + barWidth for r in range(len(models))], models)\n",
    "        plt.legend()\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot updated training time comparison\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(models, training_time)\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Training Time (seconds)')\n",
    "        plt.title('Training Time Comparison (including Deep Learning)')\n",
    "        plt.yscale('log')  # Use log scale for better visualization\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Save full comparison as Hive table\n",
    "        full_comparison_spark_df = spark.createDataFrame(full_comparison_df)\n",
    "        full_comparison_spark_df.write.mode(\"overwrite\").saveAsTable(\"full_model_comparison_results\")\n",
    "        print(\"Full model comparison results saved as Hive table: full_model_comparison_results\")\n",
    "    else:\n",
    "        print(\"LSTM results table exists but is empty. Skipping deep learning comparison.\")\n",
    "except Exception as e:\n",
    "    print(f\"LSTM results not available. Skipping deep learning comparison. Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Cleanup\n",
    "\n",
    "We'll clean up cached DataFrames to free memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Unpersist cached DataFrames\n",
    "working_df.unpersist()\n",
    "train_df.unpersist()\n",
    "test_df.unpersist()\n",
    "\n",
    "print(\"Memory cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented and evaluated traditional machine learning models (Naive Bayes and Random Forest) for fake news detection using PySpark MLlib. These models serve as strong baselines and are particularly well-suited for the Databricks Community Edition due to their efficiency.\n",
    "\n",
    "Key findings:\n",
    "1. Both Naive Bayes and Random Forest models achieve good performance on the fake news detection task.\n",
    "2. Random Forest generally outperforms Naive Bayes in terms of accuracy, F1 score, and AUC, but requires more training time.\n",
    "3. Feature importance analysis reveals the most predictive words for fake news detection.\n",
    "4. Traditional models can be trained on the full dataset even with the limited resources of the Community Edition.\n",
    "\n",
    "These models provide a solid foundation for fake news detection and can be used as baselines for comparison with more complex models like LSTM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# Last modified: May 29, 2025
