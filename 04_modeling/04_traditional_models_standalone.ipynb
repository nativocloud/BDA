{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4768c91c",
   "metadata": {},
   "source": [
    "# Fake News Detection: Traditional Machine Learning Models\n",
    "\n",
    "This notebook contains all the necessary code for implementing traditional machine learning models in the fake news detection project. The code is organized into independent functions, without dependencies on external modules or classes, to facilitate execution in Databricks Community Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac2592a",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692dcfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, regexp_replace, length, udf, concat_ws, lower, explode, array\n",
    ")\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, ArrayType, FloatType\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f6969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session optimized for Databricks Community Edition\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection_TraditionalModels\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1985a0d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Start timer for performance tracking\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f24d76",
   "metadata": {},
   "source": [
    "## Reusable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f033ec",
   "metadata": {},
   "source": [
    "### Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce038d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_data_from_hive(fake_table_name=\"fake\", true_table_name=\"real\"):\n",
    "    \"\"\"\n",
    "    Load data from Hive tables.\n",
    "    \n",
    "    Args:\n",
    "        fake_table_name (str): Name of the Hive table with fake news\n",
    "        true_table_name (str): Name of the Hive table with real news\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (real_df, fake_df) DataFrames with loaded data\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from Hive tables '{true_table_name}' and '{fake_table_name}'...\")\n",
    "    \n",
    "    # Check if tables exist\n",
    "    tables = [row.tableName for row in spark.sql(\"SHOW TABLES\").collect()]\n",
    "    \n",
    "    if true_table_name not in tables or fake_table_name not in tables:\n",
    "        raise ValueError(f\"Hive tables '{true_table_name}' and/or '{fake_table_name}' do not exist\")\n",
    "    \n",
    "    # Load data from Hive tables\n",
    "    real_df = spark.table(true_table_name)\n",
    "    fake_df = spark.table(fake_table_name)\n",
    "    \n",
    "    # Register as temporary views for SQL queries\n",
    "    real_df.createOrReplaceTempView(\"real_news\")\n",
    "    fake_df.createOrReplaceTempView(\"fake_news\")\n",
    "    \n",
    "    # Display information about DataFrames\n",
    "    print(f\"Real news loaded: {real_df.count()} records\")\n",
    "    print(f\"Fake news loaded: {fake_df.count()} records\")\n",
    "    \n",
    "    return real_df, fake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dfd791",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_preprocessed_data(path=\"dbfs:/FileStore/fake_news_detection/preprocessed_data/preprocessed_news.parquet\"):\n",
    "    \"\"\"\n",
    "    Load preprocessed data from Parquet file.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the preprocessed data Parquet file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Spark DataFrame with preprocessed data\n",
    "    \"\"\"\n",
    "    print(f\"Loading preprocessed data from {path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load data from Parquet file\n",
    "        df = spark.read.parquet(path)\n",
    "        \n",
    "        # Display basic information\n",
    "        print(f\"Successfully loaded {df.count()} records.\")\n",
    "        df.printSchema()\n",
    "        \n",
    "        # Cache the DataFrame for better performance\n",
    "        df.cache()\n",
    "        print(\"Preprocessed DataFrame cached.\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading preprocessed data: {e}\")\n",
    "        print(\"Please ensure the preprocessing notebook ran successfully and saved data to the correct path.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25612b8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def combine_datasets(real_df, fake_df):\n",
    "    \"\"\"\n",
    "    Combine real and fake news datasets with labels.\n",
    "    \n",
    "    Args:\n",
    "        real_df (DataFrame): DataFrame with real news\n",
    "        fake_df (DataFrame): DataFrame with fake news\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Combined DataFrame with label column\n",
    "    \"\"\"\n",
    "    # Add label column (1 for real, 0 for fake)\n",
    "    real_with_label = real_df.withColumn(\"label\", lit(1))\n",
    "    fake_with_label = fake_df.withColumn(\"label\", lit(0))\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_df = real_with_label.union(fake_with_label)\n",
    "    \n",
    "    # Display class distribution\n",
    "    print(\"Class distribution:\")\n",
    "    combined_df.groupBy(\"label\").count().show()\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c17c84d",
   "metadata": {},
   "source": [
    "### Memory Management Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28d930",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_stratified_sample(df, sample_size_per_class=2000, seed=42):\n",
    "    \"\"\"\n",
    "    Create a balanced sample with equal representation from each class.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to sample from\n",
    "        sample_size_per_class (int): Number of samples per class\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with balanced samples\n",
    "    \"\"\"\n",
    "    # Get class counts\n",
    "    class_counts = df.groupBy(\"label\").count().collect()\n",
    "    \n",
    "    # Calculate sampling fractions\n",
    "    fractions = {}\n",
    "    for row in class_counts:\n",
    "        label = row[\"label\"]\n",
    "        count = row[\"count\"]\n",
    "        fraction = min(1.0, sample_size_per_class / count)\n",
    "        fractions[label] = fraction\n",
    "    \n",
    "    # Create stratified sample\n",
    "    sampled_df = df.sampleBy(\"label\", fractions, seed)\n",
    "    \n",
    "    print(f\"Original class distribution:\")\n",
    "    df.groupBy(\"label\").count().show()\n",
    "    \n",
    "    print(f\"Sampled class distribution:\")\n",
    "    sampled_df.groupBy(\"label\").count().show()\n",
    "    \n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5cc9e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_working_dataset(df, use_full_dataset=True, sample_size_per_class=2000, seed=42):\n",
    "    \"\"\"\n",
    "    Prepare working dataset based on available memory.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        use_full_dataset (bool): Whether to use the full dataset or a sample\n",
    "        sample_size_per_class (int): Number of samples per class if sampling\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Working DataFrame (full or sampled)\n",
    "    \"\"\"\n",
    "    if use_full_dataset:\n",
    "        working_df = df\n",
    "        print(\"Using full dataset\")\n",
    "    else:\n",
    "        # Create a balanced sample for development/testing\n",
    "        working_df = create_stratified_sample(df, sample_size_per_class, seed)\n",
    "        print(\"Using sampled dataset\")\n",
    "\n",
    "    # Cache the working dataset for faster processing\n",
    "    working_df.cache()\n",
    "    print(f\"Working dataset size: {working_df.count()} records\")\n",
    "    \n",
    "    return working_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a439025",
   "metadata": {},
   "source": [
    "### Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959e1e3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_text(df, text_column=\"text\", title_column=\"title\"):\n",
    "    \"\"\"\n",
    "    Preprocess text data for feature extraction.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        text_column (str): Name of the text column\n",
    "        title_column (str): Name of the title column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with preprocessed text\n",
    "    \"\"\"\n",
    "    # Combine title and text fields, and perform basic cleaning\n",
    "    preprocessed_df = df.withColumn(\n",
    "        \"content\", \n",
    "        concat_ws(\" \", \n",
    "                  when(col(title_column).isNull(), \"\").otherwise(col(title_column)),\n",
    "                  when(col(text_column).isNull(), \"\").otherwise(col(text_column))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Convert to lowercase and remove special characters\n",
    "    preprocessed_df = preprocessed_df.withColumn(\"content\", lower(col(\"content\")))\n",
    "    preprocessed_df = preprocessed_df.withColumn(\n",
    "        \"content\", \n",
    "        regexp_replace(col(\"content\"), \"[^a-zA-Z0-9\\\\s]\", \" \")\n",
    "    )\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    preprocessed_df = preprocessed_df.withColumn(\n",
    "        \"content\", \n",
    "        regexp_replace(col(\"content\"), \"\\\\s+\", \" \")\n",
    "    )\n",
    "    \n",
    "    # Show sample of preprocessed content\n",
    "    preprocessed_df.select(\"content\", \"label\").show(5, truncate=50)\n",
    "    \n",
    "    return preprocessed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c532e",
   "metadata": {},
   "source": [
    "### Data Splitting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488af0c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def split_train_test(df, train_ratio=0.7, seed=42):\n",
    "    \"\"\"\n",
    "    Split data into training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame\n",
    "        train_ratio (float): Ratio of training data (0-1)\n",
    "        seed (int): Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_df, test_df) Training and testing DataFrames\n",
    "    \"\"\"\n",
    "    # Split data into training and testing sets\n",
    "    train_df, test_df = df.randomSplit([train_ratio, 1 - train_ratio], seed=seed)\n",
    "    \n",
    "    # Cache datasets for faster processing\n",
    "    train_df.cache()\n",
    "    test_df.cache()\n",
    "    \n",
    "    print(f\"Training set size: {train_df.count()} records\")\n",
    "    print(f\"Testing set size: {test_df.count()} records\")\n",
    "    \n",
    "    # Check class distribution in training set\n",
    "    print(\"Training set class distribution:\")\n",
    "    train_df.groupBy(\"label\").count().show()\n",
    "    \n",
    "    # Check class distribution in testing set\n",
    "    print(\"Testing set class distribution:\")\n",
    "    test_df.groupBy(\"label\").count().show()\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc5e5c4",
   "metadata": {},
   "source": [
    "### Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff30e22",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_tfidf_pipeline(input_col=\"content\", output_col=\"features\", num_features=10000):\n",
    "    \"\"\"\n",
    "    Create a TF-IDF feature extraction pipeline.\n",
    "    \n",
    "    Args:\n",
    "        input_col (str): Input column name\n",
    "        output_col (str): Output column name\n",
    "        num_features (int): Number of features for HashingTF\n",
    "        \n",
    "    Returns:\n",
    "        Pipeline: Spark ML Pipeline for TF-IDF feature extraction\n",
    "    \"\"\"\n",
    "    # Define feature extraction pipeline\n",
    "    tokenizer = Tokenizer(inputCol=input_col, outputCol=\"words\")\n",
    "    remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "    hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=num_features)\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=output_col)\n",
    "    \n",
    "    # Create feature extraction pipeline\n",
    "    feature_pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n",
    "    \n",
    "    return feature_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a0636",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_features(train_df, test_df, input_col=\"content\", output_col=\"features\", num_features=10000):\n",
    "    \"\"\"\n",
    "    Extract TF-IDF features from training and testing data.\n",
    "    \n",
    "    Args:\n",
    "        train_df (DataFrame): Training DataFrame\n",
    "        test_df (DataFrame): Testing DataFrame\n",
    "        input_col (str): Input column name\n",
    "        output_col (str): Output column name\n",
    "        num_features (int): Number of features for HashingTF\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (feature_model, train_features, test_features) - Fitted pipeline model and transformed DataFrames\n",
    "    \"\"\"\n",
    "    # Create feature extraction pipeline\n",
    "    feature_pipeline = create_tfidf_pipeline(input_col, output_col, num_features)\n",
    "    \n",
    "    # Fit the pipeline on the training data\n",
    "    feature_model = feature_pipeline.fit(train_df)\n",
    "    \n",
    "    # Transform the training and testing data\n",
    "    train_features = feature_model.transform(train_df)\n",
    "    test_features = feature_model.transform(test_df)\n",
    "    \n",
    "    # Show sample of features\n",
    "    train_features.select(input_col, \"filtered\", output_col, \"label\").show(2, truncate=50)\n",
    "    \n",
    "    return feature_model, train_features, test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa98a09",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5737a5ff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_evaluators():\n",
    "    \"\"\"\n",
    "    Create evaluators for model assessment.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (accuracy_evaluator, f1_evaluator, auc_evaluator) - Evaluators for different metrics\n",
    "    \"\"\"\n",
    "    # Define evaluators\n",
    "    accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"accuracy\"\n",
    "    )\n",
    "    \n",
    "    f1_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"f1\"\n",
    "    )\n",
    "    \n",
    "    auc_evaluator = BinaryClassificationEvaluator(\n",
    "        labelCol=\"label\", \n",
    "        rawPredictionCol=\"rawPrediction\", \n",
    "        metricName=\"areaUnderROC\"\n",
    "    )\n",
    "    \n",
    "    return accuracy_evaluator, f1_evaluator, auc_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86924a99",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_naive_bayes(train_features, test_features, num_folds=3):\n",
    "    \"\"\"\n",
    "    Train a Naive Bayes classifier with cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        train_features (DataFrame): Training data with features\n",
    "        test_features (DataFrame): Testing data with features\n",
    "        num_folds (int): Number of folds for cross-validation\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_model, predictions, metrics) - Best model, predictions, and performance metrics\n",
    "    \"\"\"\n",
    "    print(\"Training Naive Bayes model...\")\n",
    "    \n",
    "    # Create Naive Bayes model\n",
    "    nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n",
    "    \n",
    "    # Define parameter grid for cross-validation\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(nb.smoothing, [0.1, 0.5, 1.0]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Create evaluators\n",
    "    accuracy_evaluator, f1_evaluator, auc_evaluator = create_evaluators()\n",
    "    \n",
    "    # Create cross-validator\n",
    "    cv = CrossValidator(\n",
    "        estimator=nb,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=f1_evaluator,\n",
    "        numFolds=num_folds  # Use fewer folds for Community Edition (less resource intensive)\n",
    "    )\n",
    "    \n",
    "    # Train model with cross-validation\n",
    "    start_time = time.time()\n",
    "    cv_model = cv.fit(train_features)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Naive Bayes training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = cv_model.bestModel\n",
    "    print(f\"Best smoothing parameter: {best_model.getSmoothing()}\")\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    predictions = best_model.transform(test_features)\n",
    "    \n",
    "    # Evaluate model\n",
    "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "    f1 = f1_evaluator.evaluate(predictions)\n",
    "    auc = auc_evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(f\"Naive Bayes Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Naive Bayes F1 Score: {f1:.4f}\")\n",
    "    print(f\"Naive Bayes AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Collect metrics\n",
    "    metrics = {\n",
    "        \"model\": \"Naive Bayes\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"training_time\": training_time\n",
    "    }\n",
    "    \n",
    "    return best_model, predictions, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992ce00",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_random_forest(train_features, test_features, num_folds=3):\n",
    "    \"\"\"\n",
    "    Train a Random Forest classifier with cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        train_features (DataFrame): Training data with features\n",
    "        test_features (DataFrame): Testing data with features\n",
    "        num_folds (int): Number of folds for cross-validation\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_model, predictions, metrics) - Best model, predictions, and performance metrics\n",
    "    \"\"\"\n",
    "    print(\"Training Random Forest model...\")\n",
    "    \n",
    "    # Create Random Forest model\n",
    "    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "    \n",
    "    # Define parameter grid for cross-validation\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [10, 20]) \\\n",
    "        .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Create evaluators\n",
    "    accuracy_evaluator, f1_evaluator, auc_evaluator = create_evaluators()\n",
    "    \n",
    "    # Create cross-validator\n",
    "    cv = CrossValidator(\n",
    "        estimator=rf,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=f1_evaluator,\n",
    "        numFolds=num_folds  # Use fewer folds for Community Edition (less resource intensive)\n",
    "    )\n",
    "    \n",
    "    # Train model with cross-validation\n",
    "    start_time = time.time()\n",
    "    cv_model = cv.fit(train_features)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Random Forest training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = cv_model.bestModel\n",
    "    print(f\"Best numTrees: {best_model.getNumTrees}\")\n",
    "    print(f\"Best maxDepth: {best_model.getMaxDepth()}\")\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    predictions = best_model.transform(test_features)\n",
    "    \n",
    "    # Evaluate model\n",
    "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "    f1 = f1_evaluator.evaluate(predictions)\n",
    "    auc = auc_evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Random Forest F1 Score: {f1:.4f}\")\n",
    "    print(f\"Random Forest AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Collect metrics\n",
    "    metrics = {\n",
    "        \"model\": \"Random Forest\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"training_time\": training_time\n",
    "    }\n",
    "    \n",
    "    return best_model, predictions, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d5809",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_logistic_regression(train_features, test_features, num_folds=3):\n",
    "    \"\"\"\n",
    "    Train a Logistic Regression classifier with cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        train_features (DataFrame): Training data with features\n",
    "        test_features (DataFrame): Testing data with features\n",
    "        num_folds (int): Number of folds for cross-validation\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_model, predictions, metrics) - Best model, predictions, and performance metrics\n",
    "    \"\"\"\n",
    "    print(\"Training Logistic Regression model...\")\n",
    "    \n",
    "    # Create Logistic Regression model\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "    \n",
    "    # Define parameter grid for cross-validation\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.01, 0.1, 0.3]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Create evaluators\n",
    "    accuracy_evaluator, f1_evaluator, auc_evaluator = create_evaluators()\n",
    "    \n",
    "    # Create cross-validator\n",
    "    cv = CrossValidator(\n",
    "        estimator=lr,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=f1_evaluator,\n",
    "        numFolds=num_folds  # Use fewer folds for Community Edition (less resource intensive)\n",
    "    )\n",
    "    \n",
    "    # Train model with cross-validation\n",
    "    start_time = time.time()\n",
    "    cv_model = cv.fit(train_features)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Logistic Regression training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = cv_model.bestModel\n",
    "    print(f\"Best regParam: {best_model.getRegParam()}\")\n",
    "    print(f\"Best elasticNetParam: {best_model.getElasticNetParam()}\")\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    predictions = best_model.transform(test_features)\n",
    "    \n",
    "    # Evaluate model\n",
    "    accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "    f1 = f1_evaluator.evaluate(predictions)\n",
    "    auc = auc_evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Logistic Regression F1 Score: {f1:.4f}\")\n",
    "    print(f\"Logistic Regression AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Collect metrics\n",
    "    metrics = {\n",
    "        \"model\": \"Logistic Regression\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"training_time\": training_time\n",
    "    }\n",
    "    \n",
    "    return best_model, predictions, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02377eba",
   "metadata": {},
   "source": [
    "### Feature Importance and Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5497044",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, vocabulary=None, top_n=20):\n",
    "    \"\"\"\n",
    "    Analyze feature importance from a trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model with feature importances\n",
    "        vocabulary (list): List of feature names (optional)\n",
    "        top_n (int): Number of top features to display\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with feature importances\n",
    "    \"\"\"\n",
    "    print(\"Analyzing feature importance...\")\n",
    "    \n",
    "    # Check if model has feature importances\n",
    "    if hasattr(model, 'featureImportances'):\n",
    "        # Get feature importances from model\n",
    "        feature_importances = model.featureImportances.toArray()\n",
    "        \n",
    "        # Get top N feature indices\n",
    "        top_indices = np.argsort(-feature_importances)[:top_n]\n",
    "        top_importances = feature_importances[top_indices]\n",
    "        \n",
    "        # Create DataFrame for visualization\n",
    "        if vocabulary and len(vocabulary) >= max(top_indices):\n",
    "            # If vocabulary is provided, use feature names\n",
    "            feature_names = [vocabulary[i] for i in top_indices]\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': top_importances\n",
    "            })\n",
    "        else:\n",
    "            # Otherwise use feature indices\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature Index': top_indices,\n",
    "                'Importance': top_importances\n",
    "            })\n",
    "        \n",
    "        # Plot feature importances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(len(importance_df)), importance_df['Importance'])\n",
    "        \n",
    "        if vocabulary and len(vocabulary) >= max(top_indices):\n",
    "            plt.yticks(range(len(importance_df)), importance_df['Feature'])\n",
    "        else:\n",
    "            plt.yticks(range(len(importance_df)), [f\"Feature {idx}\" for idx in importance_df['Feature Index']])\n",
    "            \n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title(f'Top {top_n} Feature Importances')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return importance_df\n",
    "    else:\n",
    "        print(\"Model does not have feature importances attribute.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce9601",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compare_models(metrics_list):\n",
    "    \"\"\"\n",
    "    Compare performance of multiple models.\n",
    "    \n",
    "    Args:\n",
    "        metrics_list (list): List of model metrics dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with model comparison\n",
    "    \"\"\"\n",
    "    # Create comparison DataFrame\n",
    "    models = [metrics['model'] for metrics in metrics_list]\n",
    "    accuracy = [metrics['accuracy'] for metrics in metrics_list]\n",
    "    f1_score = [metrics['f1'] for metrics in metrics_list]\n",
    "    auc_score = [metrics['auc'] for metrics in metrics_list]\n",
    "    training_time = [metrics['training_time'] for metrics in metrics_list]\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': models,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1_score,\n",
    "        'AUC': auc_score,\n",
    "        'Training Time (s)': training_time\n",
    "    })\n",
    "    \n",
    "    # Display comparison\n",
    "    print(\"Model Comparison:\")\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot accuracy, F1, and AUC\n",
    "    plt.subplot(2, 1, 1)\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, accuracy, width, label='Accuracy')\n",
    "    plt.bar(x, f1_score, width, label='F1 Score')\n",
    "    plt.bar(x + width, auc_score, width, label='AUC')\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.xticks(x, models)\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot training time\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.bar(x, training_time, color='green', alpha=0.7)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Training Time (seconds)')\n",
    "    plt.title('Model Training Time Comparison')\n",
    "    plt.xticks(x, models)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81d4740",
   "metadata": {},
   "source": [
    "### Model Saving and Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaccce9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    \"\"\"\n",
    "    Save a trained model to disk.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model to save\n",
    "        path (str): Path where to save the model\n",
    "    \"\"\"\n",
    "    print(f\"Saving model to {path}...\")\n",
    "    \n",
    "    try:\n",
    "        model.write().overwrite().save(path)\n",
    "        print(\"Model saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa296d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_metrics(metrics_list, path):\n",
    "    \"\"\"\n",
    "    Save model metrics to disk.\n",
    "    \n",
    "    Args:\n",
    "        metrics_list (list): List of model metrics dictionaries\n",
    "        path (str): Path where to save the metrics\n",
    "    \"\"\"\n",
    "    print(f\"Saving metrics to {path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Convert to DataFrame\n",
    "        metrics_df = pd.DataFrame(metrics_list)\n",
    "        \n",
    "        # Save to CSV\n",
    "        metrics_df.to_csv(path, index=False)\n",
    "        print(\"Metrics saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95eb4e0",
   "metadata": {},
   "source": [
    "## Complete Modeling Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11928bbb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(\n",
    "    input_path=\"dbfs:/FileStore/fake_news_detection/preprocessed_data/preprocessed_news.parquet\",\n",
    "    output_dir=\"dbfs:/FileStore/fake_news_detection/models\",\n",
    "    use_full_dataset=True,\n",
    "    sample_size_per_class=2000,\n",
    "    num_features=10000,\n",
    "    num_folds=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete pipeline for training and evaluating traditional machine learning models.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): Path to preprocessed data\n",
    "        output_dir (str): Directory to save models and results\n",
    "        use_full_dataset (bool): Whether to use the full dataset or a sample\n",
    "        sample_size_per_class (int): Number of samples per class if sampling\n",
    "        num_features (int): Number of features for TF-IDF\n",
    "        num_folds (int): Number of folds for cross-validation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with references to trained models and results\n",
    "    \"\"\"\n",
    "    print(\"Starting traditional machine learning modeling pipeline...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create output directories\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(output_dir.replace(\"dbfs:\", \"\"))\n",
    "    except:\n",
    "        print(\"Warning: Could not create directories. This is expected in local environments.\")\n",
    "    \n",
    "    # 1. Load preprocessed data\n",
    "    df = load_preprocessed_data(input_path)\n",
    "    if df is None:\n",
    "        print(\"Error: Could not load preprocessed data. Pipeline aborted.\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Prepare working dataset\n",
    "    working_df = prepare_working_dataset(df, use_full_dataset, sample_size_per_class)\n",
    "    \n",
    "    # 3. Preprocess text\n",
    "    preprocessed_df = preprocess_text(working_df)\n",
    "    \n",
    "    # 4. Split data into training and testing sets\n",
    "    train_df, test_df = split_train_test(preprocessed_df)\n",
    "    \n",
    "    # 5. Extract features\n",
    "    feature_model, train_features, test_features = extract_features(\n",
    "        train_df, test_df, input_col=\"content\", output_col=\"features\", num_features=num_features\n",
    "    )\n",
    "    \n",
    "    # 6. Train and evaluate models\n",
    "    metrics_list = []\n",
    "    \n",
    "    # 6.1. Naive Bayes\n",
    "    nb_model, nb_predictions, nb_metrics = train_naive_bayes(train_features, test_features, num_folds)\n",
    "    metrics_list.append(nb_metrics)\n",
    "    \n",
    "    # 6.2. Random Forest\n",
    "    rf_model, rf_predictions, rf_metrics = train_random_forest(train_features, test_features, num_folds)\n",
    "    metrics_list.append(rf_metrics)\n",
    "    \n",
    "    # 6.3. Logistic Regression\n",
    "    lr_model, lr_predictions, lr_metrics = train_logistic_regression(train_features, test_features, num_folds)\n",
    "    metrics_list.append(lr_metrics)\n",
    "    \n",
    "    # 7. Compare models\n",
    "    comparison_df = compare_models(metrics_list)\n",
    "    \n",
    "    # 8. Analyze feature importance (for Random Forest)\n",
    "    importance_df = analyze_feature_importance(rf_model)\n",
    "    \n",
    "    # 9. Save models\n",
    "    save_model(nb_model, f\"{output_dir}/naive_bayes_model\")\n",
    "    save_model(rf_model, f\"{output_dir}/random_forest_model\")\n",
    "    save_model(lr_model, f\"{output_dir}/logistic_regression_model\")\n",
    "    save_model(feature_model, f\"{output_dir}/tfidf_feature_model\")\n",
    "    \n",
    "    # 10. Save metrics\n",
    "    save_metrics(metrics_list, f\"{output_dir}/model_metrics.csv\")\n",
    "    \n",
    "    print(f\"\\nTraditional machine learning modeling pipeline completed in {time.time() - start_time:.2f} seconds!\")\n",
    "    \n",
    "    return {\n",
    "        \"naive_bayes_model\": nb_model,\n",
    "        \"random_forest_model\": rf_model,\n",
    "        \"logistic_regression_model\": lr_model,\n",
    "        \"feature_model\": feature_model,\n",
    "        \"metrics\": metrics_list,\n",
    "        \"comparison\": comparison_df,\n",
    "        \"importance\": importance_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8801ae",
   "metadata": {},
   "source": [
    "## Step-by-Step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f000a",
   "metadata": {},
   "source": [
    "### 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeb7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "preprocessed_df = load_preprocessed_data()\n",
    "\n",
    "# Prepare working dataset (use full dataset or sample)\n",
    "if preprocessed_df:\n",
    "    working_df = prepare_working_dataset(preprocessed_df, use_full_dataset=True)\n",
    "    \n",
    "    # Preprocess text\n",
    "    preprocessed_df = preprocess_text(working_df)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    train_df, test_df = split_train_test(preprocessed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f695f9f8",
   "metadata": {},
   "source": [
    "### 2. Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0307a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TF-IDF features\n",
    "if 'train_df' in locals() and 'test_df' in locals():\n",
    "    feature_model, train_features, test_features = extract_features(\n",
    "        train_df, test_df, input_col=\"content\", output_col=\"features\", num_features=10000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d4d68",
   "metadata": {},
   "source": [
    "### 3. Train Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes model\n",
    "if 'train_features' in locals() and 'test_features' in locals():\n",
    "    nb_model, nb_predictions, nb_metrics = train_naive_bayes(train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f112d",
   "metadata": {},
   "source": [
    "### 4. Train Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "if 'train_features' in locals() and 'test_features' in locals():\n",
    "    rf_model, rf_predictions, rf_metrics = train_random_forest(train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482bbe86",
   "metadata": {},
   "source": [
    "### 5. Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded58991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "if 'train_features' in locals() and 'test_features' in locals():\n",
    "    lr_model, lr_predictions, lr_metrics = train_logistic_regression(train_features, test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256d088",
   "metadata": {},
   "source": [
    "### 6. Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "if 'nb_metrics' in locals() and 'rf_metrics' in locals() and 'lr_metrics' in locals():\n",
    "    metrics_list = [nb_metrics, rf_metrics, lr_metrics]\n",
    "    comparison_df = compare_models(metrics_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b77b1",
   "metadata": {},
   "source": [
    "### 7. Analyze Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance from Random Forest model\n",
    "if 'rf_model' in locals():\n",
    "    importance_df = analyze_feature_importance(rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a6a6fa",
   "metadata": {},
   "source": [
    "### 8. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81aafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and results\n",
    "if 'nb_model' in locals() and 'rf_model' in locals() and 'lr_model' in locals():\n",
    "    output_dir = \"dbfs:/FileStore/fake_news_detection/models\"\n",
    "    \n",
    "    # Save models\n",
    "    save_model(nb_model, f\"{output_dir}/naive_bayes_model\")\n",
    "    save_model(rf_model, f\"{output_dir}/random_forest_model\")\n",
    "    save_model(lr_model, f\"{output_dir}/logistic_regression_model\")\n",
    "    save_model(feature_model, f\"{output_dir}/tfidf_feature_model\")\n",
    "    \n",
    "    # Save metrics\n",
    "    save_metrics(metrics_list, f\"{output_dir}/model_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313d5c4",
   "metadata": {},
   "source": [
    "### 9. Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete modeling pipeline\n",
    "results = train_and_evaluate_models(\n",
    "    input_path=\"dbfs:/FileStore/fake_news_detection/preprocessed_data/preprocessed_news.parquet\",\n",
    "    output_dir=\"dbfs:/FileStore/fake_news_detection/models\",\n",
    "    use_full_dataset=True,\n",
    "    sample_size_per_class=2000,\n",
    "    num_features=10000,\n",
    "    num_folds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e99ee2",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "1. **Model Selection**: We implemented three traditional machine learning models (Naive Bayes, Random Forest, and Logistic Regression) that serve as strong baselines for fake news detection.\n",
    "\n",
    "2. **Feature Engineering**: We used TF-IDF (Term Frequency-Inverse Document Frequency) to convert text into numerical features, which is a proven technique for text classification tasks.\n",
    "\n",
    "3. **Cross-Validation**: We used cross-validation to ensure robust model evaluation and hyperparameter tuning, which helps prevent overfitting.\n",
    "\n",
    "4. **Memory Management**: The code includes options for using the full dataset or a stratified sample, which is useful for environments with limited resources like Databricks Community Edition.\n",
    "\n",
    "5. **Vectorization**: All operations are vectorized using Spark's distributed processing capabilities, ensuring efficient computation even with large datasets.\n",
    "\n",
    "6. **Feature Importance**: We analyzed feature importance from the Random Forest model to understand what words are most predictive of fake news.\n",
    "\n",
    "7. **Model Comparison**: We compared the performance of all models using multiple metrics (accuracy, F1 score, AUC) to provide a comprehensive evaluation.\n",
    "\n",
    "8. **Databricks Integration**: The code is optimized for Databricks Community Edition with appropriate configurations for memory and processing."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
