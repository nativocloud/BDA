{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Machine Learning Models for Fake News Detection\n",
    "\n",
    "This notebook implements and evaluates traditional machine learning models (Naive Bayes and Random Forest) for fake news detection using PySpark MLlib. These models serve as strong baselines and are particularly well-suited for the Databricks Community Edition due to their efficiency.\n",
    "\n",
    "## Models Implemented\n",
    "1. TF-IDF + Naive Bayes\n",
    "2. TF-IDF + Random Forest\n",
    "\n",
    "## Key Features\n",
    "- Distributed processing with PySpark MLlib\n",
    "- Cross-validation for robust evaluation\n",
    "- Feature importance analysis\n",
    "- Optimized for Databricks Community Edition\n",
    "- Comparison with deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, we'll set up our Spark session with configurations optimized for the Databricks Community Edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, regexp_replace, length, udf, array, struct\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, ArrayType\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Configure Spark session optimized for Databricks Community Edition\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection_TraditionalModels\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We'll load data directly from the Hive metastore tables ('fake' and 'real')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import the HiveDataIngestion class\n",
    "import sys\n",
    "sys.path.append('/dbfs/FileStore/tables')\n",
    "from hive_data_ingestion import HiveDataIngestion\n",
    "\n",
    "# Create an instance\n",
    "data_ingestion = HiveDataIngestion(spark)\n",
    "\n",
    "# Load data from Hive tables\n",
    "start_time = time.time()\n",
    "real_df, fake_df = data_ingestion.load_data_from_hive()\n",
    "print(f\"Data loading time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Combine datasets with labels\n",
    "combined_df = data_ingestion.combine_datasets(real_df, fake_df)\n",
    "print(f\"Combined dataset size: {combined_df.count()} records\")\n",
    "\n",
    "# Display class distribution\n",
    "combined_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management for Community Edition\n",
    "\n",
    "The Databricks Community Edition has limited resources (15.3 GB memory, 2 cores). We'll implement strategies to manage memory efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to create a stratified sample if needed\n",
    "def create_stratified_sample(df, sample_size_per_class=2000, seed=42):\n",
    "    \"\"\"\n",
    "    Create a balanced sample with equal representation from each class.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to sample from\n",
    "        sample_size_per_class: Number of samples per class\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with balanced samples\n",
    "    \"\"\"\n",
    "    # Get class counts\n",
    "    class_counts = df.groupBy(\"label\").count().collect()\n",
    "    \n",
    "    # Calculate sampling fractions\n",
    "    fractions = {}\n",
    "    for row in class_counts:\n",
    "        label = row[\"label\"]\n",
    "        count = row[\"count\"]\n",
    "        fraction = min(1.0, sample_size_per_class / count)\n",
    "        fractions[label] = fraction\n",
    "    \n",
    "    # Create stratified sample\n",
    "    sampled_df = df.sampleBy(\"label\", fractions, seed)\n",
    "    \n",
    "    print(f\"Original class distribution:\")\n",
    "    df.groupBy(\"label\").count().show()\n",
    "    \n",
    "    print(f\"Sampled class distribution:\")\n",
    "    sampled_df.groupBy(\"label\").count().show()\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "# Check if we need to sample based on available memory\n",
    "# For traditional models, we'll try to use the full dataset first\n",
    "use_full_dataset = True\n",
    "\n",
    "if use_full_dataset:\n",
    "    working_df = combined_df\n",
    "    print(\"Using full dataset\")\n",
    "else:\n",
    "    # Create a balanced sample for development/testing\n",
    "    working_df = create_stratified_sample(combined_df, sample_size_per_class=2000)\n",
    "    print(\"Using sampled dataset\")\n",
    "\n",
    "# Cache the working dataset for faster processing\n",
    "working_df.cache()\n",
    "print(f\"Working dataset size: {working_df.count()} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "We'll preprocess the text data to prepare it for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import concat_ws, lower, regexp_replace\n",
    "\n",
    "# Combine title and text fields, and perform basic cleaning\n",
    "preprocessed_df = working_df.withColumn(\n",
    "    \"content\", \n",
    "    concat_ws(\" \", \n",
    "              when(col(\"title\").isNull(), \"\").otherwise(col(\"title\")),\n",
    "              when(col(\"text\").isNull(), \"\").otherwise(col(\"text\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert to lowercase and remove special characters\n",
    "preprocessed_df = preprocessed_df.withColumn(\"content\", lower(col(\"content\")))\n",
    "preprocessed_df = preprocessed_df.withColumn(\n",
    "    \"content\", \n",
    "    regexp_replace(col(\"content\"), \"[^a-zA-Z0-9\\s]\", \" \")\n",
    ")\n",
    "\n",
    "# Remove extra whitespace\n",
    "preprocessed_df = preprocessed_df.withColumn(\n",
    "    \"content\", \n",
    "    regexp_replace(col(\"content\"), \"\\s+\", \" \")\n",
    ")\n",
    "\n",
    "# Show sample of preprocessed content\n",
    "preprocessed_df.select(\"content\", \"label\").show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "We'll split our data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data into training and testing sets (70% train, 30% test)\n",
    "train_df, test_df = preprocessed_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Cache datasets for faster processing\n",
    "train_df.cache()\n",
    "test_df.cache()\n",
    "\n",
    "print(f\"Training set size: {train_df.count()} records\")\n",
    "print(f\"Testing set size: {test_df.count()} records\")\n",
    "\n",
    "# Check class distribution in training set\n",
    "print(\"Training set class distribution:\")\n",
    "train_df.groupBy(\"label\").count().show()\n",
    "\n",
    "# Check class distribution in testing set\n",
    "print(\"Testing set class distribution:\")\n",
    "test_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with TF-IDF\n",
    "\n",
    "We'll use TF-IDF (Term Frequency-Inverse Document Frequency) to convert text into numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define feature extraction pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Create feature extraction pipeline\n",
    "feature_pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "feature_model = feature_pipeline.fit(train_df)\n",
    "\n",
    "# Transform the training and testing data\n",
    "train_features = feature_model.transform(train_df)\n",
    "test_features = feature_model.transform(test_df)\n",
    "\n",
    "# Show sample of features\n",
    "train_features.select(\"content\", \"filtered\", \"features\", \"label\").show(2, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Naive Bayes\n",
    "\n",
    "We'll implement a Naive Bayes classifier using PySpark MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create Naive Bayes model\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Define parameter grid for cross-validation\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(nb.smoothing, [0.1, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Define evaluators\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Create cross-validator\n",
    "cv = CrossValidator(\n",
    "    estimator=nb,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=f1_evaluator,\n",
    "    numFolds=3  # Use 3 folds for Community Edition (less resource intensive)\n",
    ")\n",
    "\n",
    "# Train model with cross-validation\n",
    "start_time = time.time()\n",
    "cv_model = cv.fit(train_features)\n",
    "nb_training_time = time.time() - start_time\n",
    "print(f\"Naive Bayes training time: {nb_training_time:.2f} seconds\")\n",
    "\n",
    "# Get best model\n",
    "best_nb_model = cv_model.bestModel\n",
    "print(f\"Best smoothing parameter: {best_nb_model.getSmoothing()}\")\n",
    "\n",
    "# Make predictions on test data\n",
    "nb_predictions = best_nb_model.transform(test_features)\n",
    "\n",
    "# Evaluate model\n",
    "nb_accuracy = accuracy_evaluator.evaluate(nb_predictions)\n",
    "nb_f1 = f1_evaluator.evaluate(nb_predictions)\n",
    "nb_auc = auc_evaluator.evaluate(nb_predictions)\n",
    "\n",
    "print(f\"Naive Bayes Accuracy: {nb_accuracy:.4f}\")\n",
    "print(f\"Naive Bayes F1 Score: {nb_f1:.4f}\")\n",
    "print(f\"Naive Bayes AUC: {nb_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Random Forest\n",
    "\n",
    "We'll implement a Random Forest classifier using PySpark MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create Random Forest model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Define parameter grid for cross-validation\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Create cross-validator\n",
    "cv = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=f1_evaluator,\n",
    "    numFolds=3  # Use 3 folds for Community Edition (less resource intensive)\n",
    ")\n",
    "\n",
    "# Train model with cross-validation\n",
    "start_time = time.time()\n",
    "cv_model = cv.fit(train_features)\n",
    "rf_training_time = time.time() - start_time\n",
    "print(f\"Random Forest training time: {rf_training_time:.2f} seconds\")\n",
    "\n",
    "# Get best model\n",
    "best_rf_model = cv_model.bestModel\n",
    "print(f\"Best numTrees: {best_rf_model.getNumTrees}\")\n",
    "print(f\"Best maxDepth: {best_rf_model.getMaxDepth()}\")\n",
    "\n",
    "# Make predictions on test data\n",
    "rf_predictions = best_rf_model.transform(test_features)\n",
    "\n",
    "# Evaluate model\n",
    "rf_accuracy = accuracy_evaluator.evaluate(rf_predictions)\n",
    "rf_f1 = f1_evaluator.evaluate(rf_predictions)\n",
    "rf_auc = auc_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Random Forest F1 Score: {rf_f1:.4f}\")\n",
    "print(f\"Random Forest AUC: {rf_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "We'll analyze feature importance from the Random Forest model to understand what words are most predictive of fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get feature importances from Random Forest model\n",
    "feature_importances = best_rf_model.featureImportances.toArray()\n",
    "\n",
    "# Get top 20 feature indices\n",
    "top_indices = np.argsort(-feature_importances)[:20]\n",
    "top_importances = feature_importances[top_indices]\n",
    "\n",
    "# Create a Spark DataFrame for visualization (Databricks-native approach)\n",
    "feature_importance_data = [(int(idx), float(imp)) for idx, imp in zip(top_indices, top_importances)]\n",
    "schema = [\"Feature_Index\", \"Importance\"]\n",
    "feature_importance_spark_df = spark.createDataFrame(feature_importance_data, schema)\n",
    "\n",
    "# Sort by importance for better visualization\n",
    "feature_importance_spark_df = feature_importance_spark_df.orderBy(\"Importance\", ascending=False)\n",
    "\n",
    "# Display using Databricks native visualization\n",
    "print(\"Top 20 Feature Importances:\")\n",
    "display(feature_importance_spark_df)\n",
    "\n",
    "# Save feature importance data for future reference\n",
    "feature_importance_path = \"dbfs:/FileStore/fake_news_detection/results/feature_importance.parquet\"\n",
    "feature_importance_spark_df.write.mode(\"overwrite\").parquet(feature_importance_path)\n",
    "print(f\"Feature importance data saved to {feature_importance_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "We'll compare the performance of Naive Bayes and Random Forest models using Databricks native visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create comparison DataFrame using Spark (Databricks-native approach)\n",
    "model_comparison_data = [\n",
    "    (\"Naive Bayes\", float(nb_accuracy), float(nb_f1), float(nb_auc), float(nb_training_time)),\n",
    "    (\"Random Forest\", float(rf_accuracy), float(rf_f1), float(rf_auc), float(rf_training_time))\n",
    "]\n",
    "comparison_schema = [\"Model\", \"Accuracy\", \"F1_Score\", \"AUC\", \"Training_Time_Seconds\"]\n",
    "model_comparison_df = spark.createDataFrame(model_comparison_data, comparison_schema)\n",
    "\n",
    "# Display comparison using Databricks native visualization\n",
    "print(\"Model Performance Comparison:\")\n",
    "display(model_comparison_df)\n",
    "\n",
    "# Save comparison data for future reference\n",
    "comparison_path = \"dbfs:/FileStore/fake_news_detection/results/model_comparison.parquet\"\n",
    "model_comparison_df.write.mode(\"overwrite\").parquet(comparison_path)\n",
    "print(f\"Model comparison data saved to {comparison_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis\n",
    "\n",
    "We'll analyze the confusion matrices for both models to understand their error patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import count, col, when\n",
    "\n",
    "# Function to create confusion matrix using Spark\n",
    "def create_confusion_matrix(predictions_df, model_name):\n",
    "    # Create confusion matrix\n",
    "    confusion_matrix = predictions_df.groupBy(\"label\").pivot(\"prediction\").count().fillna(0)\n",
    "    \n",
    "    # Display confusion matrix\n",
    "    print(f\"Confusion Matrix for {model_name}:\")\n",
    "    display(confusion_matrix)\n",
    "    \n",
    "    # Calculate metrics by class\n",
    "    true_positives = predictions_df.filter((col(\"label\") == 1) & (col(\"prediction\") == 1)).count()\n",
    "    false_positives = predictions_df.filter((col(\"label\") == 0) & (col(\"prediction\") == 1)).count()\n",
    "    true_negatives = predictions_df.filter((col(\"label\") == 0) & (col(\"prediction\") == 0)).count()\n",
    "    false_negatives = predictions_df.filter((col(\"label\") == 1) & (col(\"prediction\") == 0)).count()\n",
    "    \n",
    "    # Calculate precision, recall, and F1 for each class\n",
    "    precision_pos = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall_pos = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_pos = 2 * precision_pos * recall_pos / (precision_pos + recall_pos) if (precision_pos + recall_pos) > 0 else 0\n",
    "    \n",
    "    precision_neg = true_negatives / (true_negatives + false_negatives) if (true_negatives + false_negatives) > 0 else 0\n",
    "    recall_neg = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n",
    "    f1_neg = 2 * precision_neg * recall_neg / (precision_neg + recall_neg) if (precision_neg + recall_neg) > 0 else 0\n",
    "    \n",
    "    # Create metrics DataFrame\n",
    "    metrics_data = [\n",
    "        (\"Real News (1)\", float(precision_pos), float(recall_pos), float(f1_pos)),\n",
    "        (\"Fake News (0)\", float(precision_neg), float(recall_neg), float(f1_neg))\n",
    "    ]\n",
    "    metrics_schema = [\"Class\", \"Precision\", \"Recall\", \"F1_Score\"]\n",
    "    metrics_df = spark.createDataFrame(metrics_data, metrics_schema)\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"Class-wise Metrics for {model_name}:\")\n",
    "    display(metrics_df)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_path = f\"dbfs:/FileStore/fake_news_detection/results/{model_name.lower().replace(' ', '_')}_metrics.parquet\"\n",
    "    metrics_df.write.mode(\"overwrite\").parquet(metrics_path)\n",
    "    print(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "# Create confusion matrices for both models\n",
    "create_confusion_matrix(nb_predictions, \"Naive Bayes\")\n",
    "create_confusion_matrix(rf_predictions, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models\n",
    "\n",
    "We'll save the trained models for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create directory for models\n",
    "models_dir = \"dbfs:/FileStore/fake_news_detection/models\"\n",
    "nb_model_path = f\"{models_dir}/naive_bayes_model\"\n",
    "rf_model_path = f\"{models_dir}/random_forest_model\"\n",
    "\n",
    "# Save Naive Bayes model\n",
    "best_nb_model.write().overwrite().save(nb_model_path)\n",
    "print(f\"Naive Bayes model saved to {nb_model_path}\")\n",
    "\n",
    "# Save Random Forest model\n",
    "best_rf_model.write().overwrite().save(rf_model_path)\n",
    "print(f\"Random Forest model saved to {rf_model_path}\")\n",
    "\n",
    "# Save feature pipeline\n",
    "feature_pipeline_path = f\"{models_dir}/feature_pipeline\"\n",
    "feature_model.write().overwrite().save(feature_pipeline_path)\n",
    "print(f\"Feature pipeline saved to {feature_pipeline_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we implemented and evaluated two traditional machine learning models for fake news detection using PySpark MLlib:\n",
    "\n",
    "1. **Naive Bayes**: A simple probabilistic classifier based on Bayes' theorem\n",
    "2. **Random Forest**: An ensemble learning method that constructs multiple decision trees\n",
    "\n",
    "Both models were trained on the full dataset and evaluated using cross-validation. The results show that these traditional models can achieve good performance on the fake news detection task, making them strong baselines for comparison with more complex models.\n",
    "\n",
    "The implementation is optimized for the Databricks Community Edition, with careful memory management and efficient distributed processing using PySpark MLlib. All visualizations use Databricks-native display functions for better integration with the Databricks environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Compare these traditional models with deep learning approaches (LSTM, Transformers)\n",
    "2. Explore more advanced feature engineering techniques\n",
    "3. Implement model deployment for real-time inference\n",
    "4. Analyze model interpretability to understand prediction factors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
