{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e63b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformer model utility functions for fake news detection project.\n",
    "This module contains functions for creating and training transformer-based models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cdbf00",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff473e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    \"\"\"Dataset for transformer-based models.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of text documents\n",
    "            labels (list): List of labels\n",
    "            tokenizer: Transformer tokenizer\n",
    "            max_length (int): Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5fca61",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_transformer_data(spark_df, text_col=\"text\", label_col=\"label\"):\n",
    "    \"\"\"\n",
    "    Prepare data for transformer models.\n",
    "    \n",
    "    Args:\n",
    "        spark_df: Spark DataFrame\n",
    "        text_col (str): Column containing text data\n",
    "        label_col (str): Column containing labels\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Convert to pandas\n",
    "    pandas_df = spark_df.select(text_col, label_col).toPandas()\n",
    "    \n",
    "    return pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3a040e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_transformer_model(model_name=\"distilbert-base-uncased\", num_labels=2):\n",
    "    \"\"\"\n",
    "    Create a transformer model for text classification.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the pretrained model\n",
    "        num_labels (int): Number of output labels\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226f686",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_transformer_model(model, tokenizer, train_texts, train_labels, val_texts, val_labels, \n",
    "                           batch_size=8, epochs=3, learning_rate=2e-5, max_length=512, \n",
    "                           model_path=None, device=None):\n",
    "    \"\"\"\n",
    "    Train a transformer model.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model\n",
    "        tokenizer: Transformer tokenizer\n",
    "        train_texts (list): Training texts\n",
    "        train_labels (list): Training labels\n",
    "        val_texts (list): Validation texts\n",
    "        val_labels (list): Validation labels\n",
    "        batch_size (int): Batch size\n",
    "        epochs (int): Number of epochs\n",
    "        learning_rate (float): Learning rate\n",
    "        max_length (int): Maximum sequence length\n",
    "        model_path (str): Path to save the model\n",
    "        device: Torch device\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trained_model, training_stats)\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = NewsDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "    val_dataset = NewsDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Prepare optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    training_stats = []\n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            model.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                true = labels.cpu().numpy()\n",
    "                \n",
    "                val_preds.extend(preds)\n",
    "                val_true.extend(true)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        \n",
    "        # Save stats\n",
    "        epoch_stats = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_accuracy': val_accuracy\n",
    "        }\n",
    "        \n",
    "        training_stats.append(epoch_stats)\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if model_path and val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "            model.save_pretrained(model_path)\n",
    "            tokenizer.save_pretrained(os.path.join(os.path.dirname(model_path), 'tokenizer'))\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return model, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe503e5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_transformer_model(model, tokenizer, test_texts, test_labels, batch_size=8, max_length=512, log_dir=None, device=None):\n",
    "    \"\"\"\n",
    "    Evaluate a transformer model.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model\n",
    "        tokenizer: Transformer tokenizer\n",
    "        test_texts (list): Test texts\n",
    "        test_labels (list): Test labels\n",
    "        batch_size (int): Batch size\n",
    "        max_length (int): Maximum sequence length\n",
    "        log_dir (str): Directory to save evaluation logs\n",
    "        device: Torch device\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    test_dataset = NewsDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluation\"):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            true = labels.cpu().numpy()\n",
    "            \n",
    "            test_preds.extend(preds)\n",
    "            test_true.extend(true)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(test_true, test_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(test_true, test_preds, average='binary')\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "    \n",
    "    # Log metrics\n",
    "    if log_dir:\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        with open(os.path.join(log_dir, \"transformer_evaluation_metrics.txt\"), \"w\") as f:\n",
    "            f.write(f\"Transformer Model Evaluation\\n\")\n",
    "            f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "            f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "            f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "            f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "            f.write(\"\\nClassification Report:\\n\")\n",
    "            f.write(classification_report(test_true, test_preds))\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02aa5e8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_transformer_model(model, tokenizer, model_path):\n",
    "    \"\"\"\n",
    "    Save transformer model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model\n",
    "        tokenizer: Transformer tokenizer\n",
    "        model_path (str): Path to save the model\n",
    "    \"\"\"\n",
    "    # Save model and tokenizer\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(os.path.join(model_path, 'tokenizer'))\n",
    "    \n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    print(f\"Tokenizer saved to {os.path.join(model_path, 'tokenizer')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e9ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transformer_model(model_path):\n",
    "    \"\"\"\n",
    "    Load transformer model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer)\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(os.path.join(model_path, 'tokenizer'))\n",
    "    \n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    print(f\"Tokenizer loaded from {os.path.join(model_path, 'tokenizer')}\")\n",
    "    \n",
    "    return model, tokenizer"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
