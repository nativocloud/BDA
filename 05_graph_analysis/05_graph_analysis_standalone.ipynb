{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae5b6a8",
   "metadata": {},
   "source": [
    "# Fake News Detection: Graph Analysis\n",
    "\n",
    "This notebook contains all the necessary code for graph-based entity analysis in the fake news detection project. The code is organized into independent functions, without dependencies on external modules or classes, to facilitate execution in Databricks Community Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f2bf02",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f094ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, explode, array, lit, collect_list, count, when, udf, struct, \n",
    "    array_contains, concat_ws, split, size, expr\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType, StringType, StructType, StructField, IntegerType, \n",
    "    FloatType, BooleanType, MapType\n",
    ")\n",
    "\n",
    "# Try to import GraphFrames - this may fail in environments without GraphX support\n",
    "try:\n",
    "    from graphframes import GraphFrame\n",
    "    graphframes_available = True\n",
    "    print(\"GraphFrames is available\")\n",
    "except ImportError:\n",
    "    graphframes_available = False\n",
    "    print(\"GraphFrames is not available - will use alternative implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session optimized for Databricks Community Edition\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection_GraphAnalysis\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# If GraphFrames is available, configure it\n",
    "if graphframes_available:\n",
    "    try:\n",
    "        spark.conf.set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.0-s_2.12\")\n",
    "    except:\n",
    "        print(\"Warning: Could not configure GraphFrames package. If running in Databricks, this may be normal.\")\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2bdbf6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Start timer for performance tracking\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996aaf5",
   "metadata": {},
   "source": [
    "## Reusable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f53ad7",
   "metadata": {},
   "source": [
    "### Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5572f7c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_preprocessed_data(path=\"dbfs:/FileStore/fake_news_detection/preprocessed_data/preprocessed_news.parquet\"):\n",
    "    \"\"\"\n",
    "    Load preprocessed data from Parquet file.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the preprocessed data Parquet file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Spark DataFrame with preprocessed data\n",
    "    \"\"\"\n",
    "    print(f\"Loading preprocessed data from {path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load data from Parquet file\n",
    "        df = spark.read.parquet(path)\n",
    "        \n",
    "        # Display basic information\n",
    "        print(f\"Successfully loaded {df.count()} records.\")\n",
    "        df.printSchema()\n",
    "        \n",
    "        # Cache the DataFrame for better performance\n",
    "        df.cache()\n",
    "        print(\"Preprocessed DataFrame cached.\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading preprocessed data: {e}\")\n",
    "        print(\"Please ensure the preprocessing notebook ran successfully and saved data to the correct path.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84fbfb7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_feature_data(path=\"dbfs:/FileStore/fake_news_detection/feature_data/features.parquet\"):\n",
    "    \"\"\"\n",
    "    Load feature data from Parquet file.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the feature data Parquet file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Spark DataFrame with feature data\n",
    "    \"\"\"\n",
    "    print(f\"Loading feature data from {path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load data from Parquet file\n",
    "        df = spark.read.parquet(path)\n",
    "        \n",
    "        # Display basic information\n",
    "        print(f\"Successfully loaded {df.count()} records.\")\n",
    "        df.printSchema()\n",
    "        \n",
    "        # Cache the DataFrame for better performance\n",
    "        df.cache()\n",
    "        print(\"Feature DataFrame cached.\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading feature data: {e}\")\n",
    "        print(\"Please ensure the feature engineering notebook ran successfully and saved data to the correct path.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fc21c",
   "metadata": {},
   "source": [
    "### Named Entity Recognition Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06bfd86",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_entities_with_spacy(text):\n",
    "    \"\"\"\n",
    "    Extract named entities from text using SpaCy.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to extract entities from\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with entity lists by type\n",
    "    \"\"\"\n",
    "    # This is a simplified version - in production, use a proper SpaCy pipeline\n",
    "    # Here we use regex patterns to simulate entity extraction\n",
    "    \n",
    "    if not text or not isinstance(text, str):\n",
    "        return {\"people\": [], \"places\": [], \"organizations\": [], \"events\": []}\n",
    "    \n",
    "    # Simple regex patterns for demonstration\n",
    "    people_pattern = r'(Mr\\.|Mrs\\.|Ms\\.|Dr\\.|Prof\\.) [A-Z][a-z]+ [A-Z][a-z]+|[A-Z][a-z]+ [A-Z][a-z]+'\n",
    "    places_pattern = r'(in|at|from) ([A-Z][a-z]+(,)? [A-Z][a-z]+|[A-Z][a-z]+)'\n",
    "    org_pattern = r'([A-Z][a-z]* (Corporation|Inc\\.|Company|Association|Organization|Agency|Department))|([A-Z][A-Z]+)'\n",
    "    event_pattern = r'(conference|meeting|summit|election|war|attack|ceremony|festival|celebration)'\n",
    "    \n",
    "    # Extract entities\n",
    "    people = list(set(re.findall(people_pattern, text)))\n",
    "    places = list(set([m.group(2) for m in re.finditer(places_pattern, text)]))\n",
    "    organizations = list(set(re.findall(org_pattern, text)))\n",
    "    events = list(set(re.findall(event_pattern, text, re.IGNORECASE)))\n",
    "    \n",
    "    # Clean up entities\n",
    "    people = [p.strip() for p in people]\n",
    "    places = [p.strip() for p in places]\n",
    "    organizations = [o.strip() for o in organizations]\n",
    "    events = [e.strip() for e in events]\n",
    "    \n",
    "    return {\n",
    "        \"people\": people,\n",
    "        \"places\": places,\n",
    "        \"organizations\": organizations,\n",
    "        \"events\": events\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6333db56",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_entities_from_dataframe(df, text_column=\"text\", batch_size=1000):\n",
    "    \"\"\"\n",
    "    Extract named entities from a DataFrame using SpaCy.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Spark DataFrame with text data\n",
    "        text_column (str): Name of the column containing text\n",
    "        batch_size (int): Number of rows to process in each batch\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with extracted entities\n",
    "    \"\"\"\n",
    "    print(\"Extracting named entities from text...\")\n",
    "    \n",
    "    # Register UDF for entity extraction\n",
    "    extract_entities_udf = udf(extract_entities_with_spacy, MapType(StringType(), ArrayType(StringType())))\n",
    "    \n",
    "    # Apply UDF to extract entities\n",
    "    df_with_entities = df.withColumn(\"entities\", extract_entities_udf(col(text_column)))\n",
    "    \n",
    "    # Explode the entities map into separate columns\n",
    "    df_with_entities = df_with_entities \\\n",
    "        .withColumn(\"people\", col(\"entities\")[\"people\"]) \\\n",
    "        .withColumn(\"places\", col(\"entities\")[\"places\"]) \\\n",
    "        .withColumn(\"organizations\", col(\"entities\")[\"organizations\"]) \\\n",
    "        .withColumn(\"events\", col(\"entities\")[\"events\"]) \\\n",
    "        .drop(\"entities\")\n",
    "    \n",
    "    # Show sample of extracted entities\n",
    "    df_with_entities.select(\"id\", \"label\", \"people\", \"places\", \"organizations\", \"events\") \\\n",
    "        .show(5, truncate=50, vertical=True)\n",
    "    \n",
    "    return df_with_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf64a046",
   "metadata": {},
   "source": [
    "### Graph Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64179c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_entity_nodes(df_with_entities, min_entity_freq=2):\n",
    "    \"\"\"\n",
    "    Create entity nodes for graph analysis.\n",
    "    \n",
    "    Args:\n",
    "        df_with_entities (DataFrame): DataFrame with extracted entities\n",
    "        min_entity_freq (int): Minimum frequency for entity to be included\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with entity nodes\n",
    "    \"\"\"\n",
    "    print(\"Creating entity nodes...\")\n",
    "    \n",
    "    # Explode people entities\n",
    "    people_df = df_with_entities.select(\n",
    "        explode(col(\"people\")).alias(\"entity\"),\n",
    "        lit(\"person\").alias(\"entity_type\"),\n",
    "        col(\"label\")\n",
    "    )\n",
    "    \n",
    "    # Explode place entities\n",
    "    places_df = df_with_entities.select(\n",
    "        explode(col(\"places\")).alias(\"entity\"),\n",
    "        lit(\"place\").alias(\"entity_type\"),\n",
    "        col(\"label\")\n",
    "    )\n",
    "    \n",
    "    # Explode organization entities\n",
    "    org_df = df_with_entities.select(\n",
    "        explode(col(\"organizations\")).alias(\"entity\"),\n",
    "        lit(\"organization\").alias(\"entity_type\"),\n",
    "        col(\"label\")\n",
    "    )\n",
    "    \n",
    "    # Explode event entities\n",
    "    event_df = df_with_entities.select(\n",
    "        explode(col(\"events\")).alias(\"entity\"),\n",
    "        lit(\"event\").alias(\"entity_type\"),\n",
    "        col(\"label\")\n",
    "    )\n",
    "    \n",
    "    # Union all entity dataframes\n",
    "    all_entities_df = people_df.union(places_df).union(org_df).union(event_df)\n",
    "    \n",
    "    # Count entity occurrences and filter by minimum frequency\n",
    "    entity_counts = all_entities_df.groupBy(\"entity\", \"entity_type\") \\\n",
    "        .agg(count(\"*\").alias(\"count\")) \\\n",
    "        .filter(col(\"count\") >= min_entity_freq)\n",
    "    \n",
    "    # Count entity occurrences by label\n",
    "    entity_label_counts = all_entities_df.groupBy(\"entity\", \"entity_type\", \"label\") \\\n",
    "        .agg(count(\"*\").alias(\"label_count\"))\n",
    "    \n",
    "    # Join to get fake and real counts\n",
    "    entity_stats = entity_counts.join(\n",
    "        entity_label_counts.filter(col(\"label\") == 0).select(\n",
    "            col(\"entity\"),\n",
    "            col(\"label_count\").alias(\"fake_count\")\n",
    "        ),\n",
    "        \"entity\",\n",
    "        \"left\"\n",
    "    ).join(\n",
    "        entity_label_counts.filter(col(\"label\") == 1).select(\n",
    "            col(\"entity\"),\n",
    "            col(\"label_count\").alias(\"real_count\")\n",
    "        ),\n",
    "        \"entity\",\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # Fill null values with 0\n",
    "    entity_stats = entity_stats.fillna({\"fake_count\": 0, \"real_count\": 0})\n",
    "    \n",
    "    # Calculate fake and real ratios\n",
    "    entity_stats = entity_stats.withColumn(\n",
    "        \"fake_ratio\", \n",
    "        col(\"fake_count\") / (col(\"fake_count\") + col(\"real_count\"))\n",
    "    ).withColumn(\n",
    "        \"real_ratio\", \n",
    "        col(\"real_count\") / (col(\"fake_count\") + col(\"real_count\"))\n",
    "    )\n",
    "    \n",
    "    # Create vertices for GraphFrames\n",
    "    vertices = entity_stats.select(\n",
    "        col(\"entity\").alias(\"id\"),\n",
    "        col(\"entity_type\"),\n",
    "        col(\"count\"),\n",
    "        col(\"fake_count\"),\n",
    "        col(\"real_count\"),\n",
    "        col(\"fake_ratio\"),\n",
    "        col(\"real_ratio\")\n",
    "    )\n",
    "    \n",
    "    print(f\"Created {vertices.count()} entity nodes\")\n",
    "    \n",
    "    return vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29c10d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_entity_edges(df_with_entities, min_edge_weight=2):\n",
    "    \"\"\"\n",
    "    Create edges between co-occurring entities.\n",
    "    \n",
    "    Args:\n",
    "        df_with_entities (DataFrame): DataFrame with extracted entities\n",
    "        min_edge_weight (int): Minimum co-occurrence weight for edge to be included\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with entity edges\n",
    "    \"\"\"\n",
    "    print(\"Creating entity relationship edges...\")\n",
    "    \n",
    "    # Create a UDF to generate all entity pairs in a document\n",
    "    @udf(ArrayType(StructType([\n",
    "        StructField(\"src\", StringType()),\n",
    "        StructField(\"dst\", StringType()),\n",
    "        StructField(\"src_type\", StringType()),\n",
    "        StructField(\"dst_type\", StringType())\n",
    "    ])))\n",
    "    def generate_entity_pairs(people, places, organizations, events):\n",
    "        # Collect all entities with their types\n",
    "        all_entities = []\n",
    "        if people:\n",
    "            all_entities.extend([(entity, \"person\") for entity in people])\n",
    "        if places:\n",
    "            all_entities.extend([(entity, \"place\") for entity in places])\n",
    "        if organizations:\n",
    "            all_entities.extend([(entity, \"organization\") for entity in organizations])\n",
    "        if events:\n",
    "            all_entities.extend([(entity, \"event\") for entity in events])\n",
    "        \n",
    "        # Generate all pairs\n",
    "        pairs = []\n",
    "        for i in range(len(all_entities)):\n",
    "            for j in range(i+1, len(all_entities)):\n",
    "                # Create edge in both directions for undirected graph\n",
    "                pairs.append({\n",
    "                    \"src\": all_entities[i][0],\n",
    "                    \"dst\": all_entities[j][0],\n",
    "                    \"src_type\": all_entities[i][1],\n",
    "                    \"dst_type\": all_entities[j][1]\n",
    "                })\n",
    "                pairs.append({\n",
    "                    \"src\": all_entities[j][0],\n",
    "                    \"dst\": all_entities[i][0],\n",
    "                    \"src_type\": all_entities[j][1],\n",
    "                    \"dst_type\": all_entities[i][1]\n",
    "                })\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    # Generate all entity pairs\n",
    "    pairs_df = df_with_entities.select(\n",
    "        col(\"people\"),\n",
    "        col(\"places\"),\n",
    "        col(\"organizations\"),\n",
    "        col(\"events\"),\n",
    "        col(\"label\"),\n",
    "        generate_entity_pairs(\n",
    "            col(\"people\"),\n",
    "            col(\"places\"),\n",
    "            col(\"organizations\"),\n",
    "            col(\"events\")\n",
    "        ).alias(\"pairs\")\n",
    "    )\n",
    "    \n",
    "    # Explode the pairs\n",
    "    edges_df = pairs_df.select(\n",
    "        explode(col(\"pairs\")).alias(\"pair\"),\n",
    "        col(\"label\")\n",
    "    ).select(\n",
    "        col(\"pair.src\").alias(\"src\"),\n",
    "        col(\"pair.dst\").alias(\"dst\"),\n",
    "        col(\"pair.src_type\").alias(\"src_type\"),\n",
    "        col(\"pair.dst_type\").alias(\"dst_type\"),\n",
    "        col(\"label\")\n",
    "    )\n",
    "    \n",
    "    # Count co-occurrences\n",
    "    edge_counts = edges_df.groupBy(\"src\", \"dst\", \"src_type\", \"dst_type\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"weight\"),\n",
    "            count(when(col(\"label\") == 0, 1)).alias(\"fake_weight\"),\n",
    "            count(when(col(\"label\") == 1, 1)).alias(\"real_weight\")\n",
    "        )\n",
    "    \n",
    "    # Filter edges by minimum weight\n",
    "    filtered_edges = edge_counts.filter(col(\"weight\") >= min_edge_weight)\n",
    "    \n",
    "    # Create edges for GraphFrames\n",
    "    edges = filtered_edges.select(\n",
    "        col(\"src\"),\n",
    "        col(\"dst\"),\n",
    "        col(\"weight\"),\n",
    "        col(\"fake_weight\"),\n",
    "        col(\"real_weight\"),\n",
    "        col(\"src_type\"),\n",
    "        col(\"dst_type\")\n",
    "    )\n",
    "    \n",
    "    print(f\"Created {edges.count()} relationship edges\")\n",
    "    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d82450",
   "metadata": {},
   "source": [
    "### GraphX Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27319935",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_graphframe(vertices, edges):\n",
    "    \"\"\"\n",
    "    Create a GraphFrame from vertices and edges.\n",
    "    \n",
    "    Args:\n",
    "        vertices (DataFrame): DataFrame with entity nodes\n",
    "        edges (DataFrame): DataFrame with entity edges\n",
    "        \n",
    "    Returns:\n",
    "        GraphFrame: GraphFrame object or None if GraphFrames not available\n",
    "    \"\"\"\n",
    "    if not graphframes_available:\n",
    "        print(\"GraphFrames is not available. Cannot create GraphFrame.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Creating GraphFrame...\")\n",
    "    try:\n",
    "        g = GraphFrame(vertices, edges)\n",
    "        print(f\"GraphFrame created with {g.vertices.count()} vertices and {g.edges.count()} edges\")\n",
    "        return g\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating GraphFrame: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2b021",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_pagerank(g, reset_probability=0.15, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Run PageRank algorithm on a GraphFrame.\n",
    "    \n",
    "    Args:\n",
    "        g (GraphFrame): GraphFrame object\n",
    "        reset_probability (float): Reset probability for PageRank\n",
    "        tolerance (float): Convergence tolerance\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (vertices_with_pagerank, edges_with_pagerank) DataFrames with PageRank scores\n",
    "    \"\"\"\n",
    "    if g is None:\n",
    "        print(\"GraphFrame is None. Cannot run PageRank.\")\n",
    "        return None, None\n",
    "    \n",
    "    print(\"Running PageRank algorithm...\")\n",
    "    try:\n",
    "        results = g.pageRank(resetProbability=reset_probability, tol=tolerance)\n",
    "        pr_vertices = results.vertices.select(\n",
    "            \"id\", \"entity_type\", \"pagerank\", \"count\", \"fake_count\", \"real_count\", \"fake_ratio\", \"real_ratio\"\n",
    "        )\n",
    "        pr_edges = results.edges.select(\n",
    "            \"src\", \"dst\", \"weight\", \"fake_weight\", \"real_weight\", \"src_type\", \"dst_type\", \"weight\"\n",
    "        )\n",
    "        \n",
    "        print(\"PageRank algorithm completed successfully\")\n",
    "        return pr_vertices, pr_edges\n",
    "    except Exception as e:\n",
    "        print(f\"Error running PageRank: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b163aa8e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_connected_components(g):\n",
    "    \"\"\"\n",
    "    Run Connected Components algorithm on a GraphFrame.\n",
    "    \n",
    "    Args:\n",
    "        g (GraphFrame): GraphFrame object\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with component assignments\n",
    "    \"\"\"\n",
    "    if g is None:\n",
    "        print(\"GraphFrame is None. Cannot run Connected Components.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Running Connected Components algorithm...\")\n",
    "    try:\n",
    "        result = g.connectedComponents()\n",
    "        print(\"Connected Components algorithm completed successfully\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Connected Components: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f38eec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_triangle_count(g):\n",
    "    \"\"\"\n",
    "    Run Triangle Count algorithm on a GraphFrame.\n",
    "    \n",
    "    Args:\n",
    "        g (GraphFrame): GraphFrame object\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with triangle counts\n",
    "    \"\"\"\n",
    "    if g is None:\n",
    "        print(\"GraphFrame is None. Cannot run Triangle Count.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Running Triangle Count algorithm...\")\n",
    "    try:\n",
    "        result = g.triangleCount()\n",
    "        print(\"Triangle Count algorithm completed successfully\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error running Triangle Count: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ef0dd",
   "metadata": {},
   "source": [
    "### Non-GraphX Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303fa0e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_networkx_graph(vertices_df, edges_df):\n",
    "    \"\"\"\n",
    "    Create a NetworkX graph from vertices and edges DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        vertices_df (DataFrame): DataFrame with entity nodes\n",
    "        edges_df (DataFrame): DataFrame with entity edges\n",
    "        \n",
    "    Returns:\n",
    "        nx.Graph: NetworkX graph object\n",
    "    \"\"\"\n",
    "    print(\"Creating NetworkX graph...\")\n",
    "    \n",
    "    # Convert vertices to pandas\n",
    "    vertices_pd = vertices_df.toPandas()\n",
    "    \n",
    "    # Convert edges to pandas\n",
    "    edges_pd = edges_df.toPandas()\n",
    "    \n",
    "    # Create NetworkX graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes with attributes\n",
    "    for _, row in vertices_pd.iterrows():\n",
    "        G.add_node(\n",
    "            row['id'],\n",
    "            entity_type=row['entity_type'],\n",
    "            count=row['count'],\n",
    "            fake_count=row['fake_count'],\n",
    "            real_count=row['real_count'],\n",
    "            fake_ratio=row['fake_ratio'],\n",
    "            real_ratio=row['real_ratio']\n",
    "        )\n",
    "    \n",
    "    # Add edges with attributes\n",
    "    for _, row in edges_pd.iterrows():\n",
    "        G.add_edge(\n",
    "            row['src'],\n",
    "            row['dst'],\n",
    "            weight=row['weight'],\n",
    "            fake_weight=row['fake_weight'],\n",
    "            real_weight=row['real_weight'],\n",
    "            src_type=row['src_type'],\n",
    "            dst_type=row['dst_type']\n",
    "        )\n",
    "    \n",
    "    print(f\"NetworkX graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408bd261",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_networkx_metrics(G):\n",
    "    \"\"\"\n",
    "    Calculate various network metrics using NetworkX.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.Graph): NetworkX graph object\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with network metrics\n",
    "    \"\"\"\n",
    "    print(\"Calculating network metrics...\")\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics['num_nodes'] = G.number_of_nodes()\n",
    "    metrics['num_edges'] = G.number_of_edges()\n",
    "    metrics['density'] = nx.density(G)\n",
    "    \n",
    "    # Degree metrics\n",
    "    degrees = [d for n, d in G.degree()]\n",
    "    metrics['avg_degree'] = sum(degrees) / len(degrees) if degrees else 0\n",
    "    metrics['max_degree'] = max(degrees) if degrees else 0\n",
    "    \n",
    "    # Centrality metrics (for a sample of nodes to avoid performance issues)\n",
    "    sample_size = min(100, len(G.nodes()))\n",
    "    sample_nodes = list(G.nodes())[:sample_size]\n",
    "    sample_graph = G.subgraph(sample_nodes)\n",
    "    \n",
    "    try:\n",
    "        # Betweenness centrality\n",
    "        betweenness = nx.betweenness_centrality(sample_graph, k=min(10, sample_size-1))\n",
    "        metrics['max_betweenness'] = max(betweenness.values()) if betweenness else 0\n",
    "        metrics['avg_betweenness'] = sum(betweenness.values()) / len(betweenness) if betweenness else 0\n",
    "        \n",
    "        # Closeness centrality\n",
    "        closeness = nx.closeness_centrality(sample_graph)\n",
    "        metrics['max_closeness'] = max(closeness.values()) if closeness else 0\n",
    "        metrics['avg_closeness'] = sum(closeness.values()) / len(closeness) if closeness else 0\n",
    "        \n",
    "        # Eigenvector centrality\n",
    "        eigenvector = nx.eigenvector_centrality(sample_graph, max_iter=100)\n",
    "        metrics['max_eigenvector'] = max(eigenvector.values()) if eigenvector else 0\n",
    "        metrics['avg_eigenvector'] = sum(eigenvector.values()) / len(eigenvector) if eigenvector else 0\n",
    "    except:\n",
    "        print(\"Warning: Some centrality metrics could not be calculated\")\n",
    "    \n",
    "    # Component analysis\n",
    "    components = list(nx.connected_components(G))\n",
    "    metrics['num_components'] = len(components)\n",
    "    component_sizes = [len(c) for c in components]\n",
    "    metrics['largest_component_size'] = max(component_sizes) if component_sizes else 0\n",
    "    metrics['avg_component_size'] = sum(component_sizes) / len(component_sizes) if component_sizes else 0\n",
    "    \n",
    "    # Triangle count\n",
    "    triangles = sum(nx.triangles(G).values()) / 3  # Divide by 3 as each triangle is counted 3 times\n",
    "    metrics['triangle_count'] = triangles\n",
    "    \n",
    "    print(\"Network metrics calculated successfully\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1f65e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_pagerank_networkx(G, alpha=0.85, max_iter=100):\n",
    "    \"\"\"\n",
    "    Calculate PageRank using NetworkX.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.Graph): NetworkX graph object\n",
    "        alpha (float): Damping parameter\n",
    "        max_iter (int): Maximum number of iterations\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with PageRank scores\n",
    "    \"\"\"\n",
    "    print(\"Calculating PageRank using NetworkX...\")\n",
    "    \n",
    "    try:\n",
    "        pagerank = nx.pagerank(G, alpha=alpha, max_iter=max_iter)\n",
    "        print(\"PageRank calculation completed successfully\")\n",
    "        return pagerank\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating PageRank: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68170a66",
   "metadata": {},
   "source": [
    "### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166f743",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_entity_distribution(vertices_df, top_n=20):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of entities by type and label.\n",
    "    \n",
    "    Args:\n",
    "        vertices_df (DataFrame): DataFrame with entity nodes\n",
    "        top_n (int): Number of top entities to display\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Convert to pandas for visualization\n",
    "    vertices_pd = vertices_df.toPandas()\n",
    "    \n",
    "    # Sort by count\n",
    "    vertices_pd = vertices_pd.sort_values('count', ascending=False)\n",
    "    \n",
    "    # Get top N entities\n",
    "    top_entities = vertices_pd.head(top_n)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # Plot 1: Entity count by type\n",
    "    entity_type_counts = vertices_pd.groupby('entity_type')['count'].sum().reset_index()\n",
    "    sns.barplot(x='entity_type', y='count', data=entity_type_counts, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Entity Count by Type')\n",
    "    axes[0, 0].set_xlabel('Entity Type')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    \n",
    "    # Plot 2: Top entities by count\n",
    "    sns.barplot(x='count', y='id', data=top_entities, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(f'Top {top_n} Entities by Count')\n",
    "    axes[0, 1].set_xlabel('Count')\n",
    "    axes[0, 1].set_ylabel('Entity')\n",
    "    \n",
    "    # Plot 3: Fake vs Real ratio for top entities\n",
    "    top_entities_melted = pd.melt(\n",
    "        top_entities, \n",
    "        id_vars=['id', 'entity_type'], \n",
    "        value_vars=['fake_count', 'real_count'],\n",
    "        var_name='label', \n",
    "        value_name='count'\n",
    "    )\n",
    "    sns.barplot(x='count', y='id', hue='label', data=top_entities_melted, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(f'Fake vs Real Count for Top {top_n} Entities')\n",
    "    axes[1, 0].set_xlabel('Count')\n",
    "    axes[1, 0].set_ylabel('Entity')\n",
    "    \n",
    "    # Plot 4: Fake ratio for top entities\n",
    "    sns.barplot(x='fake_ratio', y='id', data=top_entities, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(f'Fake Ratio for Top {top_n} Entities')\n",
    "    axes[1, 1].set_xlabel('Fake Ratio')\n",
    "    axes[1, 1].set_ylabel('Entity')\n",
    "    axes[1, 1].axvline(x=0.5, color='red', linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34794b4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_network(G, top_n=100, layout='spring'):\n",
    "    \"\"\"\n",
    "    Visualize the entity network.\n",
    "    \n",
    "    Args:\n",
    "        G (nx.Graph): NetworkX graph object\n",
    "        top_n (int): Number of top nodes to display\n",
    "        layout (str): Layout algorithm to use\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get top N nodes by degree\n",
    "    degrees = dict(G.degree())\n",
    "    top_nodes = sorted(degrees, key=degrees.get, reverse=True)[:top_n]\n",
    "    \n",
    "    # Create subgraph with top nodes\n",
    "    subgraph = G.subgraph(top_nodes)\n",
    "    \n",
    "    # Choose layout\n",
    "    if layout == 'spring':\n",
    "        pos = nx.spring_layout(subgraph, seed=42)\n",
    "    elif layout == 'circular':\n",
    "        pos = nx.circular_layout(subgraph)\n",
    "    elif layout == 'kamada_kawai':\n",
    "        pos = nx.kamada_kawai_layout(subgraph)\n",
    "    else:\n",
    "        pos = nx.spring_layout(subgraph, seed=42)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # Get node attributes\n",
    "    node_types = nx.get_node_attributes(subgraph, 'entity_type')\n",
    "    node_counts = nx.get_node_attributes(subgraph, 'count')\n",
    "    node_fake_ratios = nx.get_node_attributes(subgraph, 'fake_ratio')\n",
    "    \n",
    "    # Define colors for entity types\n",
    "    type_colors = {\n",
    "        'person': 'blue',\n",
    "        'place': 'green',\n",
    "        'organization': 'red',\n",
    "        'event': 'purple'\n",
    "    }\n",
    "    \n",
    "    # Define node colors based on entity type\n",
    "    node_colors = [type_colors.get(node_types.get(n, 'unknown'), 'gray') for n in subgraph.nodes()]\n",
    "    \n",
    "    # Define node sizes based on count\n",
    "    node_sizes = [50 + 10 * node_counts.get(n, 1) for n in subgraph.nodes()]\n",
    "    \n",
    "    # Define edge widths based on weight\n",
    "    edge_widths = [0.1 + 0.1 * subgraph[u][v].get('weight', 1) for u, v in subgraph.edges()]\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(\n",
    "        subgraph, \n",
    "        pos, \n",
    "        node_color=node_colors, \n",
    "        node_size=node_sizes, \n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(\n",
    "        subgraph, \n",
    "        pos, \n",
    "        width=edge_widths, \n",
    "        alpha=0.3, \n",
    "        edge_color='gray'\n",
    "    )\n",
    "    \n",
    "    # Draw labels for top 20 nodes\n",
    "    top_20_nodes = sorted(degrees, key=degrees.get, reverse=True)[:20]\n",
    "    labels = {n: n for n in top_20_nodes if n in subgraph}\n",
    "    nx.draw_networkx_labels(\n",
    "        subgraph, \n",
    "        pos, \n",
    "        labels=labels, \n",
    "        font_size=10, \n",
    "        font_weight='bold'\n",
    "    )\n",
    "    \n",
    "    # Add legend for entity types\n",
    "    legend_elements = [\n",
    "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=entity_type)\n",
    "        for entity_type, color in type_colors.items()\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.title(f'Entity Network (Top {top_n} Nodes by Degree)')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0286f3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_pagerank_distribution(vertices_with_pagerank, top_n=20):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of PageRank scores.\n",
    "    \n",
    "    Args:\n",
    "        vertices_with_pagerank (DataFrame): DataFrame with PageRank scores\n",
    "        top_n (int): Number of top entities to display\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if vertices_with_pagerank is None:\n",
    "        print(\"No PageRank data available for visualization.\")\n",
    "        return\n",
    "    \n",
    "    # Convert to pandas for visualization\n",
    "    vertices_pd = vertices_with_pagerank.toPandas()\n",
    "    \n",
    "    # Sort by PageRank\n",
    "    vertices_pd = vertices_pd.sort_values('pagerank', ascending=False)\n",
    "    \n",
    "    # Get top N entities by PageRank\n",
    "    top_entities = vertices_pd.head(top_n)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # Plot 1: PageRank distribution\n",
    "    sns.histplot(vertices_pd['pagerank'], kde=True, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('PageRank Distribution')\n",
    "    axes[0, 0].set_xlabel('PageRank')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    \n",
    "    # Plot 2: Top entities by PageRank\n",
    "    sns.barplot(x='pagerank', y='id', data=top_entities, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(f'Top {top_n} Entities by PageRank')\n",
    "    axes[0, 1].set_xlabel('PageRank')\n",
    "    axes[0, 1].set_ylabel('Entity')\n",
    "    \n",
    "    # Plot 3: PageRank by entity type\n",
    "    sns.boxplot(x='entity_type', y='pagerank', data=vertices_pd, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('PageRank by Entity Type')\n",
    "    axes[1, 0].set_xlabel('Entity Type')\n",
    "    axes[1, 0].set_ylabel('PageRank')\n",
    "    \n",
    "    # Plot 4: PageRank vs Fake Ratio\n",
    "    sns.scatterplot(x='fake_ratio', y='pagerank', hue='entity_type', data=vertices_pd, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('PageRank vs Fake Ratio')\n",
    "    axes[1, 1].set_xlabel('Fake Ratio')\n",
    "    axes[1, 1].set_ylabel('PageRank')\n",
    "    axes[1, 1].axvline(x=0.5, color='red', linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7475df3f",
   "metadata": {},
   "source": [
    "### Data Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066bf536",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_to_parquet(df, path, partition_by=None):\n",
    "    \"\"\"\n",
    "    Save a DataFrame in Parquet format.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to save\n",
    "        path (str): Path where to save the DataFrame\n",
    "        partition_by (str): Column to partition by (optional)\n",
    "    \"\"\"\n",
    "    print(f\"Saving DataFrame to {path}...\")\n",
    "    \n",
    "    writer = df.write.mode(\"overwrite\")\n",
    "    \n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(partition_by)\n",
    "    \n",
    "    writer.parquet(path)\n",
    "    print(f\"DataFrame saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f8031",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_to_hive_table(df, table_name, partition_by=None):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a Hive table.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame to save\n",
    "        table_name (str): Name of the Hive table to create or replace\n",
    "        partition_by (str): Column to partition by (optional)\n",
    "    \"\"\"\n",
    "    print(f\"Saving DataFrame to Hive table {table_name}...\")\n",
    "    \n",
    "    writer = df.write.mode(\"overwrite\").format(\"parquet\")\n",
    "    \n",
    "    if partition_by:\n",
    "        writer = writer.partitionBy(partition_by)\n",
    "    \n",
    "    writer.saveAsTable(table_name)\n",
    "    print(f\"DataFrame saved to Hive table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037256c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_network_metrics(metrics, path):\n",
    "    \"\"\"\n",
    "    Save network metrics to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        metrics (dict): Dictionary with network metrics\n",
    "        path (str): Path where to save the metrics\n",
    "    \"\"\"\n",
    "    print(f\"Saving network metrics to {path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Convert to JSON\n",
    "        metrics_json = json.dumps(metrics, indent=2)\n",
    "        \n",
    "        # Write to file\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(metrics_json)\n",
    "        \n",
    "        print(f\"Network metrics saved to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving network metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd6dcb",
   "metadata": {},
   "source": [
    "## Complete Graph Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690bb187",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_entity_graph(\n",
    "    input_path=\"dbfs:/FileStore/fake_news_detection/preprocessed_data/preprocessed_news.parquet\",\n",
    "    output_dir=\"dbfs:/FileStore/fake_news_detection/graph_data\",\n",
    "    min_entity_freq=2,\n",
    "    min_edge_weight=2,\n",
    "    top_n=20,\n",
    "    create_tables=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete pipeline for graph-based entity analysis.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): Path to preprocessed data\n",
    "        output_dir (str): Directory to save results\n",
    "        min_entity_freq (int): Minimum frequency for entity to be included\n",
    "        min_edge_weight (int): Minimum co-occurrence weight for edge to be included\n",
    "        top_n (int): Number of top entities to display in visualizations\n",
    "        create_tables (bool): Whether to create Hive tables\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with references to analysis results\n",
    "    \"\"\"\n",
    "    print(\"Starting graph-based entity analysis pipeline...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create output directories\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(output_dir.replace(\"dbfs:\", \"\"))\n",
    "    except:\n",
    "        print(\"Warning: Could not create directories. This is expected in local environments.\")\n",
    "        os.makedirs(output_dir.replace(\"dbfs:/\", \"/tmp/\"), exist_ok=True)\n",
    "    \n",
    "    # 1. Load preprocessed data\n",
    "    df = load_preprocessed_data(input_path)\n",
    "    if df is None:\n",
    "        print(\"Error: Could not load preprocessed data. Pipeline aborted.\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Extract named entities\n",
    "    df_with_entities = extract_entities_from_dataframe(df, \"text\")\n",
    "    \n",
    "    # 3. Create entity nodes\n",
    "    vertices = create_entity_nodes(df_with_entities, min_entity_freq)\n",
    "    \n",
    "    # 4. Create entity edges\n",
    "    edges = create_entity_edges(df_with_entities, min_edge_weight)\n",
    "    \n",
    "    # 5. Visualize entity distribution\n",
    "    visualize_entity_distribution(vertices, top_n)\n",
    "    \n",
    "    # 6. Create graph and run analysis\n",
    "    results = {}\n",
    "    \n",
    "    if graphframes_available:\n",
    "        # 6.1. GraphX analysis\n",
    "        g = create_graphframe(vertices, edges)\n",
    "        \n",
    "        if g is not None:\n",
    "            # Run PageRank\n",
    "            vertices_with_pagerank, edges_with_pagerank = run_pagerank(g)\n",
    "            results['vertices_with_pagerank'] = vertices_with_pagerank\n",
    "            results['edges_with_pagerank'] = edges_with_pagerank\n",
    "            \n",
    "            # Visualize PageRank distribution\n",
    "            visualize_pagerank_distribution(vertices_with_pagerank, top_n)\n",
    "            \n",
    "            # Run Connected Components\n",
    "            components = run_connected_components(g)\n",
    "            results['components'] = components\n",
    "            \n",
    "            # Run Triangle Count\n",
    "            triangles = run_triangle_count(g)\n",
    "            results['triangles'] = triangles\n",
    "            \n",
    "            # Save results\n",
    "            if vertices_with_pagerank is not None:\n",
    "                save_to_parquet(vertices_with_pagerank, f\"{output_dir}/vertices_with_pagerank.parquet\")\n",
    "                if create_tables:\n",
    "                    save_to_hive_table(vertices_with_pagerank, \"entity_vertices_with_pagerank\")\n",
    "            \n",
    "            if edges_with_pagerank is not None:\n",
    "                save_to_parquet(edges_with_pagerank, f\"{output_dir}/edges_with_pagerank.parquet\")\n",
    "                if create_tables:\n",
    "                    save_to_hive_table(edges_with_pagerank, \"entity_edges_with_pagerank\")\n",
    "    \n",
    "    # 6.2. NetworkX analysis (as fallback or complement)\n",
    "    G = create_networkx_graph(vertices, edges)\n",
    "    results['networkx_graph'] = G\n",
    "    \n",
    "    # Calculate network metrics\n",
    "    metrics = calculate_networkx_metrics(G)\n",
    "    results['network_metrics'] = metrics\n",
    "    \n",
    "    # Calculate PageRank using NetworkX\n",
    "    pagerank = calculate_pagerank_networkx(G)\n",
    "    results['networkx_pagerank'] = pagerank\n",
    "    \n",
    "    # Visualize network\n",
    "    visualize_network(G, top_n)\n",
    "    \n",
    "    # Save network metrics\n",
    "    try:\n",
    "        save_network_metrics(metrics, f\"{output_dir.replace('dbfs:/', '/tmp/')}/network_metrics.json\")\n",
    "    except:\n",
    "        print(\"Warning: Could not save network metrics to file. This is expected in Databricks.\")\n",
    "    \n",
    "    # 7. Save basic entity data\n",
    "    save_to_parquet(vertices, f\"{output_dir}/entity_vertices.parquet\")\n",
    "    save_to_parquet(edges, f\"{output_dir}/entity_edges.parquet\")\n",
    "    \n",
    "    if create_tables:\n",
    "        save_to_hive_table(vertices, \"entity_vertices\")\n",
    "        save_to_hive_table(edges, \"entity_edges\")\n",
    "    \n",
    "    print(f\"\\nGraph-based entity analysis pipeline completed in {time.time() - start_time:.2f} seconds!\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2ac11",
   "metadata": {},
   "source": [
    "## Step-by-Step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11647ba7",
   "metadata": {},
   "source": [
    "### 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "preprocessed_df = load_preprocessed_data()\n",
    "\n",
    "# Display sample data\n",
    "if preprocessed_df:\n",
    "    print(\"Preprocessed data sample:\")\n",
    "    preprocessed_df.show(5, truncate=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb6c4f",
   "metadata": {},
   "source": [
    "### 2. Extract Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8fd402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract named entities from text\n",
    "if preprocessed_df:\n",
    "    df_with_entities = extract_entities_from_dataframe(preprocessed_df, \"text\")\n",
    "    \n",
    "    # Display sample of extracted entities\n",
    "    print(\"Sample of extracted entities:\")\n",
    "    df_with_entities.select(\"id\", \"label\", \"people\", \"places\", \"organizations\", \"events\").show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63908eb",
   "metadata": {},
   "source": [
    "### 3. Create Entity Nodes and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e276d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity nodes\n",
    "if 'df_with_entities' in locals():\n",
    "    vertices = create_entity_nodes(df_with_entities, min_entity_freq=2)\n",
    "    \n",
    "    # Display sample of entity nodes\n",
    "    print(\"Sample of entity nodes:\")\n",
    "    vertices.show(10, truncate=50)\n",
    "    \n",
    "    # Create entity edges\n",
    "    edges = create_entity_edges(df_with_entities, min_edge_weight=2)\n",
    "    \n",
    "    # Display sample of entity edges\n",
    "    print(\"Sample of entity edges:\")\n",
    "    edges.show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5116e9b",
   "metadata": {},
   "source": [
    "### 4. Visualize Entity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a32d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entity distribution\n",
    "if 'vertices' in locals():\n",
    "    visualize_entity_distribution(vertices, top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e74ce6a",
   "metadata": {},
   "source": [
    "### 5. GraphX Analysis (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GraphFrame and run GraphX analysis\n",
    "if 'vertices' in locals() and 'edges' in locals() and graphframes_available:\n",
    "    # Create GraphFrame\n",
    "    g = create_graphframe(vertices, edges)\n",
    "    \n",
    "    if g is not None:\n",
    "        # Run PageRank\n",
    "        vertices_with_pagerank, edges_with_pagerank = run_pagerank(g)\n",
    "        \n",
    "        # Display sample of vertices with PageRank\n",
    "        if vertices_with_pagerank is not None:\n",
    "            print(\"Sample of vertices with PageRank:\")\n",
    "            vertices_with_pagerank.orderBy(\"pagerank\", ascending=False).show(10, truncate=50)\n",
    "        \n",
    "        # Visualize PageRank distribution\n",
    "        visualize_pagerank_distribution(vertices_with_pagerank, top_n=20)\n",
    "        \n",
    "        # Run Connected Components\n",
    "        components = run_connected_components(g)\n",
    "        \n",
    "        # Display sample of components\n",
    "        if components is not None:\n",
    "            print(\"Sample of connected components:\")\n",
    "            components.show(10, truncate=50)\n",
    "        \n",
    "        # Run Triangle Count\n",
    "        triangles = run_triangle_count(g)\n",
    "        \n",
    "        # Display sample of triangle counts\n",
    "        if triangles is not None:\n",
    "            print(\"Sample of triangle counts:\")\n",
    "            triangles.show(10, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5b281",
   "metadata": {},
   "source": [
    "### 6. NetworkX Analysis (alternative to GraphX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d9f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NetworkX graph and run analysis\n",
    "if 'vertices' in locals() and 'edges' in locals():\n",
    "    # Create NetworkX graph\n",
    "    G = create_networkx_graph(vertices, edges)\n",
    "    \n",
    "    # Calculate network metrics\n",
    "    metrics = calculate_networkx_metrics(G)\n",
    "    \n",
    "    # Display network metrics\n",
    "    print(\"Network metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    \n",
    "    # Calculate PageRank using NetworkX\n",
    "    pagerank = calculate_pagerank_networkx(G)\n",
    "    \n",
    "    # Display top entities by PageRank\n",
    "    print(\"Top entities by PageRank (NetworkX):\")\n",
    "    top_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for entity, score in top_pagerank:\n",
    "        print(f\"{entity}: {score:.6f}\")\n",
    "    \n",
    "    # Visualize network\n",
    "    visualize_network(G, top_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbac617",
   "metadata": {},
   "source": [
    "### 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to Parquet files and Hive tables\n",
    "if 'vertices' in locals() and 'edges' in locals():\n",
    "    # Save entity nodes and edges\n",
    "    save_to_parquet(vertices, \"dbfs:/FileStore/fake_news_detection/graph_data/entity_vertices.parquet\")\n",
    "    save_to_parquet(edges, \"dbfs:/FileStore/fake_news_detection/graph_data/entity_edges.parquet\")\n",
    "    \n",
    "    # Save PageRank results if available\n",
    "    if 'vertices_with_pagerank' in locals() and vertices_with_pagerank is not None:\n",
    "        save_to_parquet(vertices_with_pagerank, \"dbfs:/FileStore/fake_news_detection/graph_data/vertices_with_pagerank.parquet\")\n",
    "    \n",
    "    if 'edges_with_pagerank' in locals() and edges_with_pagerank is not None:\n",
    "        save_to_parquet(edges_with_pagerank, \"dbfs:/FileStore/fake_news_detection/graph_data/edges_with_pagerank.parquet\")\n",
    "    \n",
    "    # Save network metrics if available\n",
    "    if 'metrics' in locals():\n",
    "        try:\n",
    "            save_network_metrics(metrics, \"/tmp/network_metrics.json\")\n",
    "            print(\"Network metrics saved to /tmp/network_metrics.json\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving network metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9017036",
   "metadata": {},
   "source": [
    "### 8. Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b08a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete graph analysis pipeline\n",
    "results = analyze_entity_graph(\n",
    "    input_path=\"dbfs:/FileStore/fake_news_detection/preprocessed_data/preprocessed_news.parquet\",\n",
    "    output_dir=\"dbfs:/FileStore/fake_news_detection/graph_data\",\n",
    "    min_entity_freq=2,\n",
    "    min_edge_weight=2,\n",
    "    top_n=20,\n",
    "    create_tables=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70bb8f4",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "1. **Graph Analysis Importance**: Graph-based analysis is crucial for fake news detection as it helps identify relationships between entities (people, places, organizations, events) that may indicate suspicious patterns.\n",
    "\n",
    "2. **GraphX vs NetworkX**: This notebook provides two implementation paths:\n",
    "   - GraphX (via GraphFrames) for distributed processing in Spark\n",
    "   - NetworkX as a fallback for environments without GraphX support\n",
    "\n",
    "3. **Entity Extraction**: We use a simplified entity extraction approach for demonstration. In production, use a proper NLP pipeline with SpaCy or similar tools.\n",
    "\n",
    "4. **Graph Algorithms**: We implement several graph algorithms:\n",
    "   - PageRank to identify influential entities\n",
    "   - Connected Components to find entity clusters\n",
    "   - Triangle Count to measure network cohesion\n",
    "\n",
    "5. **Visualization**: The notebook includes several visualization functions to help understand the entity network structure and distribution.\n",
    "\n",
    "6. **Performance Considerations**: Graph algorithms can be computationally expensive. For large datasets, consider:\n",
    "   - Increasing minimum entity frequency and edge weight thresholds\n",
    "   - Using sampling techniques\n",
    "   - Leveraging distributed processing with GraphX\n",
    "\n",
    "7. **Databricks Integration**: The code is optimized for Databricks Community Edition with appropriate configurations for memory and processing."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
