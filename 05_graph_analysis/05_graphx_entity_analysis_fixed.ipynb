{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac16372",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to implement GraphX-based entity analysis for fake news detection.\n",
    "This script creates a graph structure from extracted entities and analyzes relationships.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, array, lit, collect_list, count, when\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, IntegerType\n",
    "from graphframes import GraphFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed2434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_dir = \"/home/ubuntu/fake_news_detection/data\"\n",
    "results_dir = \"/home/ubuntu/fake_news_detection/logs\"\n",
    "config_dir = \"/home/ubuntu/fake_news_detection/config\"\n",
    "checkpoint_dir = \"/home/ubuntu/fake_news_detection/checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b68b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46799a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "config = {\n",
    "    \"min_entity_freq\": 2,  # Minimum frequency for entity to be included in graph\n",
    "    \"top_n_entities\": 20,  # Number of top entities to display in visualizations\n",
    "    \"min_edge_weight\": 2   # Minimum co-occurrence weight for edge to be included\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6796fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration\n",
    "with open(f\"{config_dir}/graphx_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3dbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Spark session...\")\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GraphX Entity Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.0-s_2.12\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set checkpoint directory for connected components algorithm\n",
    "spark.sparkContext.setCheckpointDir(checkpoint_dir)\n",
    "print(f\"Set checkpoint directory to {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32efa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "# Try to load the NER-enhanced dataset\n",
    "try:\n",
    "    # First try to read the NER-enhanced dataset\n",
    "    df = pd.read_csv(f\"{data_dir}/news_sample_ner_enhanced.csv\")\n",
    "    print(f\"Loaded NER-enhanced dataset with {len(df)} records\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        # Fall back to metadata-enhanced dataset\n",
    "        df = pd.read_csv(f\"{data_dir}/news_sample_enhanced.csv\")\n",
    "        print(f\"NER-enhanced dataset not found, loaded metadata-enhanced dataset with {len(df)} records\")\n",
    "    except FileNotFoundError:\n",
    "        # Fall back to original sample\n",
    "        df = pd.read_csv(f\"{data_dir}/news_sample.csv\")\n",
    "        print(f\"Enhanced datasets not found, loaded original sample with {len(df)} records\")\n",
    "        # Add empty entity columns\n",
    "        df['people'] = df.apply(lambda x: [], axis=1)\n",
    "        df['places'] = df.apply(lambda x: [], axis=1)\n",
    "        df['organizations'] = df.apply(lambda x: [], axis=1)\n",
    "        df['events'] = df.apply(lambda x: [], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64da7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string representations of lists to actual lists if needed\n",
    "for col_name in ['people', 'places', 'organizations', 'event_types']:\n",
    "    if col_name in df.columns:\n",
    "        if df[col_name].dtype == 'object' and isinstance(df[col_name].iloc[0], str):\n",
    "            df[col_name] = df[col_name].apply(lambda x: eval(x) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark DataFrame\n",
    "print(\"Creating Spark DataFrame...\")\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"label\", IntegerType(), False),\n",
    "    StructField(\"people\", ArrayType(StringType()), True),\n",
    "    StructField(\"places\", ArrayType(StringType()), True),\n",
    "    StructField(\"organizations\", ArrayType(StringType()), True),\n",
    "    StructField(\"event_types\", ArrayType(StringType()), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e90eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ID column if not present\n",
    "if 'id' not in df.columns:\n",
    "    df['id'] = range(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "# Ensure lists are properly handled\n",
    "spark_df = spark.createDataFrame([\n",
    "    (\n",
    "        int(row['id']), \n",
    "        int(row['label']), \n",
    "        row['people'] if 'people' in df.columns and isinstance(row['people'], list) else [],\n",
    "        row['places'] if 'places' in df.columns and isinstance(row['places'], list) else [],\n",
    "        row['organizations'] if 'organizations' in df.columns and isinstance(row['organizations'], list) else [],\n",
    "        row['event_types'] if 'event_types' in df.columns and isinstance(row['event_types'], list) else []\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a8afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity nodes\n",
    "print(\"Creating entity nodes...\")\n",
    "# Explode people entities\n",
    "people_df = spark_df.select(\n",
    "    explode(col(\"people\")).alias(\"entity\"),\n",
    "    lit(\"person\").alias(\"entity_type\"),\n",
    "    col(\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode place entities\n",
    "places_df = spark_df.select(\n",
    "    explode(col(\"places\")).alias(\"entity\"),\n",
    "    lit(\"place\").alias(\"entity_type\"),\n",
    "    col(\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode organization entities\n",
    "org_df = spark_df.select(\n",
    "    explode(col(\"organizations\")).alias(\"entity\"),\n",
    "    lit(\"organization\").alias(\"entity_type\"),\n",
    "    col(\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode event entities\n",
    "event_df = spark_df.select(\n",
    "    explode(col(\"event_types\")).alias(\"entity\"),\n",
    "    lit(\"event\").alias(\"entity_type\"),\n",
    "    col(\"label\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union all entity dataframes\n",
    "all_entities_df = people_df.union(places_df).union(org_df).union(event_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54dfb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count entity occurrences and filter by minimum frequency\n",
    "entity_counts = all_entities_df.groupBy(\"entity\", \"entity_type\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .filter(col(\"count\") >= config[\"min_entity_freq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count entity occurrences by label\n",
    "entity_label_counts = all_entities_df.groupBy(\"entity\", \"entity_type\", \"label\") \\\n",
    "    .agg(count(\"*\").alias(\"label_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to get fake and real counts\n",
    "entity_stats = entity_counts.join(\n",
    "    entity_label_counts.filter(col(\"label\") == 0).select(\n",
    "        col(\"entity\"),\n",
    "        col(\"label_count\").alias(\"fake_count\")\n",
    "    ),\n",
    "    \"entity\",\n",
    "    \"left\"\n",
    ").join(\n",
    "    entity_label_counts.filter(col(\"label\") == 1).select(\n",
    "        col(\"entity\"),\n",
    "        col(\"label_count\").alias(\"real_count\")\n",
    "    ),\n",
    "    \"entity\",\n",
    "    \"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values with 0\n",
    "entity_stats = entity_stats.fillna({\"fake_count\": 0, \"real_count\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fake and real ratios\n",
    "entity_stats = entity_stats.withColumn(\n",
    "    \"fake_ratio\", \n",
    "    col(\"fake_count\") / (col(\"fake_count\") + col(\"real_count\"))\n",
    ").withColumn(\n",
    "    \"real_ratio\", \n",
    "    col(\"real_count\") / (col(\"fake_count\") + col(\"real_count\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71555e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vertices for GraphFrames\n",
    "vertices = entity_stats.select(\n",
    "    col(\"entity\").alias(\"id\"),\n",
    "    col(\"entity_type\"),\n",
    "    col(\"count\"),\n",
    "    col(\"fake_count\"),\n",
    "    col(\"real_count\"),\n",
    "    col(\"fake_ratio\"),\n",
    "    col(\"real_ratio\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbcc6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Created {vertices.count()} entity nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17de539",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create edges between co-occurring entities\n",
    "print(\"Creating entity relationship edges...\")\n",
    "# Function to create all pairs of entities in a document\n",
    "def create_entity_pairs(row):\n",
    "    all_entities = []\n",
    "    if 'people' in row and row['people']:\n",
    "        all_entities.extend([(entity, 'person') for entity in row['people']])\n",
    "    if 'places' in row and row['places']:\n",
    "        all_entities.extend([(entity, 'place') for entity in row['places']])\n",
    "    if 'organizations' in row and row['organizations']:\n",
    "        all_entities.extend([(entity, 'organization') for entity in row['organizations']])\n",
    "    if 'event_types' in row and row['event_types']:\n",
    "        all_entities.extend([(entity, 'event') for entity in row['event_types']])\n",
    "    \n",
    "    pairs = []\n",
    "    for i in range(len(all_entities)):\n",
    "        for j in range(i+1, len(all_entities)):\n",
    "            # Create edge in both directions for undirected graph\n",
    "            pairs.append((all_entities[i][0], all_entities[j][0], all_entities[i][1], all_entities[j][1], row['label']))\n",
    "            pairs.append((all_entities[j][0], all_entities[i][0], all_entities[j][1], all_entities[i][1], row['label']))\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b080f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all entity pairs\n",
    "all_pairs = []\n",
    "for _, row in df.iterrows():\n",
    "    all_pairs.extend(create_entity_pairs(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for edges\n",
    "edge_schema = StructType([\n",
    "    StructField(\"src\", StringType(), False),\n",
    "    StructField(\"dst\", StringType(), False),\n",
    "    StructField(\"src_type\", StringType(), False),\n",
    "    StructField(\"dst_type\", StringType(), False),\n",
    "    StructField(\"label\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = spark.createDataFrame(all_pairs, schema=edge_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6dbd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count co-occurrences\n",
    "edge_counts = edges_df.groupBy(\"src\", \"dst\", \"src_type\", \"dst_type\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"weight\"),\n",
    "        count(when(col(\"label\") == 0, 1)).alias(\"fake_weight\"),\n",
    "        count(when(col(\"label\") == 1, 1)).alias(\"real_weight\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6ce4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter edges by minimum weight\n",
    "filtered_edges = edge_counts.filter(col(\"weight\") >= config[\"min_edge_weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edges for GraphFrames\n",
    "edges = filtered_edges.select(\n",
    "    col(\"src\"),\n",
    "    col(\"dst\"),\n",
    "    col(\"weight\"),\n",
    "    col(\"fake_weight\"),\n",
    "    col(\"real_weight\"),\n",
    "    col(\"src_type\"),\n",
    "    col(\"dst_type\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a6f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Created {edges.count()} relationship edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed49790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GraphFrame\n",
    "print(\"Creating GraphFrame...\")\n",
    "g = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503010fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PageRank algorithm\n",
    "print(\"Running PageRank algorithm...\")\n",
    "results = g.pageRank(resetProbability=0.15, tol=0.01)\n",
    "pr_vertices = results.vertices.select(\"id\", \"entity_type\", \"pagerank\", \"count\", \"fake_count\", \"real_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c81954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run connected components algorithm\n",
    "print(\"Finding connected components...\")\n",
    "try:\n",
    "    connected_components = g.connectedComponents()\n",
    "    cc_vertices = connected_components.select(\"id\", \"component\")\n",
    "\n",
    "    # Join PageRank and connected components results\n",
    "    enriched_vertices = pr_vertices.join(cc_vertices, \"id\")\n",
    "\n",
    "    # Get top entities by PageRank\n",
    "    top_entities = enriched_vertices.orderBy(col(\"pagerank\").desc()).limit(config[\"top_n_entities\"])\n",
    "    top_entities_pd = top_entities.toPandas()\n",
    "\n",
    "    print(\"Top entities by PageRank:\")\n",
    "    for i, row in top_entities_pd.iterrows():\n",
    "        print(f\"{i+1}. {row['id']} ({row['entity_type']}): {row['pagerank']:.4f}\")\n",
    "\n",
    "    # Get top connected components\n",
    "    top_components = connected_components.groupBy(\"component\") \\\n",
    "        .agg(count(\"*\").alias(\"size\")) \\\n",
    "        .orderBy(col(\"size\").desc()) \\\n",
    "        .limit(10)\n",
    "\n",
    "    top_components_pd = top_components.toPandas()\n",
    "    print(\"\\nTop connected components:\")\n",
    "    for i, row in top_components_pd.iterrows():\n",
    "        print(f\"{i+1}. Component {row['component']}: {row['size']} entities\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running connected components: {e}\")\n",
    "    print(\"Continuing without connected components analysis...\")\n",
    "    # Create a simplified version without connected components\n",
    "    enriched_vertices = pr_vertices\n",
    "    top_entities = enriched_vertices.orderBy(col(\"pagerank\").desc()).limit(config[\"top_n_entities\"])\n",
    "    top_entities_pd = top_entities.toPandas()\n",
    "    \n",
    "    print(\"Top entities by PageRank:\")\n",
    "    for i, row in top_entities_pd.iterrows():\n",
    "        print(f\"{i+1}. {row['id']} ({row['entity_type']}): {row['pagerank']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "print(\"Creating visualizations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c755fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for visualization\n",
    "vertices_pd = vertices.toPandas()\n",
    "edges_pd = edges.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23815683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity type distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "entity_type_counts = vertices_pd.groupby('entity_type').size()\n",
    "entity_type_counts.plot(kind='bar')\n",
    "plt.title('Entity Type Distribution')\n",
    "plt.xlabel('Entity Type')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dir}/graphx_entity_type_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492cfa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity fake vs real distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Get top entities by count\n",
    "top_by_count = vertices_pd.nlargest(10, 'count')\n",
    "# Create stacked bar chart\n",
    "top_by_count.plot(kind='bar', x='id', y=['fake_count', 'real_count'], stacked=True)\n",
    "plt.title('Top 10 Entities - Fake vs Real Distribution')\n",
    "plt.xlabel('Entity')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dir}/graphx_top_entities_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2cd8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PageRank distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(top_entities_pd['count'], top_entities_pd['pagerank'], \n",
    "           c=top_entities_pd['entity_type'].astype('category').cat.codes, \n",
    "           alpha=0.7, s=100)\n",
    "for i, row in top_entities_pd.iterrows():\n",
    "    plt.annotate(row['id'], (row['count'], row['pagerank']), \n",
    "                fontsize=9, ha='center', va='bottom')\n",
    "plt.title('Entity PageRank vs Frequency')\n",
    "plt.xlabel('Entity Frequency')\n",
    "plt.ylabel('PageRank Score')\n",
    "plt.colorbar(label='Entity Type')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dir}/graphx_pagerank_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317de24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network visualization\n",
    "print(\"Creating network visualization...\")\n",
    "try:\n",
    "    import networkx as nx\n",
    "    \n",
    "    # Create graph from top entities\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Get top entities by PageRank\n",
    "    top_entities_list = [row['id'] for _, row in top_entities_pd.iterrows()]\n",
    "    \n",
    "    # Add nodes with attributes\n",
    "    for _, row in vertices_pd[vertices_pd['id'].isin(top_entities_list)].iterrows():\n",
    "        G.add_node(row['id'], \n",
    "                  entity_type=row['entity_type'], \n",
    "                  count=row['count'],\n",
    "                  fake_ratio=row['fake_ratio'] if 'fake_ratio' in row else 0)\n",
    "    \n",
    "    # Add edges between top entities\n",
    "    for _, row in edges_pd[(edges_pd['src'].isin(top_entities_list)) & (edges_pd['dst'].isin(top_entities_list))].iterrows():\n",
    "        G.add_edge(row['src'], row['dst'], weight=row['weight'])\n",
    "    \n",
    "    # Create layout\n",
    "    pos = nx.spring_layout(G, k=0.3, iterations=50)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    # Node colors by entity type\n",
    "    entity_types = nx.get_node_attributes(G, 'entity_type')\n",
    "    type_colors = {'person': 'red', 'place': 'blue', 'organization': 'green', 'event': 'purple'}\n",
    "    node_colors = [type_colors.get(entity_types.get(node, 'other'), 'gray') for node in G.nodes()]\n",
    "    \n",
    "    # Node sizes by count\n",
    "    counts = nx.get_node_attributes(G, 'count')\n",
    "    node_sizes = [50 + 10 * counts.get(node, 1) for node in G.nodes()]\n",
    "    \n",
    "    # Edge weights\n",
    "    edge_weights = [G[u][v]['weight'] * 0.5 for u, v in G.edges()]\n",
    "    \n",
    "    # Draw the graph\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, alpha=0.8)\n",
    "    nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.3)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, \n",
    "                                 label=etype, markersize=10) \n",
    "                      for etype, color in type_colors.items()]\n",
    "    plt.legend(handles=legend_elements, title='Entity Type')\n",
    "    \n",
    "    plt.title(f'Entity Relationship Network (Top {len(top_entities_list)} Entities)')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{results_dir}/graphx_network_visualization.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "except ImportError:\n",
    "    print(\"NetworkX not available, skipping network visualization\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating network visualization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e87ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features for machine learning\n",
    "print(\"Creating GraphX-based features for machine learning...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24b88d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Get entity PageRank scores\n",
    "pagerank_scores = results.vertices.select(\"id\", \"pagerank\").toPandas()\n",
    "pagerank_dict = dict(zip(pagerank_scores['id'], pagerank_scores['pagerank']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392e9af",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create entity-based features\n",
    "def create_graphx_features(row):\n",
    "    features = {}\n",
    "    \n",
    "    # Entity count features\n",
    "    features['person_count'] = len(row['people']) if 'people' in row and row['people'] else 0\n",
    "    features['place_count'] = len(row['places']) if 'places' in row and row['places'] else 0\n",
    "    features['org_count'] = len(row['organizations']) if 'organizations' in row and row['organizations'] else 0\n",
    "    features['event_count'] = len(row['event_types']) if 'event_types' in row and row['event_types'] else 0\n",
    "    \n",
    "    # Average PageRank features\n",
    "    if 'people' in row and row['people']:\n",
    "        pr_scores = [pagerank_dict.get(entity, 0) for entity in row['people']]\n",
    "        features['avg_person_pagerank'] = sum(pr_scores) / len(pr_scores) if pr_scores else 0\n",
    "        features['max_person_pagerank'] = max(pr_scores) if pr_scores else 0\n",
    "    else:\n",
    "        features['avg_person_pagerank'] = 0\n",
    "        features['max_person_pagerank'] = 0\n",
    "        \n",
    "    if 'places' in row and row['places']:\n",
    "        pr_scores = [pagerank_dict.get(entity, 0) for entity in row['places']]\n",
    "        features['avg_place_pagerank'] = sum(pr_scores) / len(pr_scores) if pr_scores else 0\n",
    "        features['max_place_pagerank'] = max(pr_scores) if pr_scores else 0\n",
    "    else:\n",
    "        features['avg_place_pagerank'] = 0\n",
    "        features['max_place_pagerank'] = 0\n",
    "        \n",
    "    if 'organizations' in row and row['organizations']:\n",
    "        pr_scores = [pagerank_dict.get(entity, 0) for entity in row['organizations']]\n",
    "        features['avg_org_pagerank'] = sum(pr_scores) / len(pr_scores) if pr_scores else 0\n",
    "        features['max_org_pagerank'] = max(pr_scores) if pr_scores else 0\n",
    "    else:\n",
    "        features['avg_org_pagerank'] = 0\n",
    "        features['max_org_pagerank'] = 0\n",
    "        \n",
    "    if 'event_types' in row and row['event_types']:\n",
    "        pr_scores = [pagerank_dict.get(entity, 0) for entity in row['event_types']]\n",
    "        features['avg_event_pagerank'] = sum(pr_scores) / len(pr_scores) if pr_scores else 0\n",
    "        features['max_event_pagerank'] = max(pr_scores) if pr_scores else 0\n",
    "    else:\n",
    "        features['avg_event_pagerank'] = 0\n",
    "        features['max_event_pagerank'] = 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature creation\n",
    "graphx_features = []\n",
    "for _, row in df.iterrows():\n",
    "    features = create_graphx_features(row)\n",
    "    graphx_features.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f413af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "graphx_features_df = pd.DataFrame(graphx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to original DataFrame\n",
    "for col in graphx_features_df.columns:\n",
    "    df[f'graphx_{col}'] = graphx_features_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae37dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced dataset with GraphX features\n",
    "df.to_csv(f\"{data_dir}/news_sample_graphx_enhanced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38227f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GraphX analysis results\n",
    "graphx_results = {\n",
    "    \"vertices\": {\n",
    "        \"count\": int(vertices.count()),\n",
    "        \"by_type\": vertices.groupBy(\"entity_type\").count().toPandas().set_index(\"entity_type\")[\"count\"].to_dict()\n",
    "    },\n",
    "    \"edges\": {\n",
    "        \"count\": int(edges.count()),\n",
    "        \"avg_weight\": float(edges.agg({\"weight\": \"avg\"}).collect()[0][0])\n",
    "    },\n",
    "    \"top_entities\": top_entities_pd.to_dict(orient=\"records\"),\n",
    "    \"execution_time\": time.time() - start_time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d25417",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{results_dir}/graphx_analysis.json\", \"w\") as f:\n",
    "    json.dump(graphx_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee707130",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nGraphX entity analysis completed in {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"Enhanced dataset saved to {data_dir}/news_sample_graphx_enhanced.csv\")\n",
    "print(f\"Results saved to {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc4900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
