{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to implement NER-based entity extraction for enhanced metadata analysis.\n",
    "Extracts people, places, organizations, events, dates, sources, and dateline locations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aabeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db204f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_dir = \"/home/ubuntu/fake_news_detection/data\"\n",
    "results_dir = \"/home/ubuntu/fake_news_detection/logs\"\n",
    "config_dir = \"/home/ubuntu/fake_news_detection/config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb73d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(config_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d974b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "config = {\n",
    "    \"min_entity_freq\": 2,  # Minimum frequency for entity to be included in analysis\n",
    "    \"top_n_entities\": 20,  # Number of top entities to display in visualizations\n",
    "    \"event_keywords\": [\n",
    "        \"hurricane\", \"storm\", \"earthquake\", \"flood\", \"war\", \"attack\", \"bombing\",\n",
    "        \"election\", \"vote\", \"campaign\", \"protest\", \"demonstration\", \"riot\",\n",
    "        \"pandemic\", \"outbreak\", \"crisis\", \"scandal\", \"investigation\", \"summit\",\n",
    "        \"conference\", \"meeting\", \"agreement\", \"deal\", \"treaty\", \"legislation\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c54ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration\n",
    "with open(f\"{config_dir}/ner_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac63e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "# Load the enhanced data from previous step\n",
    "try:\n",
    "    df = pd.read_csv(f\"{data_dir}/news_sample_enhanced.csv\")\n",
    "    print(f\"Loaded enhanced dataset with {len(df)} records\")\n",
    "except FileNotFoundError:\n",
    "    # Fall back to original sample if enhanced not available\n",
    "    df = pd.read_csv(f\"{data_dir}/news_sample.csv\")\n",
    "    print(f\"Enhanced dataset not found, loaded original sample with {len(df)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "print(\"Setting up NLP tools...\")\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "except:\n",
    "    print(\"NLTK download failed, but continuing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5739c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Loaded spaCy model\")\n",
    "except:\n",
    "    print(\"Installing spaCy model...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        print(\"Loaded spaCy model after installation\")\n",
    "    except:\n",
    "        print(\"Failed to load spaCy model, using pattern-based extraction only\")\n",
    "        nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c469b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to extract dateline\n",
    "def extract_dateline(text):\n",
    "    \"\"\"Extract dateline (location at beginning of article) if present.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    \n",
    "    # Common dateline patterns\n",
    "    patterns = [\n",
    "        r'^([A-Z]+[A-Z\\s]+),\\s',  # WASHINGTON, ...\n",
    "        r'^([A-Z]+[A-Z\\s]+)\\s\\(',  # WASHINGTON (Reuters) ...\n",
    "        r'^([A-Z]+[A-Z\\s]+)\\s-\\s',  # WASHINGTON - ...\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d411ec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to extract source location\n",
    "def extract_source_location(text):\n",
    "    \"\"\"Extract location where the source claims to be reporting from.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    \n",
    "    # Look for patterns like \"reporting from [LOCATION]\"\n",
    "    patterns = [\n",
    "        r'reporting from ([A-Za-z\\s]+)',\n",
    "        r'reports from ([A-Za-z\\s]+)',\n",
    "        r'in ([A-Za-z\\s]+) for',\n",
    "        r'from ([A-Za-z\\s]+) reports',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72077d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to detect events\n",
    "def detect_events(text, event_keywords):\n",
    "    \"\"\"Detect events mentioned in the text based on keywords.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    events = []\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for keyword in event_keywords:\n",
    "        if keyword in text_lower:\n",
    "            # Get context around the keyword\n",
    "            pattern = r'[^.!?]*\\b' + keyword + r'\\b[^.!?]*[.!?]'\n",
    "            matches = re.findall(pattern, text_lower)\n",
    "            for match in matches:\n",
    "                events.append((keyword, match.strip()))\n",
    "    \n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78b54e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to perform NER using spaCy\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extract named entities using spaCy.\"\"\"\n",
    "    if pd.isna(text) or nlp is None:\n",
    "        return [], [], [], []\n",
    "    \n",
    "    people = []\n",
    "    places = []\n",
    "    organizations = []\n",
    "    dates = []\n",
    "    \n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text[:1000000])  # Limit text length to avoid memory issues\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            people.append(ent.text)\n",
    "        elif ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "            places.append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            organizations.append(ent.text)\n",
    "        elif ent.label_ in [\"DATE\", \"TIME\"]:\n",
    "            dates.append(ent.text)\n",
    "    \n",
    "    return people, places, organizations, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8b6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting entities...\")\n",
    "# Apply extraction to the dataset\n",
    "df['dateline'] = df['text'].apply(extract_dateline)\n",
    "df['source_location'] = df['text'].apply(extract_source_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1222867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract events\n",
    "df['events'] = df['text'].apply(lambda x: detect_events(x, config[\"event_keywords\"]))\n",
    "df['event_count'] = df['events'].apply(len)\n",
    "df['event_types'] = df['events'].apply(lambda events: [event[0] for event in events])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract named entities\n",
    "if nlp is not None:\n",
    "    print(\"Performing NER extraction...\")\n",
    "    # Process in batches to avoid memory issues\n",
    "    batch_size = 50\n",
    "    all_people = []\n",
    "    all_places = []\n",
    "    all_orgs = []\n",
    "    all_dates = []\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df['text'].iloc[i:i+batch_size]\n",
    "        batch_results = [extract_entities(text) for text in batch]\n",
    "        \n",
    "        batch_people = [res[0] for res in batch_results]\n",
    "        batch_places = [res[1] for res in batch_results]\n",
    "        batch_orgs = [res[2] for res in batch_results]\n",
    "        batch_dates = [res[3] for res in batch_results]\n",
    "        \n",
    "        all_people.extend(batch_people)\n",
    "        all_places.extend(batch_places)\n",
    "        all_orgs.extend(batch_orgs)\n",
    "        all_dates.extend(batch_dates)\n",
    "        \n",
    "        print(f\"Processed batch {i//batch_size + 1}/{(len(df)-1)//batch_size + 1}\")\n",
    "    \n",
    "    df['people'] = all_people\n",
    "    df['places'] = all_places\n",
    "    df['organizations'] = all_orgs\n",
    "    df['dates'] = all_dates\n",
    "    \n",
    "    # Count entities per document\n",
    "    df['people_count'] = df['people'].apply(len)\n",
    "    df['places_count'] = df['places'].apply(len)\n",
    "    df['org_count'] = df['organizations'].apply(len)\n",
    "    df['date_count'] = df['dates'].apply(len)\n",
    "else:\n",
    "    print(\"Skipping NER extraction due to missing spaCy model\")\n",
    "    df['people'] = [[] for _ in range(len(df))]\n",
    "    df['places'] = [[] for _ in range(len(df))]\n",
    "    df['organizations'] = [[] for _ in range(len(df))]\n",
    "    df['dates'] = [[] for _ in range(len(df))]\n",
    "    df['people_count'] = 0\n",
    "    df['places_count'] = 0\n",
    "    df['org_count'] = 0\n",
    "    df['date_count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b49cc9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Analyze entity distributions\n",
    "print(\"Analyzing entity distributions...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6472de",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to count and analyze entities\n",
    "def analyze_entities(entity_lists, min_freq=2):\n",
    "    \"\"\"Count and analyze entities across documents.\"\"\"\n",
    "    all_entities = []\n",
    "    for entities in entity_lists:\n",
    "        all_entities.extend(entities)\n",
    "    \n",
    "    # Count entities\n",
    "    entity_counts = Counter(all_entities)\n",
    "    \n",
    "    # Filter by minimum frequency\n",
    "    filtered_counts = {k: v for k, v in entity_counts.items() if v >= min_freq}\n",
    "    \n",
    "    return filtered_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36511c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze entities by type\n",
    "people_counts = analyze_entities(df['people'], config[\"min_entity_freq\"])\n",
    "place_counts = analyze_entities(df['places'], config[\"min_entity_freq\"])\n",
    "org_counts = analyze_entities(df['organizations'], config[\"min_entity_freq\"])\n",
    "event_counts = Counter([event[0] for events in df['event_types'] for event in events])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae2039",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"Found {len(people_counts)} unique people mentioned at least {config['min_entity_freq']} times\")\n",
    "print(f\"Found {len(place_counts)} unique places mentioned at least {config['min_entity_freq']} times\")\n",
    "print(f\"Found {len(org_counts)} unique organizations mentioned at least {config['min_entity_freq']} times\")\n",
    "print(f\"Found {len(event_counts)} unique event types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e83f17",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Analyze entity distribution by label (fake vs real)\n",
    "def analyze_entity_by_label(df, entity_col):\n",
    "    \"\"\"Analyze entity distribution by label.\"\"\"\n",
    "    fake_entities = []\n",
    "    real_entities = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if row['label'] == 0:  # Fake\n",
    "            fake_entities.extend(row[entity_col])\n",
    "        else:  # Real\n",
    "            real_entities.extend(row[entity_col])\n",
    "    \n",
    "    fake_counts = Counter(fake_entities)\n",
    "    real_counts = Counter(real_entities)\n",
    "    \n",
    "    return fake_counts, real_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze entities by label\n",
    "people_fake, people_real = analyze_entity_by_label(df, 'people')\n",
    "places_fake, places_real = analyze_entity_by_label(df, 'places')\n",
    "orgs_fake, orgs_real = analyze_entity_by_label(df, 'organizations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd3ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze events by label\n",
    "events_fake = []\n",
    "events_real = []\n",
    "for i, row in df.iterrows():\n",
    "    if row['label'] == 0:  # Fake\n",
    "        events_fake.extend(row['event_types'])\n",
    "    else:  # Real\n",
    "        events_real.extend(row['event_types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_fake_counts = Counter(events_fake)\n",
    "events_real_counts = Counter(events_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze datelines and source locations\n",
    "dateline_counts = df['dateline'].value_counts()\n",
    "source_location_counts = df['source_location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3fa7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(dateline_counts)} unique datelines\")\n",
    "print(f\"Found {len(source_location_counts)} unique source locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea1368b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "print(\"Creating visualizations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be89fac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to create entity comparison chart\n",
    "def create_entity_comparison(fake_counts, real_counts, entity_type, top_n=10):\n",
    "    \"\"\"Create comparison chart for entity distribution in fake vs real news.\"\"\"\n",
    "    # Get top entities by total count\n",
    "    combined = Counter()\n",
    "    for k, v in fake_counts.items():\n",
    "        combined[k] += v\n",
    "    for k, v in real_counts.items():\n",
    "        combined[k] += v\n",
    "    \n",
    "    top_entities = [entity for entity, count in combined.most_common(top_n)]\n",
    "    \n",
    "    # Create dataframe for plotting\n",
    "    plot_data = []\n",
    "    for entity in top_entities:\n",
    "        fake_count = fake_counts.get(entity, 0)\n",
    "        real_count = real_counts.get(entity, 0)\n",
    "        total = fake_count + real_count\n",
    "        if total > 0:\n",
    "            fake_ratio = fake_count / total\n",
    "            real_ratio = real_count / total\n",
    "            plot_data.append({\n",
    "                'Entity': entity,\n",
    "                'Fake': fake_count,\n",
    "                'Real': real_count,\n",
    "                'Fake_Ratio': fake_ratio,\n",
    "                'Real_Ratio': real_ratio\n",
    "            })\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_df[['Fake', 'Real']].plot(kind='bar', stacked=True, figsize=(12, 8))\n",
    "    plt.title(f'Distribution of {entity_type} in Fake and Real News')\n",
    "    plt.xlabel(entity_type)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(len(plot_df)), plot_df['Entity'], rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{results_dir}/{entity_type.lower()}_distribution.png\")\n",
    "    \n",
    "    return plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce529244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity comparison charts\n",
    "if people_counts:\n",
    "    people_comparison = create_entity_comparison(people_fake, people_real, 'People', config[\"top_n_entities\"])\n",
    "if place_counts:\n",
    "    places_comparison = create_entity_comparison(places_fake, places_real, 'Places', config[\"top_n_entities\"])\n",
    "if org_counts:\n",
    "    orgs_comparison = create_entity_comparison(orgs_fake, orgs_real, 'Organizations', config[\"top_n_entities\"])\n",
    "events_comparison = create_entity_comparison(events_fake_counts, events_real_counts, 'Events', config[\"top_n_entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0daac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity count comparison by article type\n",
    "plt.figure(figsize=(10, 6))\n",
    "entity_counts = df.groupby('label').agg({\n",
    "    'people_count': 'mean',\n",
    "    'places_count': 'mean',\n",
    "    'org_count': 'mean',\n",
    "    'event_count': 'mean'\n",
    "}).reset_index()\n",
    "entity_counts['label'] = entity_counts['label'].map({0: 'Fake', 1: 'Real'})\n",
    "entity_counts = entity_counts.melt(id_vars=['label'], var_name='Entity Type', value_name='Average Count')\n",
    "entity_counts['Entity Type'] = entity_counts['Entity Type'].str.replace('_count', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86727734",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Entity Type', y='Average Count', hue='label', data=entity_counts)\n",
    "plt.title('Average Entity Counts in Fake vs Real News')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dir}/avg_entity_counts.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826e3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dashboard visualization\n",
    "plt.figure(figsize=(15, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37890de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity count comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x='Entity Type', y='Average Count', hue='label', data=entity_counts)\n",
    "plt.title('Average Entity Counts')\n",
    "plt.legend(title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e8570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top people comparison\n",
    "if people_counts:\n",
    "    plt.subplot(2, 2, 2)\n",
    "    top_5_people = people_comparison.sort_values('Fake', ascending=False).head(5)\n",
    "    top_5_people[['Fake', 'Real']].plot(kind='bar', stacked=True)\n",
    "    plt.title('Top 5 People Mentioned')\n",
    "    plt.xticks(range(len(top_5_people)), top_5_people['Entity'], rotation=45, ha='right')\n",
    "    plt.legend(title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a5a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top places comparison\n",
    "if place_counts:\n",
    "    plt.subplot(2, 2, 3)\n",
    "    top_5_places = places_comparison.sort_values('Fake', ascending=False).head(5)\n",
    "    top_5_places[['Fake', 'Real']].plot(kind='bar', stacked=True)\n",
    "    plt.title('Top 5 Places Mentioned')\n",
    "    plt.xticks(range(len(top_5_places)), top_5_places['Entity'], rotation=45, ha='right')\n",
    "    plt.legend(title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f71c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top events comparison\n",
    "plt.subplot(2, 2, 4)\n",
    "top_5_events = events_comparison.sort_values('Fake', ascending=False).head(5)\n",
    "top_5_events[['Fake', 'Real']].plot(kind='bar', stacked=True)\n",
    "plt.title('Top 5 Events Mentioned')\n",
    "plt.xticks(range(len(top_5_events)), top_5_events['Entity'], rotation=45, ha='right')\n",
    "plt.legend(title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca1006",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dir}/entity_dashboard.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a56a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced dataset with NER\n",
    "df.to_csv(f\"{data_dir}/news_sample_ner_enhanced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408529bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entity analysis results\n",
    "entity_results = {\n",
    "    \"people\": {\n",
    "        \"unique_count\": len(people_counts),\n",
    "        \"top_people\": {k: v for k, v in sorted(people_counts.items(), key=lambda x: x[1], reverse=True)[:config[\"top_n_entities\"]]}\n",
    "    },\n",
    "    \"places\": {\n",
    "        \"unique_count\": len(place_counts),\n",
    "        \"top_places\": {k: v for k, v in sorted(place_counts.items(), key=lambda x: x[1], reverse=True)[:config[\"top_n_entities\"]]}\n",
    "    },\n",
    "    \"organizations\": {\n",
    "        \"unique_count\": len(org_counts),\n",
    "        \"top_organizations\": {k: v for k, v in sorted(org_counts.items(), key=lambda x: x[1], reverse=True)[:config[\"top_n_entities\"]]}\n",
    "    },\n",
    "    \"events\": {\n",
    "        \"unique_count\": len(event_counts),\n",
    "        \"top_events\": {k: v for k, v in sorted(event_counts.items(), key=lambda x: x[1], reverse=True)[:config[\"top_n_entities\"]]}\n",
    "    },\n",
    "    \"datelines\": {\n",
    "        \"unique_count\": len(dateline_counts),\n",
    "        \"top_datelines\": dateline_counts.head(config[\"top_n_entities\"]).to_dict()\n",
    "    },\n",
    "    \"source_locations\": {\n",
    "        \"unique_count\": len(source_location_counts),\n",
    "        \"top_source_locations\": source_location_counts.head(config[\"top_n_entities\"]).to_dict()\n",
    "    },\n",
    "    \"execution_time\": time.time() - start_time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{results_dir}/entity_analysis.json\", \"w\") as f:\n",
    "    json.dump(entity_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nNER entity extraction and analysis completed in {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"Enhanced dataset saved to {data_dir}/news_sample_ner_enhanced.csv\")\n",
    "print(f\"Results saved to {results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
