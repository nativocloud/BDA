{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f4a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to implement non-GraphX NER-based entity extraction and feature engineering for fake news detection.\n",
    "This script uses traditional NLP techniques to extract and analyze entities and create features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1456735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479369cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "data_dir = \"/home/ubuntu/fake_news_detection/data\"\n",
    "results_dir = \"/home/ubuntu/fake_news_detection/logs\"\n",
    "config_dir = \"/home/ubuntu/fake_news_detection/config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(config_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "config = {\n",
    "    \"min_entity_freq\": 2,  # Minimum frequency for entity to be included in analysis\n",
    "    \"top_n_entities\": 20,  # Number of top entities to display in visualizations\n",
    "    \"n_topics\": 5,         # Number of topics to extract\n",
    "    \"n_features\": 100      # Number of features to use for topic modeling\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc531f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration\n",
    "with open(f\"{config_dir}/non_graphx_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc433e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "# Try to load the NER-enhanced dataset\n",
    "try:\n",
    "    # First try to read the NER-enhanced dataset\n",
    "    df = pd.read_csv(f\"{data_dir}/news_sample_ner_enhanced.csv\")\n",
    "    print(f\"Loaded NER-enhanced dataset with {len(df)} records\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        # Fall back to metadata-enhanced dataset\n",
    "        df = pd.read_csv(f\"{data_dir}/news_sample_enhanced.csv\")\n",
    "        print(f\"NER-enhanced dataset not found, loaded metadata-enhanced dataset with {len(df)} records\")\n",
    "    except FileNotFoundError:\n",
    "        # Fall back to original sample\n",
    "        df = pd.read_csv(f\"{data_dir}/news_sample.csv\")\n",
    "        print(f\"Enhanced datasets not found, loaded original sample with {len(df)} records\")\n",
    "        # Add empty entity columns\n",
    "        df['people'] = df.apply(lambda x: [], axis=1)\n",
    "        df['places'] = df.apply(lambda x: [], axis=1)\n",
    "        df['organizations'] = df.apply(lambda x: [], axis=1)\n",
    "        df['events'] = df.apply(lambda x: [], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string representations of lists to actual lists if needed\n",
    "for col_name in ['people', 'places', 'organizations', 'event_types']:\n",
    "    if col_name in df.columns:\n",
    "        if df[col_name].dtype == 'object' and isinstance(df[col_name].iloc[0], str):\n",
    "            try:\n",
    "                df[col_name] = df[col_name].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "            except:\n",
    "                print(f\"Warning: Could not convert {col_name} to list, using empty lists\")\n",
    "                df[col_name] = df[col_name].apply(lambda x: [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a05496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure text column is available\n",
    "if 'text' not in df.columns and 'content' in df.columns:\n",
    "    df['text'] = df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac9b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "print(\"Setting up NLP tools...\")\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "except:\n",
    "    print(\"NLTK download failed, but continuing...\")\n",
    "    stop_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6aae7d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Loaded spaCy model\")\n",
    "except:\n",
    "    print(\"Installing spaCy model...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        print(\"Loaded spaCy model after installation\")\n",
    "    except:\n",
    "        print(\"Failed to load spaCy model, using pattern-based extraction only\")\n",
    "        nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f945d77",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to extract entities if not already present\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extract named entities using spaCy.\"\"\"\n",
    "    if pd.isna(text) or nlp is None:\n",
    "        return [], [], [], []\n",
    "    \n",
    "    people = []\n",
    "    places = []\n",
    "    organizations = []\n",
    "    dates = []\n",
    "    \n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text[:100000])  # Limit text length to avoid memory issues\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            people.append(ent.text)\n",
    "        elif ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "            places.append(ent.text)\n",
    "        elif ent.label_ == \"ORG\":\n",
    "            organizations.append(ent.text)\n",
    "        elif ent.label_ in [\"DATE\", \"TIME\"]:\n",
    "            dates.append(ent.text)\n",
    "    \n",
    "    return people, places, organizations, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply entity extraction if needed\n",
    "if 'people' not in df.columns or len(df['people'].iloc[0]) == 0:\n",
    "    print(\"Extracting entities...\")\n",
    "    # Process in batches to avoid memory issues\n",
    "    batch_size = 50\n",
    "    all_people = []\n",
    "    all_places = []\n",
    "    all_orgs = []\n",
    "    all_dates = []\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df['text'].iloc[i:i+batch_size]\n",
    "        batch_results = [extract_entities(text) for text in batch]\n",
    "        \n",
    "        batch_people = [res[0] for res in batch_results]\n",
    "        batch_places = [res[1] for res in batch_results]\n",
    "        batch_orgs = [res[2] for res in batch_results]\n",
    "        batch_dates = [res[3] for res in batch_results]\n",
    "        \n",
    "        all_people.extend(batch_people)\n",
    "        all_places.extend(batch_places)\n",
    "        all_orgs.extend(batch_orgs)\n",
    "        all_dates.extend(batch_dates)\n",
    "        \n",
    "        print(f\"Processed batch {i//batch_size + 1}/{(len(df)-1)//batch_size + 1}\")\n",
    "    \n",
    "    df['people'] = all_people\n",
    "    df['places'] = all_places\n",
    "    df['organizations'] = all_orgs\n",
    "    df['dates'] = all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad9933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count entities per document\n",
    "df['people_count'] = df['people'].apply(len)\n",
    "df['places_count'] = df['places'].apply(len)\n",
    "df['org_count'] = df['organizations'].apply(len)\n",
    "if 'dates' in df.columns:\n",
    "    df['date_count'] = df['dates'].apply(len)\n",
    "if 'event_types' in df.columns:\n",
    "    df['event_count'] = df['event_types'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce23adc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Analyze entity distributions\n",
    "print(\"Analyzing entity distributions...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89fa73a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to count and analyze entities\n",
    "def analyze_entities(entity_lists, min_freq=2):\n",
    "    \"\"\"Count and analyze entities across documents.\"\"\"\n",
    "    all_entities = []\n",
    "    for entities in entity_lists:\n",
    "        all_entities.extend(entities)\n",
    "    \n",
    "    # Count entities\n",
    "    entity_counts = Counter(all_entities)\n",
    "    \n",
    "    # Filter by minimum frequency\n",
    "    filtered_counts = {k: v for k, v in entity_counts.items() if v >= min_freq}\n",
    "    \n",
    "    return filtered_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cedbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze entities by type\n",
    "people_counts = analyze_entities(df['people'], config[\"min_entity_freq\"])\n",
    "place_counts = analyze_entities(df['places'], config[\"min_entity_freq\"])\n",
    "org_counts = analyze_entities(df['organizations'], config[\"min_entity_freq\"])\n",
    "if 'event_types' in df.columns:\n",
    "    event_counts = Counter([event for events in df['event_types'] for event in events if isinstance(events, list)])\n",
    "else:\n",
    "    event_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a56bb7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"Found {len(people_counts)} unique people mentioned at least {config['min_entity_freq']} times\")\n",
    "print(f\"Found {len(place_counts)} unique places mentioned at least {config['min_entity_freq']} times\")\n",
    "print(f\"Found {len(org_counts)} unique organizations mentioned at least {config['min_entity_freq']} times\")\n",
    "print(f\"Found {len(event_counts)} unique event types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0c479",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Analyze entity distribution by label (fake vs real)\n",
    "def analyze_entity_by_label(df, entity_col):\n",
    "    \"\"\"Analyze entity distribution by label.\"\"\"\n",
    "    fake_entities = []\n",
    "    real_entities = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if row['label'] == 0:  # Fake\n",
    "            fake_entities.extend(row[entity_col])\n",
    "        else:  # Real\n",
    "            real_entities.extend(row[entity_col])\n",
    "    \n",
    "    fake_counts = Counter(fake_entities)\n",
    "    real_counts = Counter(real_entities)\n",
    "    \n",
    "    return fake_counts, real_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb695a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze entities by label\n",
    "people_fake, people_real = analyze_entity_by_label(df, 'people')\n",
    "places_fake, places_real = analyze_entity_by_label(df, 'places')\n",
    "orgs_fake, orgs_real = analyze_entity_by_label(df, 'organizations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze events by label if available\n",
    "if 'event_types' in df.columns:\n",
    "    events_fake = []\n",
    "    events_real = []\n",
    "    for i, row in df.iterrows():\n",
    "        if isinstance(row['event_types'], list):\n",
    "            if row['label'] == 0:  # Fake\n",
    "                events_fake.extend(row['event_types'])\n",
    "            else:  # Real\n",
    "                events_real.extend(row['event_types'])\n",
    "\n",
    "    events_fake_counts = Counter(events_fake)\n",
    "    events_real_counts = Counter(events_real)\n",
    "else:\n",
    "    events_fake_counts = Counter()\n",
    "    events_real_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3f3ea",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "print(\"Creating visualizations...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cc11a3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to create entity comparison chart\n",
    "def create_entity_comparison(fake_counts, real_counts, entity_type, top_n=10):\n",
    "    \"\"\"Create comparison chart for entity distribution in fake vs real news.\"\"\"\n",
    "    # Get top entities by total count\n",
    "    combined = Counter()\n",
    "    for k, v in fake_counts.items():\n",
    "        combined[k] += v\n",
    "    for k, v in real_counts.items():\n",
    "        combined[k] += v\n",
    "    \n",
    "    top_entities = [entity for entity, count in combined.most_common(top_n)]\n",
    "    \n",
    "    # Create dataframe for plotting\n",
    "    plot_data = []\n",
    "    for entity in top_entities:\n",
    "        fake_count = fake_counts.get(entity, 0)\n",
    "        real_count = real_counts.get(entity, 0)\n",
    "        total = fake_count + real_count\n",
    "        if total > 0:\n",
    "            fake_ratio = fake_count / total\n",
    "            real_ratio = real_count / total\n",
    "            plot_data.append({\n",
    "                'Entity': entity,\n",
    "                'Fake': fake_count,\n",
    "                'Real': real_count,\n",
    "                'Fake_Ratio': fake_ratio,\n",
    "                'Real_Ratio': real_ratio\n",
    "            })\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_df[['Fake', 'Real']].plot(kind='bar', stacked=True, figsize=(12, 8))\n",
    "    plt.title(f'Distribution of {entity_type} in Fake and Real News')\n",
    "    plt.xlabel(entity_type)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(len(plot_df)), plot_df['Entity'], rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{results_dir}/non_graphx_{entity_type.lower()}_distribution.png\")\n",
    "    \n",
    "    return plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392912f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity comparison charts\n",
    "if people_counts:\n",
    "    people_comparison = create_entity_comparison(people_fake, people_real, 'People', config[\"top_n_entities\"])\n",
    "if place_counts:\n",
    "    places_comparison = create_entity_comparison(places_fake, places_real, 'Places', config[\"top_n_entities\"])\n",
    "if org_counts:\n",
    "    orgs_comparison = create_entity_comparison(orgs_fake, orgs_real, 'Organizations', config[\"top_n_entities\"])\n",
    "if event_counts:\n",
    "    events_comparison = create_entity_comparison(events_fake_counts, events_real_counts, 'Events', config[\"top_n_entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity count comparison by article type\n",
    "plt.figure(figsize=(10, 6))\n",
    "entity_counts = df.groupby('label').agg({\n",
    "    'people_count': 'mean',\n",
    "    'places_count': 'mean',\n",
    "    'org_count': 'mean'\n",
    "}).reset_index()\n",
    "if 'event_count' in df.columns:\n",
    "    entity_counts['event_count'] = df.groupby('label')['event_count'].mean().values\n",
    "entity_counts['label'] = entity_counts['label'].map({0: 'Fake', 1: 'Real'})\n",
    "entity_counts = entity_counts.melt(id_vars=['label'], var_name='Entity Type', value_name='Average Count')\n",
    "entity_counts['Entity Type'] = entity_counts['Entity Type'].str.replace('_count', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68d5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Entity Type', y='Average Count', hue='label', data=entity_counts)\n",
    "plt.title('Average Entity Counts in Fake vs Real News')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dir}/non_graphx_avg_entity_counts.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1add3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dashboard visualization\n",
    "plt.figure(figsize=(15, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity count comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x='Entity Type', y='Average Count', hue='label', data=entity_counts)\n",
    "plt.title('Average Entity Counts')\n",
    "plt.legend(title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53dfe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top people comparison\n",
    "if people_counts:\n",
    "    plt.subplot(2, 2, 2)\n",
    "    top_5_people = people_comparison.sort_values('Fake', ascending=False).head(5)\n",
    "    top_5_people[['Fake', 'Real']].plot(kind='bar', stacked=True)\n",
    "    plt.title('Top 5 People Mentioned')\n",
    "    plt.xticks(range(len(top_5_people)), top_5_people['Entity'], rotation=45, ha='right')\n",
    "    plt.legend(title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d23bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top places comparison\n",
    "if place_counts:\n",
    "    plt.subplot(2, 2, 3)\n",
    "    top_5_places = places_comparison.sort_values('Fake', ascending=False).head(5)\n",
    "    top_5_places[['Fake', 'Real']].plot(kind='bar', stacked=True)\n",
    "    plt.title('Top 5 Places Mentioned')\n",
    "    plt.xticks(range(len(top_5_places)), top_5_places['Entity'], rotation=45, ha='right')\n",
    "    plt.legend(title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749b46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top events comparison\n",
    "if event_counts:\n",
    "    plt.subplot(2, 2, 4)\n",
    "    top_5_events = events_comparison.sort_values('Fake', ascending=False).head(5)\n",
    "    top_5_events[['Fake', 'Real']].plot(kind='bar', stacked=True)\n",
    "    plt.title('Top 5 Events Mentioned')\n",
    "    plt.xticks(range(len(top_5_events)), top_5_events['Entity'], rotation=45, ha='right')\n",
    "    plt.legend(title='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e06619",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dir}/non_graphx_entity_dashboard.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf73a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modeling with TF-IDF and SVD\n",
    "print(\"Performing topic modeling...\")\n",
    "# Prepare text data\n",
    "texts = df['text'].fillna('').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=config[\"n_features\"],\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform topic modeling with SVD (similar to LSA)\n",
    "svd = TruncatedSVD(n_components=config[\"n_topics\"])\n",
    "topic_matrix = svd.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d32e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top terms for each topic\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "topics = []\n",
    "for i, comp in enumerate(svd.components_):\n",
    "    terms_idx = comp.argsort()[:-11:-1]  # Get indices of top 10 terms\n",
    "    terms = [feature_names[idx] for idx in terms_idx]\n",
    "    topics.append(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad65edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Topics extracted:\")\n",
    "for i, topic_terms in enumerate(topics):\n",
    "    print(f\"Topic {i}: {', '.join(topic_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign dominant topic to each document\n",
    "df['dominant_topic'] = topic_matrix.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43485727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze topic distribution by label\n",
    "topic_distribution = pd.crosstab(df['dominant_topic'], df['label'], rownames=['dominant_topic'], colnames=['label'])\n",
    "topic_distribution.columns = ['Fake', 'Real']\n",
    "topic_distribution['Total'] = topic_distribution['Fake'] + topic_distribution['Real']\n",
    "topic_distribution['Fake_Ratio'] = topic_distribution['Fake'] / topic_distribution['Total']\n",
    "topic_distribution['Real_Ratio'] = topic_distribution['Real'] / topic_distribution['Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f04d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Topic distribution by label:\")\n",
    "print(topic_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80023e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "topic_distribution[['Fake', 'Real']].plot(kind='bar', stacked=True)\n",
    "plt.title('Topic Distribution in Fake vs Real News')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{results_dir}/non_graphx_topic_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a7010",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create features for machine learning\n",
    "print(\"Creating non-GraphX features for machine learning...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05f07c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Entity-based features\n",
    "def create_entity_features(row):\n",
    "    features = {}\n",
    "    \n",
    "    # Entity count features\n",
    "    features['person_count'] = len(row['people']) if 'people' in row and row['people'] else 0\n",
    "    features['place_count'] = len(row['places']) if 'places' in row and row['places'] else 0\n",
    "    features['org_count'] = len(row['organizations']) if 'organizations' in row and row['organizations'] else 0\n",
    "    if 'event_types' in row and row['event_types']:\n",
    "        features['event_count'] = len(row['event_types'])\n",
    "    else:\n",
    "        features['event_count'] = 0\n",
    "    \n",
    "    # Entity presence features (for top entities)\n",
    "    top_people = [entity for entity, _ in people_counts.most_common(10)]\n",
    "    for person in top_people:\n",
    "        features[f'has_person_{person.replace(\" \", \"_\")}'] = 1 if person in row['people'] else 0\n",
    "        \n",
    "    top_places = [entity for entity, _ in place_counts.most_common(10)]\n",
    "    for place in top_places:\n",
    "        features[f'has_place_{place.replace(\" \", \"_\")}'] = 1 if place in row['places'] else 0\n",
    "        \n",
    "    top_orgs = [entity for entity, _ in org_counts.most_common(10)]\n",
    "    for org in top_orgs:\n",
    "        features[f'has_org_{org.replace(\" \", \"_\")}'] = 1 if org in row['organizations'] else 0\n",
    "    \n",
    "    # Topic features\n",
    "    for i in range(config[\"n_topics\"]):\n",
    "        features[f'topic_{i}_score'] = topic_matrix[row.name, i]\n",
    "    \n",
    "    # Text statistics features\n",
    "    if 'text' in row and not pd.isna(row['text']):\n",
    "        text = row['text']\n",
    "        features['text_length'] = len(text)\n",
    "        features['word_count'] = len(text.split())\n",
    "        features['sentence_count'] = len(sent_tokenize(text))\n",
    "        features['avg_word_length'] = sum(len(word) for word in text.split()) / max(1, len(text.split()))\n",
    "        features['avg_sentence_length'] = features['word_count'] / max(1, features['sentence_count'])\n",
    "    else:\n",
    "        features['text_length'] = 0\n",
    "        features['word_count'] = 0\n",
    "        features['sentence_count'] = 0\n",
    "        features['avg_word_length'] = 0\n",
    "        features['avg_sentence_length'] = 0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cebfbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature creation\n",
    "non_graphx_features = []\n",
    "for i, row in df.iterrows():\n",
    "    features = create_entity_features(row)\n",
    "    non_graphx_features.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a9fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "non_graphx_features_df = pd.DataFrame(non_graphx_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393079ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to original DataFrame\n",
    "for col in non_graphx_features_df.columns:\n",
    "    df[f'non_graphx_{col}'] = non_graphx_features_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced dataset with non-GraphX features\n",
    "df.to_csv(f\"{data_dir}/news_sample_non_graphx_enhanced.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df892f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save non-GraphX analysis results\n",
    "non_graphx_results = {\n",
    "    \"entities\": {\n",
    "        \"people_count\": len(people_counts),\n",
    "        \"places_count\": len(place_counts),\n",
    "        \"organizations_count\": len(org_counts),\n",
    "        \"events_count\": len(event_counts)\n",
    "    },\n",
    "    \"top_people\": {k: v for k, v in sorted(people_counts.items(), key=lambda x: x[1], reverse=True)[:config[\"top_n_entities\"]]},\n",
    "    \"top_places\": {k: v for k, v in sorted(place_counts.items(), key=lambda x: x[1], reverse=True)[:config[\"top_n_entities\"]]},\n",
    "    \"top_organizations\": {k: v for k, v in sorted(org_counts.items(), key=lambda x: x[1], reverse=True)[:config[\"top_n_entities\"]]},\n",
    "    \"topics\": {i: topics[i] for i in range(len(topics))},\n",
    "    \"topic_distribution\": topic_distribution.to_dict(),\n",
    "    \"execution_time\": time.time() - start_time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb35de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{results_dir}/non_graphx_analysis.json\", \"w\") as f:\n",
    "    json.dump(non_graphx_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9760e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nNon-GraphX entity analysis completed in {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"Enhanced dataset saved to {data_dir}/news_sample_non_graphx_enhanced.csv\")\n",
    "print(f\"Results saved to {results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Last modified: May 29, 2025
