{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9586ce99",
   "metadata": {},
   "source": [
    "# Fake News Detection: Clustering Analysis\n",
    "\n",
    "This notebook contains all the necessary code for clustering analysis in the fake news detection project. The code is organized into independent functions, without dependencies on external modules or classes, to facilitate execution in Databricks Community Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8710d",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec5fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session optimized for Databricks Community Edition\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection_ClusteringAnalysis\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce38563",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Start timer for performance tracking\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36b575",
   "metadata": {},
   "source": [
    "## Reusable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39879a2",
   "metadata": {},
   "source": [
    "### Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec74347",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_preprocessed_data(path=\"dbfs:/FileStore/fake_news_detection/preprocessed_data/preprocessed_news.parquet\"):\n",
    "    \"\"\"\n",
    "    Load preprocessed data from Parquet file.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the preprocessed data Parquet file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Spark DataFrame with preprocessed data\n",
    "    \"\"\"\n",
    "    print(f\"Loading preprocessed data from {path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load data from Parquet file\n",
    "        df = spark.read.parquet(path)\n",
    "        \n",
    "        # Display basic information\n",
    "        print(f\"Successfully loaded {df.count()} records.\")\n",
    "        df.printSchema()\n",
    "        \n",
    "        # Cache the DataFrame for better performance\n",
    "        df.cache()\n",
    "        print(\"Preprocessed DataFrame cached.\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading preprocessed data: {e}\")\n",
    "        print(\"Please ensure the preprocessing notebook ran successfully and saved data to the correct path.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff08393",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def convert_spark_to_pandas(spark_df, columns=None, limit=None):\n",
    "    \"\"\"\n",
    "    Convert Spark DataFrame to Pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        spark_df (DataFrame): Spark DataFrame to convert\n",
    "        columns (list): List of columns to include (None for all)\n",
    "        limit (int): Maximum number of rows to convert (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame\n",
    "    \"\"\"\n",
    "    print(\"Converting Spark DataFrame to Pandas DataFrame...\")\n",
    "    \n",
    "    if spark_df is None:\n",
    "        print(\"Error: Input DataFrame is None\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Select specified columns or all columns\n",
    "        if columns:\n",
    "            df = spark_df.select(columns)\n",
    "        else:\n",
    "            df = spark_df\n",
    "        \n",
    "        # Limit rows if specified\n",
    "        if limit:\n",
    "            df = df.limit(limit)\n",
    "        \n",
    "        # Convert to Pandas\n",
    "        pandas_df = df.toPandas()\n",
    "        \n",
    "        print(f\"Converted {len(pandas_df)} rows to Pandas DataFrame\")\n",
    "        return pandas_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting to Pandas DataFrame: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb23d7",
   "metadata": {},
   "source": [
    "### Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3147ae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_text(df, text_column=\"text\", title_column=None):\n",
    "    \"\"\"\n",
    "    Preprocess text data for clustering.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Pandas DataFrame with text data\n",
    "        text_column (str): Name of the text column\n",
    "        title_column (str): Name of the title column (optional)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with preprocessed text\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing text data...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Fill NaN values\n",
    "    processed_df[text_column] = processed_df[text_column].fillna('')\n",
    "    \n",
    "    # Process title if available\n",
    "    if title_column and title_column in processed_df.columns:\n",
    "        processed_df[title_column] = processed_df[title_column].fillna('')\n",
    "        # Combine title and text for better context\n",
    "        processed_df['content'] = processed_df[title_column] + \" \" + processed_df[text_column]\n",
    "    else:\n",
    "        processed_df['content'] = processed_df[text_column]\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    processed_df['content'] = processed_df['content'].str.lower()\n",
    "    \n",
    "    print(f\"Dataset shape: {processed_df.shape}\")\n",
    "    if 'label' in processed_df.columns:\n",
    "        print(f\"Class distribution:\\n{processed_df['label'].value_counts()}\")\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a13ef43",
   "metadata": {},
   "source": [
    "### Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3100b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_tfidf_features(df, text_column=\"content\", max_features=1000):\n",
    "    \"\"\"\n",
    "    Extract TF-IDF features from text.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Pandas DataFrame with text data\n",
    "        text_column (str): Name of the text column\n",
    "        max_features (int): Maximum number of features to extract\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X_tfidf, vectorizer) - TF-IDF matrix and vectorizer\n",
    "    \"\"\"\n",
    "    print(f\"Extracting TF-IDF features (max_features={max_features})...\")\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features, stop_words='english')\n",
    "    \n",
    "    # Fit and transform text data\n",
    "    X_tfidf = vectorizer.fit_transform(df[text_column])\n",
    "    \n",
    "    print(f\"TF-IDF matrix shape: {X_tfidf.shape}\")\n",
    "    \n",
    "    # Get feature names for later use\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(f\"Number of features: {len(feature_names)}\")\n",
    "    \n",
    "    return X_tfidf, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3f168",
   "metadata": {},
   "source": [
    "### Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972d3d4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def find_optimal_k(X, k_range=range(2, 11)):\n",
    "    \"\"\"\n",
    "    Find optimal number of clusters using silhouette score.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        k_range: Range of k values to try\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (optimal_k, silhouette_scores) - Optimal k and all scores\n",
    "    \"\"\"\n",
    "    print(\"Finding optimal number of clusters using silhouette score...\")\n",
    "    \n",
    "    silhouette_scores = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        print(f\"Trying k={k}...\")\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        print(f\"K={k}, Silhouette Score={silhouette_avg:.4f}\")\n",
    "    \n",
    "    # Find optimal k\n",
    "    optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "    print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "    \n",
    "    return optimal_k, silhouette_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec226035",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_kmeans_clustering(X, k):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        k: Number of clusters\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (kmeans, cluster_labels) - KMeans model and cluster labels\n",
    "    \"\"\"\n",
    "    print(f\"Performing K-means clustering with k={k}...\")\n",
    "    \n",
    "    # Run K-means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    print(f\"K-means clustering completed with {k} clusters\")\n",
    "    \n",
    "    return kmeans, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bdc0b9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_dbscan_clustering(X, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Perform DBSCAN clustering.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        eps: Maximum distance between samples\n",
    "        min_samples: Minimum number of samples in a neighborhood\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (dbscan, cluster_labels) - DBSCAN model and cluster labels\n",
    "    \"\"\"\n",
    "    print(f\"Performing DBSCAN clustering (eps={eps}, min_samples={min_samples})...\")\n",
    "    \n",
    "    # Run DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    cluster_labels = dbscan.fit_predict(X)\n",
    "    \n",
    "    # Count number of clusters (excluding noise points with label -1)\n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    n_noise = list(cluster_labels).count(-1)\n",
    "    \n",
    "    print(f\"DBSCAN clustering completed with {n_clusters} clusters and {n_noise} noise points\")\n",
    "    \n",
    "    return dbscan, cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bad3db",
   "metadata": {},
   "source": [
    "### Topic Modeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea901f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_lda_topic_modeling(X, n_topics):\n",
    "    \"\"\"\n",
    "    Perform topic modeling with Latent Dirichlet Allocation.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        n_topics: Number of topics\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (lda, doc_topic_dist) - LDA model and document-topic distributions\n",
    "    \"\"\"\n",
    "    print(f\"Performing topic modeling with LDA (n_topics={n_topics})...\")\n",
    "    \n",
    "    # Run LDA\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(X)\n",
    "    \n",
    "    # Get document-topic distributions\n",
    "    doc_topic_dist = lda.transform(X)\n",
    "    \n",
    "    print(f\"LDA topic modeling completed with {n_topics} topics\")\n",
    "    \n",
    "    return lda, doc_topic_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52243b9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_top_terms(model, feature_names, n_top_words=10):\n",
    "    \"\"\"\n",
    "    Extract top terms for each cluster or topic.\n",
    "    \n",
    "    Args:\n",
    "        model: Clustering or topic model\n",
    "        feature_names: List of feature names\n",
    "        n_top_words: Number of top words to extract\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with top terms for each cluster/topic\n",
    "    \"\"\"\n",
    "    print(f\"Extracting top {n_top_words} terms for each cluster/topic...\")\n",
    "    \n",
    "    top_terms = {}\n",
    "    \n",
    "    # Check if model is KMeans\n",
    "    if hasattr(model, 'cluster_centers_'):\n",
    "        # For KMeans\n",
    "        order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "        for cluster in range(len(order_centroids)):\n",
    "            top_terms[cluster] = [feature_names[ind] for ind in order_centroids[cluster, :n_top_words]]\n",
    "            print(f\"Cluster {cluster}: {', '.join(top_terms[cluster])}\")\n",
    "    \n",
    "    # Check if model is LDA\n",
    "    elif hasattr(model, 'components_'):\n",
    "        # For LDA\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            top_indices = topic.argsort()[:-n_top_words-1:-1]\n",
    "            top_terms[topic_idx] = [feature_names[i] for i in top_indices]\n",
    "            print(f\"Topic {topic_idx}: {', '.join(top_terms[topic_idx])}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Error: Unsupported model type\")\n",
    "        return None\n",
    "    \n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a84e98",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558687f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_pca(X, n_components=2):\n",
    "    \"\"\"\n",
    "    Perform Principal Component Analysis for dimensionality reduction.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        n_components: Number of components\n",
    "        \n",
    "    Returns:\n",
    "        array: Reduced feature matrix\n",
    "    \"\"\"\n",
    "    print(f\"Performing PCA with {n_components} components...\")\n",
    "    \n",
    "    # Convert sparse matrix to dense if needed\n",
    "    if hasattr(X, 'toarray'):\n",
    "        X_dense = X.toarray()\n",
    "    else:\n",
    "        X_dense = X\n",
    "    \n",
    "    # Run PCA\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_dense)\n",
    "    \n",
    "    print(f\"PCA completed, explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    \n",
    "    return X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76be3225",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_tsne(X, n_components=2, perplexity=30):\n",
    "    \"\"\"\n",
    "    Perform t-SNE for dimensionality reduction.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        n_components: Number of components\n",
    "        perplexity: Perplexity parameter for t-SNE\n",
    "        \n",
    "    Returns:\n",
    "        array: Reduced feature matrix\n",
    "    \"\"\"\n",
    "    print(f\"Performing t-SNE with {n_components} components (perplexity={perplexity})...\")\n",
    "    \n",
    "    # Convert sparse matrix to dense if needed\n",
    "    if hasattr(X, 'toarray'):\n",
    "        X_dense = X.toarray()\n",
    "    else:\n",
    "        X_dense = X\n",
    "    \n",
    "    # Run t-SNE\n",
    "    tsne = TSNE(n_components=n_components, random_state=42, perplexity=perplexity)\n",
    "    X_tsne = tsne.fit_transform(X_dense)\n",
    "    \n",
    "    print(\"t-SNE completed\")\n",
    "    \n",
    "    return X_tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1aaf9b",
   "metadata": {},
   "source": [
    "### Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c54a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_clusters(df, cluster_column):\n",
    "    \"\"\"\n",
    "    Analyze clusters and their relationship with labels.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with cluster assignments\n",
    "        cluster_column: Name of the cluster column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with cluster statistics\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing clusters from column '{cluster_column}'...\")\n",
    "    \n",
    "    # Check if label column exists\n",
    "    if 'label' not in df.columns:\n",
    "        print(\"Warning: 'label' column not found, cannot analyze relationship with labels\")\n",
    "        cluster_stats = df.groupby(cluster_column).size().reset_index(name='count')\n",
    "        return cluster_stats\n",
    "    \n",
    "    # Group by cluster and calculate statistics\n",
    "    cluster_stats = df.groupby(cluster_column).agg({\n",
    "        'label': ['count', 'mean'],  # mean of label gives proportion of real news (label=1)\n",
    "    })\n",
    "    \n",
    "    # Flatten column names\n",
    "    cluster_stats.columns = ['count', 'real_proportion']\n",
    "    \n",
    "    # Calculate fake proportion\n",
    "    cluster_stats['fake_proportion'] = 1 - cluster_stats['real_proportion']\n",
    "    \n",
    "    # Reset index for easier handling\n",
    "    cluster_stats = cluster_stats.reset_index()\n",
    "    \n",
    "    print(cluster_stats)\n",
    "    \n",
    "    return cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2d31a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compare_clustering_methods(df, method1_column, method2_column):\n",
    "    \"\"\"\n",
    "    Compare two clustering methods.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with cluster assignments\n",
    "        method1_column: Name of the first method's column\n",
    "        method2_column: Name of the second method's column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Cross-tabulation of the two methods\n",
    "    \"\"\"\n",
    "    print(f\"Comparing clustering methods: '{method1_column}' vs '{method2_column}'...\")\n",
    "    \n",
    "    # Create cross-tabulation\n",
    "    comparison = pd.crosstab(df[method1_column], df[method2_column])\n",
    "    \n",
    "    print(f\"{method1_column} vs {method2_column}:\")\n",
    "    print(comparison)\n",
    "    \n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b598c0a8",
   "metadata": {},
   "source": [
    "### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfec2e0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_silhouette_scores(k_values, silhouette_scores, output_path=None):\n",
    "    \"\"\"\n",
    "    Visualize silhouette scores for different k values.\n",
    "    \n",
    "    Args:\n",
    "        k_values: Range of k values\n",
    "        silhouette_scores: List of silhouette scores\n",
    "        output_path: Path to save the figure (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_values, silhouette_scores, 'o-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Score vs. Number of Clusters')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"Silhouette score plot saved to {output_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d07ef",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_clusters_2d(X_2d, labels, title, colormap='viridis', output_path=None):\n",
    "    \"\"\"\n",
    "    Visualize clusters in 2D.\n",
    "    \n",
    "    Args:\n",
    "        X_2d: 2D feature matrix\n",
    "        labels: Cluster labels\n",
    "        title: Plot title\n",
    "        colormap: Colormap to use\n",
    "        output_path: Path to save the figure (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels, cmap=colormap, alpha=0.7)\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"Cluster visualization saved to {output_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db37360",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_cluster_composition(cluster_stats, output_path=None):\n",
    "    \"\"\"\n",
    "    Visualize cluster composition (real vs fake news).\n",
    "    \n",
    "    Args:\n",
    "        cluster_stats: DataFrame with cluster statistics\n",
    "        output_path: Path to save the figure (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    if 'real_proportion' in cluster_stats.columns and 'fake_proportion' in cluster_stats.columns:\n",
    "        # Set cluster column as index if it's not already\n",
    "        if 'cluster' in cluster_stats.columns:\n",
    "            cluster_stats = cluster_stats.set_index('cluster')\n",
    "        \n",
    "        # Plot stacked bar chart\n",
    "        cluster_stats[['real_proportion', 'fake_proportion']].plot(\n",
    "            kind='bar', stacked=True, colormap='coolwarm'\n",
    "        )\n",
    "        plt.title('Cluster Composition (Real vs Fake News)')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Proportion')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.legend(['Real News', 'Fake News'])\n",
    "        \n",
    "        if output_path:\n",
    "            plt.savefig(output_path)\n",
    "            print(f\"Cluster composition plot saved to {output_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Error: Required columns not found in cluster_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79190bf8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_top_terms(top_terms, output_path=None):\n",
    "    \"\"\"\n",
    "    Visualize top terms for each cluster or topic.\n",
    "    \n",
    "    Args:\n",
    "        top_terms: Dictionary with top terms\n",
    "        output_path: Path to save the figure (optional)\n",
    "    \"\"\"\n",
    "    n_clusters = len(top_terms)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        plt.subplot(int(np.ceil(n_clusters/2)), 2, i+1)\n",
    "        y_pos = np.arange(len(top_terms[i]))\n",
    "        plt.barh(y_pos, range(len(top_terms[i]), 0, -1))\n",
    "        plt.yticks(y_pos, top_terms[i])\n",
    "        plt.title(f'Cluster/Topic {i}')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"Top terms visualization saved to {output_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b8db3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_clustering_dashboard(df, X_2d, cluster_column='cluster', output_path=None):\n",
    "    \"\"\"\n",
    "    Create a dashboard-style visualization of clustering results.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with cluster assignments\n",
    "        X_2d: 2D feature matrix (e.g., from t-SNE)\n",
    "        cluster_column: Name of the cluster column\n",
    "        output_path: Path to save the figure (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Cluster distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    cluster_counts = df[cluster_column].value_counts().sort_index()\n",
    "    plt.bar(cluster_counts.index, cluster_counts.values)\n",
    "    plt.title('Documents per Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Check if topic column exists\n",
    "    if 'dominant_topic' in df.columns:\n",
    "        # Topic distribution\n",
    "        plt.subplot(2, 2, 2)\n",
    "        topic_counts = df['dominant_topic'].value_counts().sort_index()\n",
    "        plt.bar(topic_counts.index, topic_counts.values)\n",
    "        plt.title('Documents per Topic')\n",
    "        plt.xlabel('Topic')\n",
    "        plt.ylabel('Count')\n",
    "    \n",
    "    # t-SNE visualization with clusters\n",
    "    plt.subplot(2, 2, 3)\n",
    "    scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=df[cluster_column], cmap='viridis', alpha=0.7, s=30)\n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.title('Clusters (t-SNE)')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    \n",
    "    # Check if label column exists\n",
    "    if 'label' in df.columns:\n",
    "        # t-SNE visualization with original labels\n",
    "        plt.subplot(2, 2, 4)\n",
    "        scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=df['label'], cmap='coolwarm', alpha=0.7, s=30)\n",
    "        plt.colorbar(scatter, label='Label (0=Fake, 1=Real)')\n",
    "        plt.title('Original Labels (t-SNE)')\n",
    "        plt.xlabel('t-SNE Component 1')\n",
    "        plt.ylabel('t-SNE Component 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"Clustering dashboard saved to {output_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee20c99b",
   "metadata": {},
   "source": [
    "### Data Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3aced9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    \"\"\"\n",
    "    Save a model to disk.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to save\n",
    "        path: Path where to save the model\n",
    "    \"\"\"\n",
    "    print(f\"Saving model to {path}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1ef8b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_results(df, path):\n",
    "    \"\"\"\n",
    "    Save clustering results to disk.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with clustering results\n",
    "        path: Path where to save the results\n",
    "    \"\"\"\n",
    "    print(f\"Saving clustering results to {path}...\")\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(path, index=False)\n",
    "        print(f\"Results saved to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d9be0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_to_parquet(df, path):\n",
    "    \"\"\"\n",
    "    Save DataFrame to Parquet format.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame to save\n",
    "        path: Path where to save the DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"Saving DataFrame to {path}...\")\n",
    "    \n",
    "    try:\n",
    "        df.write.mode(\"overwrite\").parquet(path)\n",
    "        print(f\"DataFrame saved to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d8418a",
   "metadata": {},
   "source": [
    "## Complete Clustering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63da8d1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_clustering_analysis(\n",
    "    input_path=\"dbfs:/FileStore/fake_news_detection/preprocessed_data/preprocessed_news.parquet\",\n",
    "    output_dir=\"dbfs:/FileStore/fake_news_detection/clustering_data\",\n",
    "    max_features=1000,\n",
    "    k_range=range(2, 11),\n",
    "    sample_size=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete pipeline for clustering analysis.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): Path to preprocessed data\n",
    "        output_dir (str): Directory to save results\n",
    "        max_features (int): Maximum number of features for TF-IDF\n",
    "        k_range (range): Range of k values to try\n",
    "        sample_size (int): Number of samples to use (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with references to analysis results\n",
    "    \"\"\"\n",
    "    print(\"Starting clustering analysis pipeline...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create output directories\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(output_dir.replace(\"dbfs:\", \"\"))\n",
    "    except:\n",
    "        print(\"Warning: Could not create directories. This is expected in local environments.\")\n",
    "        os.makedirs(output_dir.replace(\"dbfs:/\", \"/tmp/\"), exist_ok=True)\n",
    "    \n",
    "    # 1. Load preprocessed data\n",
    "    spark_df = load_preprocessed_data(input_path)\n",
    "    if spark_df is None:\n",
    "        print(\"Error: Could not load preprocessed data. Pipeline aborted.\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Convert to Pandas DataFrame\n",
    "    df = convert_spark_to_pandas(spark_df, limit=sample_size)\n",
    "    if df is None:\n",
    "        print(\"Error: Could not convert to Pandas DataFrame. Pipeline aborted.\")\n",
    "        return None\n",
    "    \n",
    "    # 3. Preprocess text\n",
    "    df = preprocess_text(df, text_column=\"text\", title_column=\"title\")\n",
    "    \n",
    "    # 4. Extract TF-IDF features\n",
    "    X_tfidf, vectorizer = extract_tfidf_features(df, text_column=\"content\", max_features=max_features)\n",
    "    \n",
    "    # 5. Find optimal number of clusters\n",
    "    optimal_k, silhouette_scores = find_optimal_k(X_tfidf, k_range)\n",
    "    \n",
    "    # 6. Visualize silhouette scores\n",
    "    visualize_silhouette_scores(\n",
    "        k_range, silhouette_scores, \n",
    "        output_path=f\"{output_dir.replace('dbfs:/', '/tmp/')}/silhouette_scores.png\"\n",
    "    )\n",
    "    \n",
    "    # 7. Perform K-means clustering\n",
    "    kmeans, cluster_labels = perform_kmeans_clustering(X_tfidf, optimal_k)\n",
    "    df['kmeans_cluster'] = cluster_labels\n",
    "    \n",
    "    # 8. Extract top terms for each cluster\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    kmeans_top_terms = extract_top_terms(kmeans, feature_names)\n",
    "    \n",
    "    # 9. Visualize top terms\n",
    "    visualize_top_terms(\n",
    "        kmeans_top_terms, \n",
    "        output_path=f\"{output_dir.replace('dbfs:/', '/tmp/')}/cluster_top_terms.png\"\n",
    "    )\n",
    "    \n",
    "    # 10. Perform dimensionality reduction for visualization\n",
    "    X_pca = perform_pca(X_tfidf)\n",
    "    X_tsne = perform_tsne(X_tfidf)\n",
    "    \n",
    "    # 11. Visualize clusters\n",
    "    visualize_clusters_2d(\n",
    "        X_pca, cluster_labels, \"K-means Clusters (PCA)\", \n",
    "        output_path=f\"{output_dir.replace('dbfs:/', '/tmp/')}/kmeans_clusters_pca.png\"\n",
    "    )\n",
    "    visualize_clusters_2d(\n",
    "        X_tsne, cluster_labels, \"K-means Clusters (t-SNE)\", \n",
    "        output_path=f\"{output_dir.replace('dbfs:/', '/tmp/')}/kmeans_clusters_tsne.png\"\n",
    "    )\n",
    "    \n",
    "    # 12. Analyze clusters\n",
    "    cluster_stats = analyze_clusters(df, 'kmeans_cluster')\n",
    "    \n",
    "    # 13. Visualize cluster composition\n",
    "    visualize_cluster_composition(\n",
    "        cluster_stats, \n",
    "        output_path=f\"{output_dir.replace('dbfs:/', '/tmp/')}/cluster_composition.png\"\n",
    "    )\n",
    "    \n",
    "    # 14. Perform topic modeling with LDA\n",
    "    lda, doc_topic_dist = perform_lda_topic_modeling(X_tfidf, optimal_k)\n",
    "    df['dominant_topic'] = np.argmax(doc_topic_dist, axis=1)\n",
    "    \n",
    "    # 15. Extract top terms for each topic\n",
    "    lda_top_terms = extract_top_terms(lda, feature_names)\n",
    "    \n",
    "    # 16. Visualize top terms for topics\n",
    "    visualize_top_terms(\n",
    "        lda_top_terms, \n",
    "        output_path=f\"{output_dir.replace('dbfs:/', '/tmp/')}/lda_topics.png\"\n",
    "    )\n",
    "    \n",
    "    # 17. Analyze topics\n",
    "    topic_stats = analyze_clusters(df, 'dominant_topic')\n",
    "    \n",
    "    # 18. Visualize topic composition\n",
    "    visualize_cluster_composition(\n",
    "        topic_stats, \n",
    "        output_path=f\"{output_dir.replace('dbfs:/', '/tmp/')}/topic_composition.png\"\n",
    "    )\n",
    "    \n",
    "    # 19. Compare K-means clusters with LDA topics\n",
    "    comparison = compare_clustering_methods(df, 'kmeans_cluster', 'dominant_topic')\n",
    "    \n",
    "    # 20. Create dashboard visualization\n",
    "    visualize_clustering_dashboard(\n",
    "        df, X_tsne, 'kmeans_cluster', \n",
    "        output_path=f\"{output_dir.replace('dbfs:/', '/tmp/')}/clustering_dashboard.png\"\n",
    "    )\n",
    "    \n",
    "    # 21. Save models and results\n",
    "    try:\n",
    "        save_model(\n",
    "            kmeans, \n",
    "            f\"{output_dir.replace('dbfs:/', '/tmp/')}/kmeans_model.pkl\"\n",
    "        )\n",
    "        save_model(\n",
    "            lda, \n",
    "            f\"{output_dir.replace('dbfs:/', '/tmp/')}/lda_model.pkl\"\n",
    "        )\n",
    "        save_model(\n",
    "            vectorizer, \n",
    "            f\"{output_dir.replace('dbfs:/', '/tmp/')}/tfidf_vectorizer.pkl\"\n",
    "        )\n",
    "        save_results(\n",
    "            df, \n",
    "            f\"{output_dir.replace('dbfs:/', '/tmp/')}/clustering_results.csv\"\n",
    "        )\n",
    "    except:\n",
    "        print(\"Warning: Could not save models and results to local files. This is expected in Databricks.\")\n",
    "    \n",
    "    # 22. Convert results back to Spark DataFrame for storage\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        results_df = spark.createDataFrame(df)\n",
    "        save_to_parquet(results_df, f\"{output_dir}/clustering_results.parquet\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save results to Parquet: {e}\")\n",
    "    \n",
    "    print(f\"\\nClustering analysis pipeline completed in {time.time() - start_time:.2f} seconds!\")\n",
    "    \n",
    "    return {\n",
    "        \"kmeans_model\": kmeans,\n",
    "        \"lda_model\": lda,\n",
    "        \"vectorizer\": vectorizer,\n",
    "        \"optimal_k\": optimal_k,\n",
    "        \"silhouette_scores\": silhouette_scores,\n",
    "        \"cluster_stats\": cluster_stats,\n",
    "        \"topic_stats\": topic_stats,\n",
    "        \"comparison\": comparison,\n",
    "        \"results_df\": df\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0dab18",
   "metadata": {},
   "source": [
    "## Step-by-Step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a3a7e2",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b807f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "spark_df = load_preprocessed_data()\n",
    "\n",
    "# Convert to Pandas DataFrame (limit to 5000 rows for demonstration)\n",
    "if spark_df:\n",
    "    df = convert_spark_to_pandas(spark_df, limit=5000)\n",
    "    \n",
    "    # Preprocess text\n",
    "    if df is not None:\n",
    "        df = preprocess_text(df, text_column=\"text\", title_column=\"title\")\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\nSample data after preprocessing:\")\n",
    "        print(df[['content', 'label']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eb1c22",
   "metadata": {},
   "source": [
    "### 2. Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TF-IDF features\n",
    "if 'df' in locals() and df is not None:\n",
    "    X_tfidf, vectorizer = extract_tfidf_features(df, text_column=\"content\", max_features=1000)\n",
    "    \n",
    "    # Display feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(\"\\nSample feature names:\")\n",
    "    print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62133b42",
   "metadata": {},
   "source": [
    "### 3. Find Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4bf955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of clusters\n",
    "if 'X_tfidf' in locals():\n",
    "    # Use a smaller range for demonstration\n",
    "    k_range = range(2, 7)\n",
    "    optimal_k, silhouette_scores = find_optimal_k(X_tfidf, k_range)\n",
    "    \n",
    "    # Visualize silhouette scores\n",
    "    visualize_silhouette_scores(k_range, silhouette_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c9d15",
   "metadata": {},
   "source": [
    "### 4. Perform K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6297d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering\n",
    "if 'X_tfidf' in locals() and 'optimal_k' in locals():\n",
    "    kmeans, cluster_labels = perform_kmeans_clustering(X_tfidf, optimal_k)\n",
    "    \n",
    "    # Add cluster labels to DataFrame\n",
    "    df['kmeans_cluster'] = cluster_labels\n",
    "    \n",
    "    # Display cluster distribution\n",
    "    print(\"\\nCluster distribution:\")\n",
    "    print(df['kmeans_cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f9b13e",
   "metadata": {},
   "source": [
    "### 5. Extract Top Terms for Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a7cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top terms for each cluster\n",
    "if 'kmeans' in locals() and 'feature_names' in locals():\n",
    "    kmeans_top_terms = extract_top_terms(kmeans, feature_names)\n",
    "    \n",
    "    # Visualize top terms\n",
    "    visualize_top_terms(kmeans_top_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87af86",
   "metadata": {},
   "source": [
    "### 6. Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a63f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform dimensionality reduction for visualization\n",
    "if 'X_tfidf' in locals():\n",
    "    # PCA for visualization\n",
    "    X_pca = perform_pca(X_tfidf)\n",
    "    \n",
    "    # t-SNE for better separation\n",
    "    X_tsne = perform_tsne(X_tfidf)\n",
    "    \n",
    "    # Visualize clusters with PCA\n",
    "    if 'cluster_labels' in locals():\n",
    "        visualize_clusters_2d(X_pca, cluster_labels, \"K-means Clusters (PCA)\")\n",
    "        \n",
    "        # Visualize clusters with t-SNE\n",
    "        visualize_clusters_2d(X_tsne, cluster_labels, \"K-means Clusters (t-SNE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19ccba9",
   "metadata": {},
   "source": [
    "### 7. Analyze Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9cac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters\n",
    "if 'df' in locals() and 'kmeans_cluster' in df.columns:\n",
    "    cluster_stats = analyze_clusters(df, 'kmeans_cluster')\n",
    "    \n",
    "    # Visualize cluster composition\n",
    "    visualize_cluster_composition(cluster_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2728032d",
   "metadata": {},
   "source": [
    "### 8. Perform Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972508a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform topic modeling with LDA\n",
    "if 'X_tfidf' in locals() and 'optimal_k' in locals():\n",
    "    lda, doc_topic_dist = perform_lda_topic_modeling(X_tfidf, optimal_k)\n",
    "    \n",
    "    # Add dominant topic to DataFrame\n",
    "    df['dominant_topic'] = np.argmax(doc_topic_dist, axis=1)\n",
    "    \n",
    "    # Display topic distribution\n",
    "    print(\"\\nTopic distribution:\")\n",
    "    print(df['dominant_topic'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8f50f",
   "metadata": {},
   "source": [
    "### 9. Extract Top Terms for Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract top terms for each topic\n",
    "if 'lda' in locals() and 'feature_names' in locals():\n",
    "    lda_top_terms = extract_top_terms(lda, feature_names)\n",
    "    \n",
    "    # Visualize top terms for topics\n",
    "    visualize_top_terms(lda_top_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1fdc10",
   "metadata": {},
   "source": [
    "### 10. Analyze Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1138b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze topics\n",
    "if 'df' in locals() and 'dominant_topic' in df.columns:\n",
    "    topic_stats = analyze_clusters(df, 'dominant_topic')\n",
    "    \n",
    "    # Visualize topic composition\n",
    "    visualize_cluster_composition(topic_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ac7a2",
   "metadata": {},
   "source": [
    "### 11. Compare K-means Clusters with LDA Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfcb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare K-means clusters with LDA topics\n",
    "if 'df' in locals() and 'kmeans_cluster' in df.columns and 'dominant_topic' in df.columns:\n",
    "    comparison = compare_clustering_methods(df, 'kmeans_cluster', 'dominant_topic')\n",
    "    \n",
    "    # Visualize comparison\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(comparison, annot=True, cmap='YlGnBu', fmt='d')\n",
    "    plt.title('K-means Clusters vs LDA Topics')\n",
    "    plt.xlabel('LDA Topic')\n",
    "    plt.ylabel('K-means Cluster')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a25abc",
   "metadata": {},
   "source": [
    "### 12. Create Dashboard Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af693f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dashboard visualization\n",
    "if 'df' in locals() and 'X_tsne' in locals() and 'kmeans_cluster' in df.columns:\n",
    "    visualize_clustering_dashboard(df, X_tsne, 'kmeans_cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c410c",
   "metadata": {},
   "source": [
    "### 13. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22424285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and results\n",
    "if all(var in locals() for var in ['kmeans', 'lda', 'vectorizer', 'df']):\n",
    "    # Create output directory\n",
    "    output_dir = \"/tmp/clustering_results\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save models\n",
    "    save_model(kmeans, f\"{output_dir}/kmeans_model.pkl\")\n",
    "    save_model(lda, f\"{output_dir}/lda_model.pkl\")\n",
    "    save_model(vectorizer, f\"{output_dir}/tfidf_vectorizer.pkl\")\n",
    "    \n",
    "    # Save results\n",
    "    save_results(df, f\"{output_dir}/clustering_results.csv\")\n",
    "    \n",
    "    print(f\"Models and results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05afad3f",
   "metadata": {},
   "source": [
    "### 14. Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ed6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete clustering analysis pipeline\n",
    "results = perform_clustering_analysis(\n",
    "    input_path=\"dbfs:/FileStore/fake_news_detection/preprocessed_data/preprocessed_news.parquet\",\n",
    "    output_dir=\"dbfs:/FileStore/fake_news_detection/clustering_data\",\n",
    "    max_features=1000,\n",
    "    k_range=range(2, 11),\n",
    "    sample_size=5000  # Limit to 5000 samples for demonstration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906888f",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "1. **Clustering Purpose**: Clustering helps identify natural groupings in the fake news dataset, revealing patterns that may not be apparent through supervised learning alone.\n",
    "\n",
    "2. **Feature Extraction**: TF-IDF vectorization is used to convert text into numerical features, capturing the importance of words in documents relative to the corpus.\n",
    "\n",
    "3. **Optimal Clusters**: The silhouette score is used to determine the optimal number of clusters, balancing cohesion within clusters and separation between clusters.\n",
    "\n",
    "4. **K-means vs LDA**: The notebook implements both K-means clustering (for direct grouping) and LDA topic modeling (for thematic analysis), providing complementary perspectives.\n",
    "\n",
    "5. **Visualization**: Multiple visualization techniques (PCA, t-SNE) are used to project high-dimensional data into 2D space for interpretation.\n",
    "\n",
    "6. **Cluster Analysis**: Each cluster is analyzed for its composition of fake vs. real news and its characteristic terms, helping identify thematic patterns.\n",
    "\n",
    "7. **Performance Considerations**: For large datasets, consider:\n",
    "   - Reducing the maximum number of features\n",
    "   - Using a sample of the data\n",
    "   - Limiting the range of k values to explore\n",
    "\n",
    "8. **Databricks Integration**: The code is optimized for Databricks Community Edition with appropriate configurations for memory and processing."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
