{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Simulation for Fake News Detection\n",
    "\n",
    "This notebook implements a streaming simulation for fake news detection using PySpark MLlib and Databricks-native functionality. Since true streaming is not available in the Community Edition, we emulate a streaming scenario by processing data in small batches.\n",
    "\n",
    "## Key Features\n",
    "- Batch-based streaming simulation\n",
    "- PySpark MLlib model inference\n",
    "- Interactive visualizations using Databricks-native and Plotly\n",
    "- Real-time analytics dashboard\n",
    "- Optimized for Databricks Community Edition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, we'll set up our Spark session with configurations optimized for the Databricks Community Edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, current_timestamp, count, desc, expr, struct, to_json\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, ArrayType, TimestampType\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "# Configure Spark session optimized for Databricks Community Edition\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection_StreamingSimulation\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths and Create Directories\n",
    "\n",
    "We'll define paths for data, models, and results, and create necessary directories in DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for Databricks DBFS\n",
    "stream_data_path = \"dbfs:/FileStore/fake_news_detection/data/stream_data\"\n",
    "stream_results_path = \"dbfs:/FileStore/fake_news_detection/data/stream_results\"\n",
    "model_path = \"dbfs:/FileStore/fake_news_detection/models/fake_news_best_model\"\n",
    "logs_path = \"dbfs:/FileStore/fake_news_detection/logs\"\n",
    "\n",
    "# Create directories if they don't exist using dbutils\n",
    "dbutils.fs.mkdirs(stream_data_path.replace(\"dbfs:\", \"\"))\n",
    "dbutils.fs.mkdirs(stream_results_path.replace(\"dbfs:\", \"\"))\n",
    "dbutils.fs.mkdirs(logs_path.replace(\"dbfs:\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "We'll load the pre-trained PySpark ML model for fake news detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "try:\n",
    "    model = PipelineModel.load(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Using a placeholder model path. Please update with the correct model path.\")\n",
    "    # If model doesn't exist, we'll use a placeholder for demonstration\n",
    "    model_path = \"/dbfs/FileStore/models/fake_news_best_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Streaming Data\n",
    "\n",
    "We'll create sample data for our streaming simulation by sampling from the Hive tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the HiveDataIngestion class\n",
    "import sys\n",
    "sys.path.append('/dbfs/FileStore/tables')\n",
    "from hive_data_ingestion import HiveDataIngestion\n",
    "\n",
    "# Create an instance\n",
    "data_ingestion = HiveDataIngestion(spark)\n",
    "\n",
    "# Load data from Hive tables\n",
    "real_df, fake_df = data_ingestion.load_data_from_hive()\n",
    "\n",
    "# Create a sample for streaming simulation\n",
    "# We'll use stratified sampling to get a balanced dataset\n",
    "real_sample = real_df.sample(fraction=0.05, seed=42)\n",
    "fake_sample = fake_df.sample(fraction=0.05, seed=42)\n",
    "\n",
    "# Combine samples and add unique IDs\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "real_sample = real_sample.withColumn(\"label\", lit(1))\n",
    "fake_sample = fake_sample.withColumn(\"label\", lit(0))\n",
    "\n",
    "# Combine samples\n",
    "stream_df = real_sample.union(fake_sample)\n",
    "\n",
    "# Add unique IDs\n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "stream_df = stream_df.withColumn(\"id\", row_number().over(window))\n",
    "\n",
    "# Shuffle the data\n",
    "stream_df = stream_df.orderBy(expr(\"rand()\"))\n",
    "\n",
    "print(f\"Created streaming sample with {stream_df.count()} records\")\n",
    "print(\"Class distribution:\")\n",
    "stream_df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Batches\n",
    "\n",
    "We'll split the data into small batches to simulate a streaming scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into batches\n",
    "def create_batches(df, batch_size=10):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into batches of specified size.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to split\n",
    "        batch_size: Number of records per batch\n",
    "        \n",
    "    Returns:\n",
    "        List of DataFrames, each containing a batch of records\n",
    "    \"\"\"\n",
    "    # Get total number of records\n",
    "    total_records = df.count()\n",
    "    \n",
    "    # Calculate number of batches\n",
    "    num_batches = (total_records + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Create batches\n",
    "    batches = []\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, total_records)\n",
    "        \n",
    "        # Use limit and offset to get a batch\n",
    "        batch_df = df.limit(end_idx).subtract(df.limit(start_idx))\n",
    "        batches.append(batch_df)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "# Create batches\n",
    "batch_size = 10  # Small batch size for demonstration\n",
    "batches = create_batches(stream_df, batch_size)\n",
    "\n",
    "print(f\"Created {len(batches)} batches of size {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Simulation Function\n",
    "\n",
    "We'll define a function to simulate streaming by processing batches with a delay between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate streaming\n",
    "def simulate_streaming(batches, model, delay_seconds=2):\n",
    "    \"\"\"\n",
    "    Simulate streaming by processing batches with a delay.\n",
    "    \n",
    "    Args:\n",
    "        batches: List of DataFrames, each containing a batch of records\n",
    "        model: PySpark ML model for prediction\n",
    "        delay_seconds: Delay between batches in seconds\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all results\n",
    "    \"\"\"\n",
    "    # Initialize results list\n",
    "    results = []\n",
    "    \n",
    "    # Process each batch\n",
    "    for i, batch_df in enumerate(batches):\n",
    "        print(f\"Processing batch {i+1}/{len(batches)}...\")\n",
    "        \n",
    "        # Add timestamp\n",
    "        batch_df = batch_df.withColumn(\"timestamp\", current_timestamp())\n",
    "        \n",
    "        # Save batch to stream data directory\n",
    "        batch_path = f\"{stream_data_path}/batch_{i}.parquet\"\n",
    "        batch_df.write.mode(\"overwrite\").parquet(batch_path)\n",
    "        \n",
    "        # Apply model to get predictions\n",
    "        try:\n",
    "            predictions = model.transform(batch_df)\n",
    "            \n",
    "            # Select relevant columns\n",
    "            result_df = predictions.select(\n",
    "                \"id\", \"title\", \"text\", \"label\", \"prediction\", \"timestamp\"\n",
    "            )\n",
    "            \n",
    "            # Save batch results\n",
    "            result_path = f\"{stream_results_path}/batch_{i}_results.parquet\"\n",
    "            result_df.write.mode(\"overwrite\").parquet(result_path)\n",
    "            \n",
    "            # Add to results list\n",
    "            results.append(result_df)\n",
    "            \n",
    "            # Display batch results\n",
    "            print(f\"Batch {i+1} results:\")\n",
    "            display(result_df.select(\"title\", \"prediction\", \"timestamp\"))\n",
    "            \n",
    "            # Update streaming metrics\n",
    "            batch_metrics = result_df.groupBy(\"prediction\").count().collect()\n",
    "            fake_count = next((row[\"count\"] for row in batch_metrics if row[\"prediction\"] == 0.0), 0)\n",
    "            real_count = next((row[\"count\"] for row in batch_metrics if row[\"prediction\"] == 1.0), 0)\n",
    "            print(f\"Batch {i+1} metrics: {fake_count} fake news, {real_count} real news\")\n",
    "            \n",
    "            # Wait before next batch\n",
    "            if i < len(batches) - 1:\n",
    "                print(f\"Waiting {delay_seconds} seconds before next batch...\")\n",
    "                time.sleep(delay_seconds)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i+1}: {e}\")\n",
    "    \n",
    "    # Combine all results\n",
    "    if results:\n",
    "        combined_results = results[0]\n",
    "        for df in results[1:]:\n",
    "            combined_results = combined_results.union(df)\n",
    "        return combined_results\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Streaming Simulation\n",
    "\n",
    "Now we'll run the streaming simulation and collect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Run streaming simulation\n",
    "try:\n",
    "    # Use a subset of batches for demonstration\n",
    "    demo_batches = batches[:10]  # Use first 10 batches\n",
    "    \n",
    "    # Run simulation\n",
    "    results_df = simulate_streaming(demo_batches, model, delay_seconds=2)\n",
    "    \n",
    "    # Cache results for faster processing\n",
    "    if results_df is not None:\n",
    "        results_df.cache()\n",
    "        print(f\"\\nStreaming simulation completed in {time.time() - start_time:.2f} seconds\")\n",
    "        print(f\"Processed {results_df.count()} records\")\n",
    "    else:\n",
    "        print(\"\\nNo results returned from streaming simulation\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in streaming simulation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Aggregated Results\n",
    "\n",
    "We'll save the aggregated results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated results\n",
    "if 'results_df' in locals() and results_df is not None:\n",
    "    # Save as parquet\n",
    "    aggregated_path = f\"{stream_results_path}/aggregated_results.parquet\"\n",
    "    results_df.write.mode(\"overwrite\").parquet(aggregated_path)\n",
    "    print(f\"Aggregated results saved to {aggregated_path}\")\n",
    "    \n",
    "    # Save as table\n",
    "    results_df.write.mode(\"overwrite\").saveAsTable(\"stream_results\")\n",
    "    print(\"Aggregated results saved as table 'stream_results'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results with Databricks-Native Visualizations\n",
    "\n",
    "We'll analyze the results using Databricks-native visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "if 'results_df' in locals() and results_df is not None:\n",
    "    # Class distribution\n",
    "    print(\"\\nPrediction Distribution:\")\n",
    "    prediction_dist = results_df.groupBy(\"prediction\").count().orderBy(\"prediction\")\n",
    "    display(prediction_dist)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    confusion_matrix = results_df.groupBy(\"label\").pivot(\"prediction\").count().fillna(0)\n",
    "    display(confusion_matrix)\n",
    "    \n",
    "    # Accuracy calculation\n",
    "    correct_predictions = results_df.filter(col(\"label\") == col(\"prediction\")).count()\n",
    "    total_predictions = results_df.count()\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "\n",
    "We'll analyze how predictions change over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis\n",
    "if 'results_df' in locals() and results_df is not None:\n",
    "    # Group by minute and count predictions\n",
    "    print(\"\\nPredictions Over Time:\")\n",
    "    time_series = results_df \\\n",
    "        .withColumn(\"minute\", expr(\"date_trunc('minute', timestamp)\")) \\\n",
    "        .groupBy(\"minute\") \\\n",
    "        .pivot(\"prediction\", [0.0, 1.0]) \\\n",
    "        .count() \\\n",
    "        .orderBy(\"minute\") \\\n",
    "        .fillna(0)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    if \"0.0\" in time_series.columns:\n",
    "        time_series = time_series.withColumnRenamed(\"0.0\", \"fake_news\")\n",
    "    else:\n",
    "        time_series = time_series.withColumn(\"fake_news\", lit(0))\n",
    "        \n",
    "    if \"1.0\" in time_series.columns:\n",
    "        time_series = time_series.withColumnRenamed(\"1.0\", \"real_news\")\n",
    "    else:\n",
    "        time_series = time_series.withColumn(\"real_news\", lit(0))\n",
    "    \n",
    "    # Display time series\n",
    "    display(time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Visualizations with Plotly\n",
    "\n",
    "We'll create interactive visualizations using Plotly for a more engaging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install plotly if not already installed\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    print(\"Plotly is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Installing plotly...\")\n",
    "    !pip install plotly\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive visualizations with Plotly\n",
    "if 'results_df' in locals() and results_df is not None:\n",
    "    # Convert to pandas for Plotly\n",
    "    prediction_dist_pd = prediction_dist.toPandas()\n",
    "    prediction_dist_pd['prediction'] = prediction_dist_pd['prediction'].map({0.0: 'Fake News', 1.0: 'Real News'})\n",
    "    \n",
    "    # Create pie chart\n",
    "    fig = px.pie(\n",
    "        prediction_dist_pd, \n",
    "        values='count', \n",
    "        names='prediction',\n",
    "        title='Prediction Distribution',\n",
    "        color='prediction',\n",
    "        color_discrete_map={'Fake News': '#FF6B6B', 'Real News': '#4ECDC4'},\n",
    "        hole=0.4\n",
    "    )\n",
    "    \n",
    "    # Add annotations\n",
    "    fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "    fig.update_layout(\n",
    "        title_font_size=24,\n",
    "        legend_title_font_size=18,\n",
    "        legend_font_size=16\n",
    "    )\n",
    "    \n",
    "    # Display the figure\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series visualization with Plotly\n",
    "if 'results_df' in locals() and results_df is not None and 'time_series' in locals():\n",
    "    # Convert to pandas for Plotly\n",
    "    time_series_pd = time_series.toPandas()\n",
    "    \n",
    "    # Create line chart\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add traces\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_series_pd['minute'],\n",
    "        y=time_series_pd['fake_news'],\n",
    "        mode='lines+markers',\n",
    "        name='Fake News',\n",
    "        line=dict(color='#FF6B6B', width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_series_pd['minute'],\n",
    "        y=time_series_pd['real_news'],\n",
    "        mode='lines+markers',\n",
    "        name='Real News',\n",
    "        line=dict(color='#4ECDC4', width=3),\n",
    "        marker=dict(size=8)\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Predictions Over Time',\n",
    "        title_font_size=24,\n",
    "        xaxis_title='Time',\n",
    "        yaxis_title='Number of Articles',\n",
    "        legend_title='Prediction',\n",
    "        legend_title_font_size=18,\n",
    "        legend_font_size=16,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    # Display the figure\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Dashboard with Custom HTML\n",
    "\n",
    "We'll create an interactive dashboard using custom HTML and JavaScript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive dashboard with custom HTML\n",
    "if 'results_df' in locals() and results_df is not None:\n",
    "    # Get metrics for dashboard\n",
    "    total_records = results_df.count()\n",
    "    fake_count = results_df.filter(col(\"prediction\") == 0.0).count()\n",
    "    real_count = results_df.filter(col(\"prediction\") == 1.0).count()\n",
    "    fake_percentage = fake_count / total_records * 100 if total_records > 0 else 0\n",
    "    real_percentage = real_count / total_records * 100 if total_records > 0 else 0\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Create metrics JSON\n",
    "    metrics = {\n",
    "        \"total_records\": total_records,\n",
    "        \"fake_count\": fake_count,\n",
    "        \"real_count\": real_count,\n",
    "        \"fake_percentage\": fake_percentage,\n",
    "        \"real_percentage\": real_percentage,\n",
    "        \"execution_time\": execution_time\n",
    "    }\n",
    "    \n",
    "    # Create HTML for dashboard\n",
    "    html = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Fake News Detection Dashboard</title>\n",
    "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }}\n",
    "            .dashboard {{ display: flex; flex-wrap: wrap; gap: 20px; }}\n",
    "            .card {{ background-color: white; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); padding: 20px; }}\n",
    "            .metric-card {{ width: 200px; text-align: center; }}\n",
    "            .chart-card {{ width: 100%; max-width: 600px; }}\n",
    "            .metric-value {{ font-size: 36px; font-weight: bold; margin: 10px 0; }}\n",
    "            .metric-label {{ font-size: 16px; color: #666; }}\n",
    "            .fake {{ color: #FF6B6B; }}\n",
    "            .real {{ color: #4ECDC4; }}\n",
    "            h1 {{ color: #333; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Fake News Detection - Streaming Dashboard</h1>\n",
    "        <div class=\"dashboard\">\n",
    "            <div class=\"card metric-card\">\n",
    "                <div class=\"metric-label\">Total Articles</div>\n",
    "                <div class=\"metric-value\">{metrics['total_records']}</div>\n",
    "            </div>\n",
    "            <div class=\"card metric-card\">\n",
    "                <div class=\"metric-label\">Fake News</div>\n",
    "                <div class=\"metric-value fake\">{metrics['fake_count']}</div>\n",
    "                <div>({metrics['fake_percentage']:.1f}%)</div>\n",
    "            </div>\n",
    "            <div class=\"card metric-card\">\n",
    "                <div class=\"metric-label\">Real News</div>\n",
    "                <div class=\"metric-value real\">{metrics['real_count']}</div>\n",
    "                <div>({metrics['real_percentage']:.1f}%)</div>\n",
    "            </div>\n",
    "            <div class=\"card metric-card\">\n",
    "                <div class=\"metric-label\">Processing Time</div>\n",
    "                <div class=\"metric-value\">{metrics['execution_time']:.2f}s</div>\n",
    "            </div>\n",
    "            <div class=\"card chart-card\">\n",
    "                <canvas id=\"distributionChart\"></canvas>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <script>\n",
    "            // Create distribution chart\n",
    "            const ctx = document.getElementById('distributionChart').getContext('2d');\n",
    "            const chart = new Chart(ctx, {{\n",
    "                type: 'doughnut',\n",
    "                data: {{\n",
    "                    labels: ['Fake News', 'Real News'],\n",
    "                    datasets: [{{\n",
    "                        data: [{metrics['fake_count']}, {metrics['real_count']}],\n",
    "                        backgroundColor: ['#FF6B6B', '#4ECDC4'],\n",
    "                        borderColor: ['#FF6B6B', '#4ECDC4'],\n",
    "                        borderWidth: 1\n",
    "                    }}]\n",
    "                }},\n",
    "                options: {{\n",
    "                    responsive: true,\n",
    "                    plugins: {{\n",
    "                        legend: {{\n",
    "                            position: 'top',\n",
    "                        }},\n",
    "                        title: {{\n",
    "                            display: true,\n",
    "                            text: 'Prediction Distribution'\n",
    "                        }}\n",
    "                    }}\n",
    "                }}\n",
    "            }});\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display the dashboard\n",
    "    displayHTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Results with Spark SQL\n",
    "\n",
    "We'll query the results using Spark SQL for additional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query results with Spark SQL\n",
    "if 'results_df' in locals() and results_df is not None:\n",
    "    # Create temporary view\n",
    "    results_df.createOrReplaceTempView(\"streaming_results\")\n",
    "    \n",
    "    # Query prediction distribution\n",
    "    print(\"\\nPrediction Distribution (SQL):\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            CASE \n",
    "                WHEN prediction = 0 THEN 'Fake News'\n",
    "                WHEN prediction = 1 THEN 'Real News'\n",
    "                ELSE 'Unknown'\n",
    "            END AS prediction_type,\n",
    "            COUNT(*) as count,\n",
    "            ROUND(COUNT(*) * 100 / (SELECT COUNT(*) FROM streaming_results), 2) as percentage\n",
    "        FROM streaming_results\n",
    "        GROUP BY prediction\n",
    "        ORDER BY prediction\n",
    "    \"\"\").show()\n",
    "    \n",
    "    # Query accuracy\n",
    "    print(\"\\nAccuracy by Batch (SQL):\")\n",
    "    spark.sql(\"\"\"\n",
    "        WITH batch_metrics AS (\n",
    "            SELECT \n",
    "                MINUTE(timestamp) as batch_minute,\n",
    "                COUNT(*) as total,\n",
    "                SUM(CASE WHEN label = prediction THEN 1 ELSE 0 END) as correct\n",
    "            FROM streaming_results\n",
    "            GROUP BY MINUTE(timestamp)\n",
    "        )\n",
    "        SELECT \n",
    "            batch_minute,\n",
    "            total,\n",
    "            correct,\n",
    "            ROUND(correct * 100 / total, 2) as accuracy_percentage\n",
    "        FROM batch_metrics\n",
    "        ORDER BY batch_minute\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Streaming Metrics\n",
    "\n",
    "We'll save the streaming metrics for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save streaming metrics\n",
    "if 'results_df' in locals() and results_df is not None:\n",
    "    # Create metrics DataFrame\n",
    "    metrics_data = [\n",
    "        (\"total_records\", total_records),\n",
    "        (\"fake_count\", fake_count),\n",
    "        (\"real_count\", real_count),\n",
    "        (\"fake_percentage\", fake_percentage),\n",
    "        (\"real_percentage\", real_percentage),\n",
    "        (\"execution_time\", execution_time),\n",
    "        (\"accuracy\", accuracy)\n",
    "    ]\n",
    "    \n",
    "    metrics_schema = [\"metric_name\", \"metric_value\"]\n",
    "    metrics_df = spark.createDataFrame(metrics_data, metrics_schema)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_path = f\"{logs_path}/streaming_metrics.parquet\"\n",
    "    metrics_df.write.mode(\"overwrite\").parquet(metrics_path)\n",
    "    print(f\"Streaming metrics saved to {metrics_path}\")\n",
    "    \n",
    "    # Display metrics\n",
    "    print(\"\\nStreaming Metrics:\")\n",
    "    display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've implemented a streaming simulation for fake news detection using PySpark MLlib and Databricks-native functionality. We've demonstrated how to:\n",
    "\n",
    "1. Load a pre-trained PySpark ML model\n",
    "2. Create sample data for streaming simulation\n",
    "3. Process data in batches to emulate streaming\n",
    "4. Apply the model to each batch\n",
    "5. Save and analyze the results\n",
    "6. Create interactive visualizations\n",
    "\n",
    "This approach can be extended to handle real streaming data in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Implement real-time alerting for high-confidence fake news\n",
    "2. Create a Databricks dashboard for monitoring streaming results\n",
    "3. Integrate with external systems for automated response\n",
    "4. Implement model retraining based on streaming feedback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}