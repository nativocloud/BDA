{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1479a1d4",
   "metadata": {},
   "source": [
    "# Fake News Detection: Streaming Analysis\n",
    "\n",
    "This notebook contains all the necessary code for streaming analysis in the fake news detection project. The code is organized into independent functions, without dependencies on external modules or classes, to facilitate execution in Databricks Community Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f9bef4",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, from_json, window, count, when\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, FloatType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cebde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session optimized for Databricks Community Edition\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection_StreamingAnalysis\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37cb73",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Start timer for performance tracking\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b4fe8",
   "metadata": {},
   "source": [
    "## Reusable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b72df7",
   "metadata": {},
   "source": [
    "### Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3faf1ae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    Load a trained machine learning model from disk.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model file\n",
    "        \n",
    "    Returns:\n",
    "        object: Loaded model\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "        return model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2323bd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_vectorizer(vectorizer_path):\n",
    "    \"\"\"\n",
    "    Load a trained TF-IDF vectorizer from disk.\n",
    "    \n",
    "    Args:\n",
    "        vectorizer_path (str): Path to the saved vectorizer file\n",
    "        \n",
    "    Returns:\n",
    "        object: Loaded vectorizer\n",
    "    \"\"\"\n",
    "    print(f\"Loading vectorizer from {vectorizer_path}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(vectorizer_path, \"rb\") as f:\n",
    "            vectorizer = pickle.load(f)\n",
    "        print(f\"Vectorizer loaded successfully from {vectorizer_path}\")\n",
    "        return vectorizer\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading vectorizer: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e601a1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_streaming_data(data_path):\n",
    "    \"\"\"\n",
    "    Load streaming data from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the streaming data file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame with streaming data\n",
    "    \"\"\"\n",
    "    print(f\"Loading streaming data from {data_path}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"Loaded {len(df)} records from {data_path}\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading streaming data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831aa50",
   "metadata": {},
   "source": [
    "### Text Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755ebc5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_text(df, text_column=\"text\", title_column=None):\n",
    "    \"\"\"\n",
    "    Preprocess text data for streaming analysis.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Pandas DataFrame with text data\n",
    "        text_column (str): Name of the text column\n",
    "        title_column (str): Name of the title column (optional)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with preprocessed text\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing text data...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying the original\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Fill NaN values\n",
    "    processed_df[text_column] = processed_df[text_column].fillna('')\n",
    "    \n",
    "    # Process title if available\n",
    "    if title_column and title_column in processed_df.columns:\n",
    "        processed_df[title_column] = processed_df[title_column].fillna('')\n",
    "        # Combine title and text for better context\n",
    "        processed_df['content'] = processed_df[title_column] + \" \" + processed_df[text_column]\n",
    "    else:\n",
    "        processed_df['content'] = processed_df[text_column]\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    processed_df['content'] = processed_df['content'].str.lower()\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe44989",
   "metadata": {},
   "source": [
    "### Batch Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e6d0ae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_batch(batch_df, vectorizer, model, batch_id):\n",
    "    \"\"\"\n",
    "    Process a batch of streaming data.\n",
    "    \n",
    "    Args:\n",
    "        batch_df (DataFrame): Pandas DataFrame with batch data\n",
    "        vectorizer: TF-IDF vectorizer\n",
    "        model: Trained machine learning model\n",
    "        batch_id (int): Batch identifier\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with prediction results\n",
    "    \"\"\"\n",
    "    print(f\"Processing batch {batch_id}...\")\n",
    "    \n",
    "    # Preprocess text\n",
    "    processed_df = preprocess_text(batch_df)\n",
    "    \n",
    "    # Extract features\n",
    "    X = vectorizer.transform(processed_df['content'])\n",
    "    \n",
    "    # Make predictions\n",
    "    processed_df['prediction'] = model.predict(X)\n",
    "    processed_df['prediction_proba'] = model.predict_proba(X)[:, 1]\n",
    "    processed_df['prediction_time'] = pd.Timestamp.now()\n",
    "    \n",
    "    print(f\"Processed batch {batch_id} with {len(processed_df)} records\")\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08781e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_batch_results(batch_df, output_dir, batch_id):\n",
    "    \"\"\"\n",
    "    Save batch processing results to disk.\n",
    "    \n",
    "    Args:\n",
    "        batch_df (DataFrame): Pandas DataFrame with batch results\n",
    "        output_dir (str): Directory to save results\n",
    "        batch_id (int): Batch identifier\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save batch results\n",
    "    output_file = f\"{output_dir}/batch_{batch_id}_results.csv\"\n",
    "    batch_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Batch {batch_id} results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27139e20",
   "metadata": {},
   "source": [
    "### Streaming Simulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20e83a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def simulate_streaming(stream_df, vectorizer, model, batch_size=5, delay_seconds=2, \n",
    "                      stream_dir=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Simulate a streaming pipeline with batched data processing.\n",
    "    \n",
    "    Args:\n",
    "        stream_df (DataFrame): Pandas DataFrame with streaming data\n",
    "        vectorizer: TF-IDF vectorizer\n",
    "        model: Trained machine learning model\n",
    "        batch_size (int): Number of records per batch\n",
    "        delay_seconds (int): Delay between batches in seconds\n",
    "        stream_dir (str): Directory to save batch files\n",
    "        output_dir (str): Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with aggregated results\n",
    "    \"\"\"\n",
    "    print(f\"Starting streaming simulation with {len(stream_df)} records...\")\n",
    "    \n",
    "    # Create directories if provided\n",
    "    if stream_dir:\n",
    "        os.makedirs(stream_dir, exist_ok=True)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Split data into batches\n",
    "    num_batches = len(stream_df) // batch_size + (1 if len(stream_df) % batch_size > 0 else 0)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        # Get batch\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(stream_df))\n",
    "        batch = stream_df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Save batch to streaming directory if provided\n",
    "        if stream_dir:\n",
    "            batch_file = f\"{stream_dir}/batch_{i}.csv\"\n",
    "            batch.to_csv(batch_file, index=False)\n",
    "            print(f\"Saved batch {i} to {batch_file}\")\n",
    "        \n",
    "        # Process batch\n",
    "        batch_results = process_batch(batch, vectorizer, model, i)\n",
    "        \n",
    "        # Save batch results if output directory is provided\n",
    "        if output_dir:\n",
    "            save_batch_results(batch_results, output_dir, i)\n",
    "        \n",
    "        # Extract key results for aggregation\n",
    "        all_results.append(batch_results[['id', 'prediction', 'prediction_proba', 'prediction_time']])\n",
    "        \n",
    "        # Simulate delay between batches\n",
    "        if i < num_batches - 1:\n",
    "            print(f\"Waiting {delay_seconds} seconds before next batch...\")\n",
    "            time.sleep(delay_seconds)\n",
    "    \n",
    "    # Combine all results\n",
    "    combined_results = pd.concat(all_results, ignore_index=True)\n",
    "    print(f\"Streaming simulation completed with {len(combined_results)} total records processed\")\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b74d81",
   "metadata": {},
   "source": [
    "### Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b049d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_streaming_results(results_df):\n",
    "    \"\"\"\n",
    "    Analyze streaming prediction results.\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): Pandas DataFrame with prediction results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with analysis metrics\n",
    "    \"\"\"\n",
    "    print(\"Analyzing streaming results...\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_processed = len(results_df)\n",
    "    fake_count = len(results_df[results_df['prediction'] == 0])\n",
    "    real_count = len(results_df[results_df['prediction'] == 1])\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        \"total_processed\": total_processed,\n",
    "        \"fake_count\": fake_count,\n",
    "        \"real_count\": real_count,\n",
    "        \"fake_percentage\": fake_count / total_processed * 100 if total_processed > 0 else 0,\n",
    "        \"real_percentage\": real_count / total_processed * 100 if total_processed > 0 else 0,\n",
    "        \"execution_time\": time.time() - start_time\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nStreaming Results Summary:\")\n",
    "    print(f\"Total records processed: {metrics['total_processed']}\")\n",
    "    print(f\"Fake news detected: {metrics['fake_count']} ({metrics['fake_percentage']:.1f}%)\")\n",
    "    print(f\"Real news detected: {metrics['real_count']} ({metrics['real_percentage']:.1f}%)\")\n",
    "    print(f\"Execution time: {metrics['execution_time']:.2f} seconds\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7f4579",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_time_series_data(results_df, resample_freq='1S'):\n",
    "    \"\"\"\n",
    "    Create time series data from streaming results.\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): Pandas DataFrame with prediction results\n",
    "        resample_freq (str): Frequency for resampling time series data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with time series data\n",
    "    \"\"\"\n",
    "    print(f\"Creating time series data with {resample_freq} frequency...\")\n",
    "    \n",
    "    # Ensure prediction_time is datetime\n",
    "    results_df['prediction_time'] = pd.to_datetime(results_df['prediction_time'])\n",
    "    \n",
    "    # Sort by time\n",
    "    sorted_df = results_df.sort_values('prediction_time')\n",
    "    \n",
    "    # Group by time and count predictions\n",
    "    time_series = sorted_df.set_index('prediction_time').resample(resample_freq).prediction.value_counts().unstack().fillna(0)\n",
    "    \n",
    "    # Ensure all prediction values are present\n",
    "    if 0 not in time_series.columns:\n",
    "        time_series[0] = 0\n",
    "    if 1 not in time_series.columns:\n",
    "        time_series[1] = 0\n",
    "    \n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f636f2",
   "metadata": {},
   "source": [
    "### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366033dd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_prediction_distribution(results_df, output_path=None):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of predictions.\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): Pandas DataFrame with prediction results\n",
    "        output_path (str): Path to save the visualization (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x='prediction', data=results_df)\n",
    "    plt.title('Streaming Predictions')\n",
    "    plt.xlabel('Prediction (0=Fake, 1=Real)')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"Prediction distribution visualization saved to {output_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972d932",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_time_series(time_series_df, output_path=None):\n",
    "    \"\"\"\n",
    "    Visualize predictions over time.\n",
    "    \n",
    "    Args:\n",
    "        time_series_df (DataFrame): Pandas DataFrame with time series data\n",
    "        output_path (str): Path to save the visualization (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    time_series_df.plot(figsize=(12, 6))\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Fake', 'Real'])\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"Time series visualization saved to {output_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00685ed5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def visualize_probability_distribution(results_df, output_path=None):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of prediction probabilities.\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): Pandas DataFrame with prediction results\n",
    "        output_path (str): Path to save the visualization (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(results_df['prediction_proba'], bins=10)\n",
    "    plt.title('Prediction Probability Distribution')\n",
    "    plt.xlabel('Probability of being Real News')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"Probability distribution visualization saved to {output_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10545385",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_streaming_dashboard(results_df, metrics, time_series_df, output_path=None):\n",
    "    \"\"\"\n",
    "    Create a dashboard-style visualization of streaming results.\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): Pandas DataFrame with prediction results\n",
    "        metrics (dict): Dictionary with analysis metrics\n",
    "        time_series_df (DataFrame): Pandas DataFrame with time series data\n",
    "        output_path (str): Path to save the visualization (optional)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Predictions count\n",
    "    plt.subplot(2, 2, 1)\n",
    "    labels = ['Fake News', 'Real News']\n",
    "    sizes = [metrics['fake_count'], metrics['real_count']]\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff'])\n",
    "    plt.axis('equal')\n",
    "    plt.title('Prediction Distribution')\n",
    "    \n",
    "    # Prediction probabilities histogram\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.histplot(results_df['prediction_proba'], bins=10, ax=plt.gca())\n",
    "    plt.title('Prediction Probability Distribution')\n",
    "    plt.xlabel('Probability of being Real News')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Time series\n",
    "    plt.subplot(2, 2, 3)\n",
    "    time_series_df.plot(ax=plt.gca())\n",
    "    plt.title('Predictions Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Fake', 'Real'])\n",
    "    \n",
    "    # Summary text\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    Streaming Pipeline Summary:\n",
    "    ---------------------------\n",
    "    Total records processed: {metrics['total_processed']}\n",
    "    Fake news detected: {metrics['fake_count']} ({metrics['fake_percentage']:.1f}%)\n",
    "    Real news detected: {metrics['real_count']} ({metrics['real_percentage']:.1f}%)\n",
    "    Execution time: {metrics['execution_time']:.2f} seconds\n",
    "    \"\"\"\n",
    "    plt.text(0.1, 0.5, summary_text, fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "        print(f\"Streaming dashboard saved to {output_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5199b6a",
   "metadata": {},
   "source": [
    "### Data Storage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00683cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_streaming_results(results_df, output_path):\n",
    "    \"\"\"\n",
    "    Save streaming results to disk.\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): Pandas DataFrame with prediction results\n",
    "        output_path (str): Path to save the results\n",
    "    \"\"\"\n",
    "    print(f\"Saving streaming results to {output_path}...\")\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Streaming results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f670b1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_streaming_metrics(metrics, output_path):\n",
    "    \"\"\"\n",
    "    Save streaming metrics to disk.\n",
    "    \n",
    "    Args:\n",
    "        metrics (dict): Dictionary with analysis metrics\n",
    "        output_path (str): Path to save the metrics\n",
    "    \"\"\"\n",
    "    print(f\"Saving streaming metrics to {output_path}...\")\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save metrics\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"Streaming metrics saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b664817e",
   "metadata": {},
   "source": [
    "### Spark Streaming Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151a2a89",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_spark_streaming_schema():\n",
    "    \"\"\"\n",
    "    Create schema for Spark Streaming data.\n",
    "    \n",
    "    Returns:\n",
    "        StructType: Spark schema for streaming data\n",
    "    \"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"id\", IntegerType(), False),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"label\", IntegerType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a8e68",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def setup_spark_streaming(spark, input_path, schema, processing_time=\"5 seconds\"):\n",
    "    \"\"\"\n",
    "    Set up a Spark Streaming context for processing streaming data.\n",
    "    \n",
    "    Args:\n",
    "        spark: Spark session\n",
    "        input_path (str): Path to the streaming data directory\n",
    "        schema: Schema for the streaming data\n",
    "        processing_time (str): Processing time interval\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Spark streaming DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"Setting up Spark Streaming from {input_path}...\")\n",
    "    \n",
    "    # Create streaming DataFrame\n",
    "    streaming_df = spark \\\n",
    "        .readStream \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"maxFilesPerTrigger\", 1) \\\n",
    "        .option(\"latestFirst\", \"true\") \\\n",
    "        .json(input_path) \\\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\")\n",
    "    \n",
    "    return streaming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96b7fd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_spark_streaming(streaming_df, output_path, query_name=\"streaming_query\"):\n",
    "    \"\"\"\n",
    "    Process streaming data with Spark Streaming.\n",
    "    \n",
    "    Args:\n",
    "        streaming_df: Spark streaming DataFrame\n",
    "        output_path (str): Path to save the streaming results\n",
    "        query_name (str): Name of the streaming query\n",
    "        \n",
    "    Returns:\n",
    "        StreamingQuery: Spark streaming query\n",
    "    \"\"\"\n",
    "    print(f\"Processing streaming data to {output_path}...\")\n",
    "    \n",
    "    # Define processing logic\n",
    "    query = streaming_df \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"json\") \\\n",
    "        .option(\"path\", output_path) \\\n",
    "        .option(\"checkpointLocation\", f\"{output_path}/checkpoint\") \\\n",
    "        .queryName(query_name) \\\n",
    "        .start()\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04ef4a",
   "metadata": {},
   "source": [
    "## Complete Streaming Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5eaac3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_streaming_pipeline(\n",
    "    model_path=\"dbfs:/FileStore/fake_news_detection/models/rf_model.pkl\",\n",
    "    vectorizer_path=\"dbfs:/FileStore/fake_news_detection/models/tfidf_vectorizer.pkl\",\n",
    "    data_path=\"dbfs:/FileStore/fake_news_detection/data/stream_sample.csv\",\n",
    "    stream_dir=\"dbfs:/FileStore/fake_news_detection/data/streaming\",\n",
    "    output_dir=\"dbfs:/FileStore/fake_news_detection/data/streaming_output\",\n",
    "    results_dir=\"dbfs:/FileStore/fake_news_detection/results\",\n",
    "    batch_size=5,\n",
    "    delay_seconds=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the complete streaming pipeline for fake news detection.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the trained model\n",
    "        vectorizer_path (str): Path to the trained vectorizer\n",
    "        data_path (str): Path to the streaming data\n",
    "        stream_dir (str): Directory to save batch files\n",
    "        output_dir (str): Directory to save batch results\n",
    "        results_dir (str): Directory to save analysis results\n",
    "        batch_size (int): Number of records per batch\n",
    "        delay_seconds (int): Delay between batches in seconds\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with references to results\n",
    "    \"\"\"\n",
    "    print(\"Starting streaming pipeline...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create directories\n",
    "    for directory in [stream_dir, output_dir, results_dir]:\n",
    "        try:\n",
    "            # For Databricks\n",
    "            dbutils.fs.mkdirs(directory.replace(\"dbfs:\", \"\"))\n",
    "        except:\n",
    "            # For local environment\n",
    "            local_dir = directory.replace(\"dbfs:/\", \"/tmp/\")\n",
    "            os.makedirs(local_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Load model and vectorizer\n",
    "    model = load_model(model_path.replace(\"dbfs:/\", \"/dbfs/\"))\n",
    "    vectorizer = load_vectorizer(vectorizer_path.replace(\"dbfs:/\", \"/dbfs/\"))\n",
    "    \n",
    "    if model is None or vectorizer is None:\n",
    "        print(\"Error: Could not load model or vectorizer. Pipeline aborted.\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Load streaming data\n",
    "    stream_df = load_streaming_data(data_path.replace(\"dbfs:/\", \"/dbfs/\"))\n",
    "    \n",
    "    if stream_df is None:\n",
    "        print(\"Error: Could not load streaming data. Pipeline aborted.\")\n",
    "        return None\n",
    "    \n",
    "    # 3. Run streaming simulation\n",
    "    local_stream_dir = stream_dir.replace(\"dbfs:/\", \"/tmp/\")\n",
    "    local_output_dir = output_dir.replace(\"dbfs:/\", \"/tmp/\")\n",
    "    \n",
    "    results = simulate_streaming(\n",
    "        stream_df, \n",
    "        vectorizer, \n",
    "        model, \n",
    "        batch_size=batch_size, \n",
    "        delay_seconds=delay_seconds,\n",
    "        stream_dir=local_stream_dir,\n",
    "        output_dir=local_output_dir\n",
    "    )\n",
    "    \n",
    "    # 4. Analyze results\n",
    "    metrics = analyze_streaming_results(results)\n",
    "    \n",
    "    # 5. Create time series data\n",
    "    time_series = create_time_series_data(results)\n",
    "    \n",
    "    # 6. Create visualizations\n",
    "    local_results_dir = results_dir.replace(\"dbfs:/\", \"/tmp/\")\n",
    "    \n",
    "    visualize_prediction_distribution(\n",
    "        results, \n",
    "        output_path=f\"{local_results_dir}/streaming_predictions.png\"\n",
    "    )\n",
    "    \n",
    "    visualize_time_series(\n",
    "        time_series, \n",
    "        output_path=f\"{local_results_dir}/streaming_time_series.png\"\n",
    "    )\n",
    "    \n",
    "    visualize_probability_distribution(\n",
    "        results, \n",
    "        output_path=f\"{local_results_dir}/streaming_probabilities.png\"\n",
    "    )\n",
    "    \n",
    "    create_streaming_dashboard(\n",
    "        results, \n",
    "        metrics, \n",
    "        time_series, \n",
    "        output_path=f\"{local_results_dir}/streaming_dashboard.png\"\n",
    "    )\n",
    "    \n",
    "    # 7. Save results\n",
    "    save_streaming_results(\n",
    "        results, \n",
    "        output_path=f\"{local_output_dir}/aggregated_results.csv\"\n",
    "    )\n",
    "    \n",
    "    save_streaming_metrics(\n",
    "        metrics, \n",
    "        output_path=f\"{local_results_dir}/streaming_metrics.json\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nStreaming pipeline completed in {time.time() - start_time:.2f} seconds!\")\n",
    "    print(f\"Results saved to {output_dir}\")\n",
    "    print(f\"Visualizations saved to {results_dir}\")\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"metrics\": metrics,\n",
    "        \"time_series\": time_series\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9048575e",
   "metadata": {},
   "source": [
    "## Step-by-Step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489b06f",
   "metadata": {},
   "source": [
    "### 1. Load Model and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80cddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "model_path = \"/tmp/models/rf_model.pkl\"\n",
    "vectorizer_path = \"/tmp/models/tfidf_vectorizer.pkl\"\n",
    "data_path = \"/tmp/data/stream_sample.csv\"\n",
    "stream_dir = \"/tmp/data/streaming\"\n",
    "output_dir = \"/tmp/data/streaming_output\"\n",
    "results_dir = \"/tmp/results\"\n",
    "\n",
    "# Create directories\n",
    "for directory in [\"/tmp/models\", \"/tmp/data\", stream_dir, output_dir, results_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Load model and vectorizer (if available)\n",
    "model = None\n",
    "vectorizer = None\n",
    "\n",
    "try:\n",
    "    model = load_model(model_path)\n",
    "    vectorizer = load_vectorizer(vectorizer_path)\n",
    "except:\n",
    "    print(\"Model or vectorizer not found. For demonstration, we'll create simple ones.\")\n",
    "    \n",
    "    # Create a simple model and vectorizer for demonstration\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    # Sample data for training\n",
    "    texts = [\n",
    "        \"This is fake news about politics\",\n",
    "        \"Real news about economy\",\n",
    "        \"Fake story about celebrities\",\n",
    "        \"True report on science\"\n",
    "    ]\n",
    "    labels = [0, 1, 0, 1]  # 0 = fake, 1 = real\n",
    "    \n",
    "    # Create and fit vectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=100)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Create and fit model\n",
    "    model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    model.fit(X, labels)\n",
    "    \n",
    "    # Save model and vectorizer\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    with open(vectorizer_path, \"wb\") as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "    \n",
    "    print(\"Created and saved simple model and vectorizer for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188cce1e",
   "metadata": {},
   "source": [
    "### 2. Create Sample Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1631110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample streaming data if not available\n",
    "try:\n",
    "    stream_df = load_streaming_data(data_path)\n",
    "    if stream_df is None:\n",
    "        raise FileNotFoundError(\"Sample data not found\")\n",
    "except:\n",
    "    print(\"Creating sample streaming data...\")\n",
    "    \n",
    "    # Create sample data\n",
    "    sample_data = {\n",
    "        \"id\": list(range(20)),\n",
    "        \"title\": [\n",
    "            \"Breaking News: Political Scandal\",\n",
    "            \"Economy Shows Signs of Recovery\",\n",
    "            \"Celebrity Caught in Controversy\",\n",
    "            \"New Scientific Discovery\",\n",
    "            \"Government Announces New Policy\",\n",
    "            \"Stock Market Hits Record High\",\n",
    "            \"Famous Actor in New Movie\",\n",
    "            \"Health Study Reveals Surprising Results\",\n",
    "            \"Sports Team Wins Championship\",\n",
    "            \"Technology Company Launches Product\",\n",
    "            \"Weather Forecast Predicts Storm\",\n",
    "            \"Education Reform Proposed\",\n",
    "            \"Environmental Crisis Worsens\",\n",
    "            \"International Relations Strained\",\n",
    "            \"Local Community Event Success\",\n",
    "            \"Transportation System Upgrade\",\n",
    "            \"Cultural Festival Announced\",\n",
    "            \"Financial Markets Volatile\",\n",
    "            \"Medical Breakthrough Reported\",\n",
    "            \"Social Media Platform Changes\"\n",
    "        ],\n",
    "        \"text\": [\n",
    "            \"Politicians involved in major scandal with evidence of corruption.\",\n",
    "            \"Economic indicators show positive trends in job growth and consumer spending.\",\n",
    "            \"Famous celebrity caught in scandal with questionable evidence.\",\n",
    "            \"Scientists discover new species with potential medical applications.\",\n",
    "            \"Government announces controversial policy that divides public opinion.\",\n",
    "            \"Stock market reaches unprecedented levels despite economic concerns.\",\n",
    "            \"A-list actor stars in upcoming blockbuster movie with record budget.\",\n",
    "            \"New health study contradicts previous findings on popular supplements.\",\n",
    "            \"Underdog team wins championship in surprising upset victory.\",\n",
    "            \"Major tech company releases innovative product to mixed reviews.\",\n",
    "            \"Meteorologists predict severe weather conditions for the weekend.\",\n",
    "            \"Lawmakers propose significant changes to education system.\",\n",
    "            \"Environmental scientists report accelerating climate change effects.\",\n",
    "            \"Diplomatic tensions rise between countries after controversial statement.\",\n",
    "            \"Local community event raises record funds for charity.\",\n",
    "            \"City approves major upgrade to public transportation infrastructure.\",\n",
    "            \"Annual cultural festival announces lineup of international artists.\",\n",
    "            \"Financial markets experience significant fluctuations due to uncertainty.\",\n",
    "            \"Researchers announce promising results in treatment development.\",\n",
    "            \"Popular social media platform implements controversial policy changes.\"\n",
    "        ],\n",
    "        \"label\": [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]  # 0 = fake, 1 = real\n",
    "    }\n",
    "    \n",
    "    stream_df = pd.DataFrame(sample_data)\n",
    "    \n",
    "    # Save sample data\n",
    "    stream_df.to_csv(data_path, index=False)\n",
    "    \n",
    "    print(f\"Created and saved sample streaming data with {len(stream_df)} records\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample streaming data:\")\n",
    "print(stream_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdee3ee",
   "metadata": {},
   "source": [
    "### 3. Process a Single Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a single batch\n",
    "if stream_df is not None and model is not None and vectorizer is not None:\n",
    "    # Get first batch\n",
    "    batch_size = 5\n",
    "    first_batch = stream_df.iloc[:batch_size].copy()\n",
    "    \n",
    "    # Process batch\n",
    "    batch_results = process_batch(first_batch, vectorizer, model, batch_id=0)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nBatch processing results:\")\n",
    "    print(batch_results[['id', 'title', 'prediction', 'prediction_proba']].head())\n",
    "    \n",
    "    # Save batch results\n",
    "    save_batch_results(batch_results, output_dir, batch_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69eb751",
   "metadata": {},
   "source": [
    "### 4. Run Streaming Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run streaming simulation with small batch size and minimal delay\n",
    "if stream_df is not None and model is not None and vectorizer is not None:\n",
    "    # Run simulation\n",
    "    results = simulate_streaming(\n",
    "        stream_df, \n",
    "        vectorizer, \n",
    "        model, \n",
    "        batch_size=5, \n",
    "        delay_seconds=1,  # Short delay for demonstration\n",
    "        stream_dir=stream_dir,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nStreaming simulation results:\")\n",
    "    print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef0175",
   "metadata": {},
   "source": [
    "### 5. Analyze Streaming Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ed0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze streaming results\n",
    "if 'results' in locals():\n",
    "    # Calculate metrics\n",
    "    metrics = analyze_streaming_results(results)\n",
    "    \n",
    "    # Create time series data\n",
    "    time_series = create_time_series_data(results)\n",
    "    \n",
    "    # Display time series data\n",
    "    print(\"\\nTime series data:\")\n",
    "    print(time_series.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113412db",
   "metadata": {},
   "source": [
    "### 6. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa56796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "if 'results' in locals() and 'metrics' in locals() and 'time_series' in locals():\n",
    "    # Prediction distribution\n",
    "    visualize_prediction_distribution(results)\n",
    "    \n",
    "    # Time series\n",
    "    visualize_time_series(time_series)\n",
    "    \n",
    "    # Probability distribution\n",
    "    visualize_probability_distribution(results)\n",
    "    \n",
    "    # Dashboard\n",
    "    create_streaming_dashboard(results, metrics, time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62012c7d",
   "metadata": {},
   "source": [
    "### 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cd5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "if 'results' in locals() and 'metrics' in locals():\n",
    "    # Save streaming results\n",
    "    save_streaming_results(results, f\"{output_dir}/aggregated_results.csv\")\n",
    "    \n",
    "    # Save metrics\n",
    "    save_streaming_metrics(metrics, f\"{results_dir}/streaming_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e5e31",
   "metadata": {},
   "source": [
    "### 8. Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete streaming pipeline\n",
    "pipeline_results = run_streaming_pipeline(\n",
    "    model_path=model_path,\n",
    "    vectorizer_path=vectorizer_path,\n",
    "    data_path=data_path,\n",
    "    stream_dir=stream_dir,\n",
    "    output_dir=output_dir,\n",
    "    results_dir=results_dir,\n",
    "    batch_size=5,\n",
    "    delay_seconds=1  # Short delay for demonstration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8807f",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "1. **Streaming Purpose**: Streaming analysis enables real-time detection of fake news as it appears, allowing for immediate response and mitigation.\n",
    "\n",
    "2. **Batch Processing**: The implementation uses a batch processing approach to simulate streaming, which is well-suited for Databricks Community Edition.\n",
    "\n",
    "3. **Model Loading**: Pre-trained models are loaded from disk, allowing the streaming pipeline to leverage previously trained classifiers.\n",
    "\n",
    "4. **Visualization**: Real-time visualizations help monitor the stream of news and track trends in fake news detection.\n",
    "\n",
    "5. **Time Series Analysis**: Tracking predictions over time helps identify patterns and potential coordinated fake news campaigns.\n",
    "\n",
    "6. **Performance Considerations**: For production environments, consider:\n",
    "   - Using Spark Structured Streaming for true streaming capabilities\n",
    "   - Implementing checkpointing for fault tolerance\n",
    "   - Optimizing batch size based on available resources\n",
    "\n",
    "7. **Databricks Integration**: The code is optimized for Databricks Community Edition with appropriate configurations for memory and processing.\n",
    "\n",
    "8. **Simulation vs. Real Streaming**: This notebook demonstrates a simulation of streaming; in a production environment, you would connect to real data sources like Twitter API, RSS feeds, or news APIs."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
