{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ce0c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualization Setup for Fake News Detection\n",
    "\n",
    "This module provides visualization functionality for the fake news detection pipeline.\n",
    "It includes functions for creating dashboards, visualizing model performance metrics,\n",
    "and integrating with Grafana for real-time monitoring.\n",
    "\n",
    "Author: BDA Team\n",
    "Date: May 27, 2025\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, expr, to_json, struct\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8611870",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizationSetup:\n",
    "    \"\"\"\n",
    "    A class for setting up visualizations for the fake news detection pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir=None, config=None):\n",
    "        \"\"\"\n",
    "        Initialize the VisualizationSetup with optional configuration.\n",
    "        \n",
    "        Args:\n",
    "            output_dir (str, optional): Directory to save visualization outputs.\n",
    "                If None, defaults to '../logs/'.\n",
    "            config (dict, optional): Configuration parameters for visualization.\n",
    "                Supported keys:\n",
    "                - theme (str): Visual theme for plots. Default: 'darkgrid'\n",
    "                - fig_size (tuple): Default figure size. Default: (12, 8)\n",
    "                - dpi (int): DPI for saved figures. Default: 300\n",
    "                - grafana_export (bool): Whether to export data for Grafana. Default: True\n",
    "                - grafana_export_path (str): Path for Grafana data exports. Default: '../logs/grafana/'\n",
    "        \"\"\"\n",
    "        # Default configuration\n",
    "        default_config = {\n",
    "            'theme': 'darkgrid',\n",
    "            'fig_size': (12, 8),\n",
    "            'dpi': 300,\n",
    "            'grafana_export': True,\n",
    "            'grafana_export_path': '../logs/grafana/'\n",
    "        }\n",
    "        \n",
    "        # Use provided config or default\n",
    "        self.config = config if config else default_config\n",
    "        \n",
    "        # Set output directory\n",
    "        self.output_dir = output_dir if output_dir else '../logs/'\n",
    "        \n",
    "        # Create output directories if they don't exist\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        if self.config['grafana_export']:\n",
    "            os.makedirs(os.path.join(self.output_dir, 'grafana'), exist_ok=True)\n",
    "        \n",
    "        # Set visualization style\n",
    "        sns.set_theme(style=self.config['theme'])\n",
    "        plt.rcParams['figure.figsize'] = self.config['fig_size']\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm, classes=None, title='Confusion Matrix', \n",
    "                             normalize=False, save_path=None, cmap=plt.cm.Blues):\n",
    "        \"\"\"\n",
    "        Plot a confusion matrix.\n",
    "        \n",
    "        Args:\n",
    "            cm (numpy.ndarray): Confusion matrix to plot\n",
    "            classes (list, optional): List of class names\n",
    "            title (str): Title for the plot\n",
    "            normalize (bool): Whether to normalize the confusion matrix\n",
    "            save_path (str, optional): Path to save the figure\n",
    "            cmap (matplotlib.colors.Colormap): Colormap for the plot\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib.figure.Figure: The figure object\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        plt.figure(figsize=self.config['fig_size'])\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        \n",
    "        if classes:\n",
    "            tick_marks = np.arange(len(classes))\n",
    "            plt.xticks(tick_marks, classes, rotation=45)\n",
    "            plt.yticks(tick_marks, classes)\n",
    "        \n",
    "        fmt = '.2f' if normalize else 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                plt.text(j, i, format(cm[i, j], fmt),\n",
    "                        horizontalalignment=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=self.config['dpi'], bbox_inches='tight')\n",
    "            \n",
    "        return plt.gcf()\n",
    "    \n",
    "    def plot_model_comparison(self, models_data, metric='accuracy', title='Model Comparison',\n",
    "                             save_path=None):\n",
    "        \"\"\"\n",
    "        Plot a comparison of multiple models based on a specified metric.\n",
    "        \n",
    "        Args:\n",
    "            models_data (dict): Dictionary with model names as keys and metrics as values\n",
    "                Example: {'RandomForest': {'accuracy': 0.95, 'f1': 0.94},\n",
    "                         'LSTM': {'accuracy': 0.97, 'f1': 0.96}}\n",
    "            metric (str): Metric to compare (must be a key in the metrics dictionaries)\n",
    "            title (str): Title for the plot\n",
    "            save_path (str, optional): Path to save the figure\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib.figure.Figure: The figure object\n",
    "        \"\"\"\n",
    "        models = list(models_data.keys())\n",
    "        values = [data[metric] for data in models_data.values()]\n",
    "        \n",
    "        plt.figure(figsize=self.config['fig_size'])\n",
    "        bars = plt.bar(models, values, color=sns.color_palette('viridis', len(models)))\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.ylim(0, max(values) + 0.1)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=self.config['dpi'], bbox_inches='tight')\n",
    "            \n",
    "        # Export data for Grafana if enabled\n",
    "        if self.config['grafana_export']:\n",
    "            export_data = pd.DataFrame({\n",
    "                'model': models,\n",
    "                metric: values\n",
    "            })\n",
    "            export_path = os.path.join(self.output_dir, 'grafana', f'model_comparison_{metric}.csv')\n",
    "            export_data.to_csv(export_path, index=False)\n",
    "            \n",
    "        return plt.gcf()\n",
    "    \n",
    "    def plot_metrics_over_time(self, metrics_data, metrics=None, title='Metrics Over Time',\n",
    "                              save_path=None):\n",
    "        \"\"\"\n",
    "        Plot metrics over time for streaming evaluation.\n",
    "        \n",
    "        Args:\n",
    "            metrics_data (pandas.DataFrame): DataFrame with 'timestamp' column and metric columns\n",
    "            metrics (list, optional): List of metric names to plot. If None, all numeric columns except timestamp are used.\n",
    "            title (str): Title for the plot\n",
    "            save_path (str, optional): Path to save the figure\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib.figure.Figure: The figure object\n",
    "        \"\"\"\n",
    "        if 'timestamp' not in metrics_data.columns:\n",
    "            raise ValueError(\"metrics_data must contain a 'timestamp' column\")\n",
    "        \n",
    "        if metrics is None:\n",
    "            # Use all numeric columns except timestamp\n",
    "            metrics = [col for col in metrics_data.columns \n",
    "                      if col != 'timestamp' and pd.api.types.is_numeric_dtype(metrics_data[col])]\n",
    "        \n",
    "        plt.figure(figsize=self.config['fig_size'])\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric in metrics_data.columns:\n",
    "                plt.plot(metrics_data['timestamp'], metrics_data[metric], marker='o', label=metric)\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=self.config['dpi'], bbox_inches='tight')\n",
    "            \n",
    "        # Export data for Grafana if enabled\n",
    "        if self.config['grafana_export']:\n",
    "            export_path = os.path.join(self.output_dir, 'grafana', 'metrics_over_time.csv')\n",
    "            metrics_data.to_csv(export_path, index=False)\n",
    "            \n",
    "        return plt.gcf()\n",
    "    \n",
    "    def create_interactive_dashboard(self, models_data, confusion_matrices, metrics_over_time=None,\n",
    "                                    save_path=None):\n",
    "        \"\"\"\n",
    "        Create an interactive dashboard with Plotly.\n",
    "        \n",
    "        Args:\n",
    "            models_data (dict): Dictionary with model names as keys and metrics as values\n",
    "            confusion_matrices (dict): Dictionary with model names as keys and confusion matrices as values\n",
    "            metrics_over_time (pandas.DataFrame, optional): DataFrame with metrics over time\n",
    "            save_path (str, optional): Path to save the HTML dashboard\n",
    "            \n",
    "        Returns:\n",
    "            plotly.graph_objects.Figure: The dashboard figure\n",
    "        \"\"\"\n",
    "        # Create subplot grid\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Model Accuracy Comparison', 'Model F1 Score Comparison',\n",
    "                           'Confusion Matrix', 'Metrics Over Time'),\n",
    "            specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "                  [{'type': 'heatmap'}, {'type': 'scatter'}]]\n",
    "        )\n",
    "        \n",
    "        # Add model comparison bars for accuracy\n",
    "        models = list(models_data.keys())\n",
    "        accuracies = [data.get('accuracy', 0) for data in models_data.values()]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=models, y=accuracies, name='Accuracy', \n",
    "                  text=[f'{acc:.3f}' for acc in accuracies],\n",
    "                  textposition='auto'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Add model comparison bars for F1 score\n",
    "        f1_scores = [data.get('f1', 0) for data in models_data.values()]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=models, y=f1_scores, name='F1 Score',\n",
    "                  text=[f'{f1:.3f}' for f1 in f1_scores],\n",
    "                  textposition='auto'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Add confusion matrix for the best model\n",
    "        best_model = models[np.argmax(accuracies)]\n",
    "        if best_model in confusion_matrices:\n",
    "            cm = confusion_matrices[best_model]\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Heatmap(\n",
    "                    z=cm,\n",
    "                    x=['Fake', 'Real'],\n",
    "                    y=['Fake', 'Real'],\n",
    "                    colorscale='Blues',\n",
    "                    showscale=True,\n",
    "                    text=[[str(int(val)) for val in row] for row in cm],\n",
    "                    texttemplate=\"%{text}\",\n",
    "                    name='Confusion Matrix'\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "        \n",
    "        # Add metrics over time if provided\n",
    "        if metrics_over_time is not None and 'timestamp' in metrics_over_time.columns:\n",
    "            metrics = [col for col in metrics_over_time.columns \n",
    "                      if col != 'timestamp' and pd.api.types.is_numeric_dtype(metrics_over_time[col])]\n",
    "            \n",
    "            for metric in metrics:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=metrics_over_time['timestamp'],\n",
    "                        y=metrics_over_time[metric],\n",
    "                        mode='lines+markers',\n",
    "                        name=metric\n",
    "                    ),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title='Fake News Detection Dashboard',\n",
    "            height=800,\n",
    "            showlegend=True,\n",
    "            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "        )\n",
    "        \n",
    "        # Update axes\n",
    "        fig.update_yaxes(title_text='Accuracy', range=[0, 1], row=1, col=1)\n",
    "        fig.update_yaxes(title_text='F1 Score', range=[0, 1], row=1, col=2)\n",
    "        fig.update_xaxes(title_text='Predicted', row=2, col=1)\n",
    "        fig.update_yaxes(title_text='Actual', row=2, col=1)\n",
    "        fig.update_xaxes(title_text='Time', row=2, col=2)\n",
    "        fig.update_yaxes(title_text='Value', row=2, col=2)\n",
    "        \n",
    "        # Save if path provided\n",
    "        if save_path:\n",
    "            fig.write_html(save_path)\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def export_metrics_for_grafana(self, metrics_data, filename='metrics.json'):\n",
    "        \"\"\"\n",
    "        Export metrics in a format suitable for Grafana.\n",
    "        \n",
    "        Args:\n",
    "            metrics_data (dict or pandas.DataFrame): Metrics data to export\n",
    "            filename (str): Name of the output file\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to the exported file\n",
    "        \"\"\"\n",
    "        if not self.config['grafana_export']:\n",
    "            return None\n",
    "        \n",
    "        export_path = os.path.join(self.output_dir, 'grafana', filename)\n",
    "        \n",
    "        # Convert DataFrame to dict if needed\n",
    "        if isinstance(metrics_data, pd.DataFrame):\n",
    "            metrics_dict = metrics_data.to_dict(orient='records')\n",
    "        else:\n",
    "            metrics_dict = metrics_data\n",
    "        \n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(export_path), exist_ok=True)\n",
    "        \n",
    "        # Write to JSON file\n",
    "        with open(export_path, 'w') as f:\n",
    "            json.dump(metrics_dict, f, indent=2)\n",
    "        \n",
    "        return export_path\n",
    "    \n",
    "    def setup_grafana_datasource(self, metrics_files=None):\n",
    "        \"\"\"\n",
    "        Generate configuration for Grafana datasources.\n",
    "        \n",
    "        Args:\n",
    "            metrics_files (list, optional): List of metrics files to include as datasources\n",
    "            \n",
    "        Returns:\n",
    "            dict: Grafana datasource configuration\n",
    "        \"\"\"\n",
    "        if not metrics_files:\n",
    "            # Find all CSV and JSON files in the grafana directory\n",
    "            grafana_dir = os.path.join(self.output_dir, 'grafana')\n",
    "            if os.path.exists(grafana_dir):\n",
    "                metrics_files = [\n",
    "                    os.path.join(grafana_dir, f) \n",
    "                    for f in os.listdir(grafana_dir) \n",
    "                    if f.endswith(('.csv', '.json'))\n",
    "                ]\n",
    "        \n",
    "        # Create datasource configuration\n",
    "        datasources = []\n",
    "        \n",
    "        for i, file_path in enumerate(metrics_files):\n",
    "            file_name = os.path.basename(file_path)\n",
    "            datasource = {\n",
    "                \"name\": f\"FakeNewsMetrics_{file_name}\",\n",
    "                \"type\": \"csv\" if file_path.endswith('.csv') else \"json\",\n",
    "                \"url\": file_path,\n",
    "                \"access\": \"direct\",\n",
    "                \"isDefault\": i == 0\n",
    "            }\n",
    "            datasources.append(datasource)\n",
    "        \n",
    "        config = {\n",
    "            \"apiVersion\": 1,\n",
    "            \"datasources\": datasources\n",
    "        }\n",
    "        \n",
    "        # Save configuration\n",
    "        config_path = os.path.join(self.output_dir, 'grafana', 'datasources.yaml')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        return config\n",
    "    \n",
    "    def create_spark_streaming_dashboard(self, spark_session, streaming_df, metrics_columns,\n",
    "                                        output_path, checkpoint_path, trigger_interval=\"1 minute\"):\n",
    "        \"\"\"\n",
    "        Create a streaming dashboard for Spark Structured Streaming.\n",
    "        \n",
    "        Args:\n",
    "            spark_session (pyspark.sql.SparkSession): Spark session\n",
    "            streaming_df (pyspark.sql.DataFrame): Streaming DataFrame with metrics\n",
    "            metrics_columns (list): List of columns to include in the dashboard\n",
    "            output_path (str): Path to save the streaming output\n",
    "            checkpoint_path (str): Path for streaming checkpoints\n",
    "            trigger_interval (str): Interval for triggering stream processing\n",
    "            \n",
    "        Returns:\n",
    "            pyspark.sql.streaming.StreamingQuery: The streaming query\n",
    "        \"\"\"\n",
    "        # Select relevant columns and convert to JSON format for easy consumption\n",
    "        dashboard_df = streaming_df.select(\n",
    "            *[col(c) for c in metrics_columns],\n",
    "            expr(\"current_timestamp() as timestamp\")\n",
    "        )\n",
    "        \n",
    "        # Write stream to CSV files for Grafana\n",
    "        query = dashboard_df.writeStream \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .format(\"csv\") \\\n",
    "            .option(\"path\", output_path) \\\n",
    "            .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "            .trigger(processingTime=trigger_interval) \\\n",
    "            .start()\n",
    "        \n",
    "        return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e04f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualization_setup(output_dir=None, config=None):\n",
    "    \"\"\"\n",
    "    Create a VisualizationSetup instance with the specified configuration.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str, optional): Directory to save visualization outputs\n",
    "        config (dict, optional): Configuration parameters for visualization\n",
    "            \n",
    "    Returns:\n",
    "        VisualizationSetup: Configured visualization setup instance\n",
    "    \"\"\"\n",
    "    return VisualizationSetup(output_dir=output_dir, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fcf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate visualization functionality.\n",
    "    \"\"\"\n",
    "    # Create visualization setup\n",
    "    viz = create_visualization_setup()\n",
    "    \n",
    "    # Sample model comparison data\n",
    "    models_data = {\n",
    "        'RandomForest': {'accuracy': 0.95, 'precision': 0.94, 'recall': 0.95, 'f1': 0.94},\n",
    "        'NaiveBayes': {'accuracy': 0.92, 'precision': 0.91, 'recall': 0.93, 'f1': 0.92},\n",
    "        'LSTM': {'accuracy': 0.97, 'precision': 0.96, 'recall': 0.97, 'f1': 0.96},\n",
    "        'Transformer': {'accuracy': 0.98, 'precision': 0.97, 'recall': 0.98, 'f1': 0.97}\n",
    "    }\n",
    "    \n",
    "    # Sample confusion matrices\n",
    "    confusion_matrices = {\n",
    "        'RandomForest': np.array([[45, 5], [5, 45]]),\n",
    "        'NaiveBayes': np.array([[43, 7], [8, 42]]),\n",
    "        'LSTM': np.array([[47, 3], [3, 47]]),\n",
    "        'Transformer': np.array([[48, 2], [2, 48]])\n",
    "    }\n",
    "    \n",
    "    # Sample metrics over time\n",
    "    timestamps = pd.date_range(start='2025-05-01', periods=10, freq='D')\n",
    "    metrics_over_time = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'accuracy': np.linspace(0.90, 0.98, 10),\n",
    "        'f1': np.linspace(0.89, 0.97, 10),\n",
    "        'precision': np.linspace(0.88, 0.98, 10),\n",
    "        'recall': np.linspace(0.90, 0.96, 10)\n",
    "    })\n",
    "    \n",
    "    # Create visualizations\n",
    "    viz.plot_model_comparison(models_data, metric='accuracy', \n",
    "                             save_path=os.path.join(viz.output_dir, 'model_accuracy_comparison.png'))\n",
    "    \n",
    "    viz.plot_model_comparison(models_data, metric='f1', \n",
    "                             save_path=os.path.join(viz.output_dir, 'model_f1_comparison.png'))\n",
    "    \n",
    "    viz.plot_confusion_matrix(confusion_matrices['Transformer'], classes=['Fake', 'Real'],\n",
    "                             save_path=os.path.join(viz.output_dir, 'transformer_confusion_matrix.png'))\n",
    "    \n",
    "    viz.plot_metrics_over_time(metrics_over_time,\n",
    "                              save_path=os.path.join(viz.output_dir, 'metrics_over_time.png'))\n",
    "    \n",
    "    # Create interactive dashboard\n",
    "    viz.create_interactive_dashboard(models_data, confusion_matrices, metrics_over_time,\n",
    "                                    save_path=os.path.join(viz.output_dir, 'dashboard.html'))\n",
    "    \n",
    "    # Export metrics for Grafana\n",
    "    viz.export_metrics_for_grafana(models_data, filename='model_metrics.json')\n",
    "    viz.export_metrics_for_grafana(metrics_over_time, filename='streaming_metrics.json')\n",
    "    \n",
    "    # Setup Grafana datasources\n",
    "    viz.setup_grafana_datasource()\n",
    "    \n",
    "    print(f\"Visualizations saved to {viz.output_dir}\")\n",
    "    print(f\"Grafana data exported to {os.path.join(viz.output_dir, 'grafana')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06766165",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Last modified: May 29, 2025
