{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db02d7de",
   "metadata": {},
   "source": [
    "# Fake News Detection: Visualization Analysis\n",
    "\n",
    "This notebook contains all the necessary code for visualization analysis in the fake news detection project. The code is organized into independent functions, without dependencies on external modules or classes, to facilitate execution in Databricks Community Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c623551d",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, to_json, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1dc5a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Initialize Spark session optimized for Databricks Community Edition\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeNewsDetection_VisualizationAnalysis\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Display Spark configuration\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Driver memory: {spark.conf.get('spark.driver.memory')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b85eff",
   "metadata": {},
   "source": [
    "## Reusable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc987a93",
   "metadata": {},
   "source": [
    "### Configuration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b804e4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_visualization_config(theme='darkgrid', fig_size=(12, 8), dpi=300, \n",
    "                               grafana_export=True, grafana_export_path=None):\n",
    "    \"\"\"\n",
    "    Create a configuration dictionary for visualizations.\n",
    "    \n",
    "    Args:\n",
    "        theme (str): Visual theme for plots\n",
    "        fig_size (tuple): Default figure size\n",
    "        dpi (int): DPI for saved figures\n",
    "        grafana_export (bool): Whether to export data for Grafana\n",
    "        grafana_export_path (str): Path for Grafana data exports\n",
    "        \n",
    "    Returns:\n",
    "        dict: Configuration dictionary\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        'theme': theme,\n",
    "        'fig_size': fig_size,\n",
    "        'dpi': dpi,\n",
    "        'grafana_export': grafana_export,\n",
    "        'grafana_export_path': grafana_export_path\n",
    "    }\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da67697",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def setup_visualization_environment(config=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Set up the visualization environment with the specified configuration.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Configuration dictionary\n",
    "        output_dir (str): Directory to save visualization outputs\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (config, output_dir) - Updated configuration and output directory\n",
    "    \"\"\"\n",
    "    # Default configuration\n",
    "    if config is None:\n",
    "        config = create_visualization_config()\n",
    "    \n",
    "    # Default output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = '/tmp/fake_news_detection/logs/'\n",
    "    \n",
    "    # Set Grafana export path if not specified\n",
    "    if config['grafana_export'] and config['grafana_export_path'] is None:\n",
    "        config['grafana_export_path'] = os.path.join(output_dir, 'grafana')\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if config['grafana_export']:\n",
    "        os.makedirs(config['grafana_export_path'], exist_ok=True)\n",
    "    \n",
    "    # Set visualization style\n",
    "    sns.set_theme(style=config['theme'])\n",
    "    plt.rcParams['figure.figsize'] = config['fig_size']\n",
    "    \n",
    "    return config, output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae54e2",
   "metadata": {},
   "source": [
    "### Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23ab93",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_model_metrics(metrics_path):\n",
    "    \"\"\"\n",
    "    Load model metrics from a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        metrics_path (str): Path to the metrics JSON file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with model metrics\n",
    "    \"\"\"\n",
    "    print(f\"Loading model metrics from {metrics_path}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(metrics_path, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        print(f\"Successfully loaded metrics for {len(metrics)} models\")\n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model metrics: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ed971",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_confusion_matrices(cm_path):\n",
    "    \"\"\"\n",
    "    Load confusion matrices from a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        cm_path (str): Path to the confusion matrices JSON file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with confusion matrices\n",
    "    \"\"\"\n",
    "    print(f\"Loading confusion matrices from {cm_path}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(cm_path, 'r') as f:\n",
    "            cm_data = json.load(f)\n",
    "        \n",
    "        # Convert lists to numpy arrays\n",
    "        confusion_matrices = {}\n",
    "        for model, cm in cm_data.items():\n",
    "            confusion_matrices[model] = np.array(cm)\n",
    "        \n",
    "        print(f\"Successfully loaded confusion matrices for {len(confusion_matrices)} models\")\n",
    "        return confusion_matrices\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading confusion matrices: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da8bcec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_time_series_metrics(time_series_path):\n",
    "    \"\"\"\n",
    "    Load time series metrics from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        time_series_path (str): Path to the time series CSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame with time series metrics\n",
    "    \"\"\"\n",
    "    print(f\"Loading time series metrics from {time_series_path}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(time_series_path)\n",
    "        \n",
    "        # Convert timestamp column to datetime if it exists\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        print(f\"Successfully loaded time series metrics with {len(df)} records\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading time series metrics: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1acbdd7",
   "metadata": {},
   "source": [
    "### Basic Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42aa5a6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes=None, title='Confusion Matrix', \n",
    "                         normalize=False, config=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        cm (numpy.ndarray): Confusion matrix to plot\n",
    "        classes (list): List of class names\n",
    "        title (str): Title for the plot\n",
    "        normalize (bool): Whether to normalize the confusion matrix\n",
    "        config (dict): Visualization configuration\n",
    "        save_path (str): Path to save the figure\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The figure object\n",
    "    \"\"\"\n",
    "    # Use default config if not provided\n",
    "    if config is None:\n",
    "        config = create_visualization_config()\n",
    "    \n",
    "    # Normalize if requested\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=config['fig_size'])\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Set tick marks and labels\n",
    "    if classes is None:\n",
    "        classes = ['Fake', 'Real']\n",
    "    \n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=config['dpi'], bbox_inches='tight')\n",
    "        print(f\"Confusion matrix saved to {save_path}\")\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f9f69",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_model_comparison(models_data, metric='accuracy', title='Model Comparison',\n",
    "                         config=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot a comparison of multiple models based on a specified metric.\n",
    "    \n",
    "    Args:\n",
    "        models_data (dict): Dictionary with model names as keys and metrics as values\n",
    "        metric (str): Metric to compare\n",
    "        title (str): Title for the plot\n",
    "        config (dict): Visualization configuration\n",
    "        save_path (str): Path to save the figure\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The figure object\n",
    "    \"\"\"\n",
    "    # Use default config if not provided\n",
    "    if config is None:\n",
    "        config = create_visualization_config()\n",
    "    \n",
    "    # Extract data\n",
    "    models = list(models_data.keys())\n",
    "    values = [data.get(metric, 0) for data in models_data.values()]\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=config['fig_size'])\n",
    "    bars = plt.bar(models, values, color=sns.color_palette('viridis', len(models)))\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.ylim(0, max(values) + 0.1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=config['dpi'], bbox_inches='tight')\n",
    "        print(f\"Model comparison plot saved to {save_path}\")\n",
    "    \n",
    "    # Export data for Grafana if enabled\n",
    "    if config['grafana_export'] and config['grafana_export_path']:\n",
    "        export_data = pd.DataFrame({\n",
    "            'model': models,\n",
    "            metric: values\n",
    "        })\n",
    "        export_path = os.path.join(config['grafana_export_path'], f'model_comparison_{metric}.csv')\n",
    "        export_data.to_csv(export_path, index=False)\n",
    "        print(f\"Model comparison data exported to {export_path}\")\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d22754d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_metrics_over_time(metrics_data, metrics=None, title='Metrics Over Time',\n",
    "                          config=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot metrics over time for streaming evaluation.\n",
    "    \n",
    "    Args:\n",
    "        metrics_data (DataFrame): DataFrame with 'timestamp' column and metric columns\n",
    "        metrics (list): List of metric names to plot\n",
    "        title (str): Title for the plot\n",
    "        config (dict): Visualization configuration\n",
    "        save_path (str): Path to save the figure\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The figure object\n",
    "    \"\"\"\n",
    "    # Use default config if not provided\n",
    "    if config is None:\n",
    "        config = create_visualization_config()\n",
    "    \n",
    "    # Check for timestamp column\n",
    "    if 'timestamp' not in metrics_data.columns:\n",
    "        raise ValueError(\"metrics_data must contain a 'timestamp' column\")\n",
    "    \n",
    "    # Use all numeric columns if metrics not specified\n",
    "    if metrics is None:\n",
    "        metrics = [col for col in metrics_data.columns \n",
    "                  if col != 'timestamp' and pd.api.types.is_numeric_dtype(metrics_data[col])]\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=config['fig_size'])\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric in metrics_data.columns:\n",
    "            plt.plot(metrics_data['timestamp'], metrics_data[metric], marker='o', label=metric)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=config['dpi'], bbox_inches='tight')\n",
    "        print(f\"Metrics over time plot saved to {save_path}\")\n",
    "    \n",
    "    # Export data for Grafana if enabled\n",
    "    if config['grafana_export'] and config['grafana_export_path']:\n",
    "        export_path = os.path.join(config['grafana_export_path'], 'metrics_over_time.csv')\n",
    "        metrics_data.to_csv(export_path, index=False)\n",
    "        print(f\"Metrics over time data exported to {export_path}\")\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a388b0",
   "metadata": {},
   "source": [
    "### Advanced Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e4f41",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(feature_names, importance_values, title='Feature Importance',\n",
    "                           top_n=20, config=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot feature importance from a trained model.\n",
    "    \n",
    "    Args:\n",
    "        feature_names (list): List of feature names\n",
    "        importance_values (list): List of importance values\n",
    "        title (str): Title for the plot\n",
    "        top_n (int): Number of top features to display\n",
    "        config (dict): Visualization configuration\n",
    "        save_path (str): Path to save the figure\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The figure object\n",
    "    \"\"\"\n",
    "    # Use default config if not provided\n",
    "    if config is None:\n",
    "        config = create_visualization_config()\n",
    "    \n",
    "    # Create DataFrame for easier sorting\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance_values\n",
    "    })\n",
    "    \n",
    "    # Sort by importance and get top N\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False).head(top_n)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=config['fig_size'])\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=config['dpi'], bbox_inches='tight')\n",
    "        print(f\"Feature importance plot saved to {save_path}\")\n",
    "    \n",
    "    # Export data for Grafana if enabled\n",
    "    if config['grafana_export'] and config['grafana_export_path']:\n",
    "        export_path = os.path.join(config['grafana_export_path'], 'feature_importance.csv')\n",
    "        feature_importance.to_csv(export_path, index=False)\n",
    "        print(f\"Feature importance data exported to {export_path}\")\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0014ae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr_dict, tpr_dict, auc_dict, title='ROC Curve',\n",
    "                  config=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for multiple models.\n",
    "    \n",
    "    Args:\n",
    "        fpr_dict (dict): Dictionary with model names as keys and false positive rates as values\n",
    "        tpr_dict (dict): Dictionary with model names as keys and true positive rates as values\n",
    "        auc_dict (dict): Dictionary with model names as keys and AUC values as values\n",
    "        title (str): Title for the plot\n",
    "        config (dict): Visualization configuration\n",
    "        save_path (str): Path to save the figure\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The figure object\n",
    "    \"\"\"\n",
    "    # Use default config if not provided\n",
    "    if config is None:\n",
    "        config = create_visualization_config()\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=config['fig_size'])\n",
    "    \n",
    "    # Plot diagonal line for random classifier\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    \n",
    "    # Plot ROC curve for each model\n",
    "    for model_name in fpr_dict.keys():\n",
    "        if model_name in tpr_dict and model_name in auc_dict:\n",
    "            plt.plot(fpr_dict[model_name], tpr_dict[model_name],\n",
    "                    label=f'{model_name} (AUC = {auc_dict[model_name]:.3f})')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=config['dpi'], bbox_inches='tight')\n",
    "        print(f\"ROC curve plot saved to {save_path}\")\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d6270",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(precision_dict, recall_dict, ap_dict, title='Precision-Recall Curve',\n",
    "                               config=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot precision-recall curves for multiple models.\n",
    "    \n",
    "    Args:\n",
    "        precision_dict (dict): Dictionary with model names as keys and precision values as values\n",
    "        recall_dict (dict): Dictionary with model names as keys and recall values as values\n",
    "        ap_dict (dict): Dictionary with model names as keys and average precision values as values\n",
    "        title (str): Title for the plot\n",
    "        config (dict): Visualization configuration\n",
    "        save_path (str): Path to save the figure\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The figure object\n",
    "    \"\"\"\n",
    "    # Use default config if not provided\n",
    "    if config is None:\n",
    "        config = create_visualization_config()\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=config['fig_size'])\n",
    "    \n",
    "    # Plot precision-recall curve for each model\n",
    "    for model_name in precision_dict.keys():\n",
    "        if model_name in recall_dict and model_name in ap_dict:\n",
    "            plt.plot(recall_dict[model_name], precision_dict[model_name],\n",
    "                    label=f'{model_name} (AP = {ap_dict[model_name]:.3f})')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=config['dpi'], bbox_inches='tight')\n",
    "        print(f\"Precision-recall curve plot saved to {save_path}\")\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33779add",
   "metadata": {},
   "source": [
    "### Interactive Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00442592",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_interactive_confusion_matrix(cm, classes=None, title='Confusion Matrix',\n",
    "                                       normalize=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Create an interactive confusion matrix with Plotly.\n",
    "    \n",
    "    Args:\n",
    "        cm (numpy.ndarray): Confusion matrix to plot\n",
    "        classes (list): List of class names\n",
    "        title (str): Title for the plot\n",
    "        normalize (bool): Whether to normalize the confusion matrix\n",
    "        save_path (str): Path to save the HTML file\n",
    "        \n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure: The figure object\n",
    "    \"\"\"\n",
    "    # Normalize if requested\n",
    "    if normalize:\n",
    "        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        z = cm_norm\n",
    "        text = [[f'{val:.2f}' for val in row] for row in cm_norm]\n",
    "    else:\n",
    "        z = cm\n",
    "        text = [[str(int(val)) for val in row] for row in cm]\n",
    "    \n",
    "    # Set default classes if not provided\n",
    "    if classes is None:\n",
    "        classes = ['Fake', 'Real']\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=z,\n",
    "        x=classes,\n",
    "        y=classes,\n",
    "        colorscale='Blues',\n",
    "        text=text,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 16},\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis=dict(title='Predicted label'),\n",
    "        yaxis=dict(title='True label'),\n",
    "        width=600,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        fig.write_html(save_path)\n",
    "        print(f\"Interactive confusion matrix saved to {save_path}\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4493904",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_interactive_model_comparison(models_data, metrics=None, title='Model Comparison',\n",
    "                                       save_path=None):\n",
    "    \"\"\"\n",
    "    Create an interactive model comparison with Plotly.\n",
    "    \n",
    "    Args:\n",
    "        models_data (dict): Dictionary with model names as keys and metrics as values\n",
    "        metrics (list): List of metrics to compare\n",
    "        title (str): Title for the plot\n",
    "        save_path (str): Path to save the HTML file\n",
    "        \n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure: The figure object\n",
    "    \"\"\"\n",
    "    # Extract model names\n",
    "    models = list(models_data.keys())\n",
    "    \n",
    "    # Use all metrics if not specified\n",
    "    if metrics is None and len(models_data) > 0:\n",
    "        # Get metrics from first model\n",
    "        first_model = next(iter(models_data.values()))\n",
    "        metrics = list(first_model.keys())\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add bar for each metric\n",
    "    for metric in metrics:\n",
    "        values = [data.get(metric, 0) for data in models_data.values()]\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=models,\n",
    "            y=values,\n",
    "            name=metric,\n",
    "            text=[f'{val:.3f}' for val in values],\n",
    "            textposition='auto'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis=dict(title='Model'),\n",
    "        yaxis=dict(title='Value', range=[0, 1]),\n",
    "        barmode='group',\n",
    "        width=800,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        fig.write_html(save_path)\n",
    "        print(f\"Interactive model comparison saved to {save_path}\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d57f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_interactive_time_series(metrics_data, metrics=None, title='Metrics Over Time',\n",
    "                                  save_path=None):\n",
    "    \"\"\"\n",
    "    Create an interactive time series plot with Plotly.\n",
    "    \n",
    "    Args:\n",
    "        metrics_data (DataFrame): DataFrame with 'timestamp' column and metric columns\n",
    "        metrics (list): List of metrics to plot\n",
    "        title (str): Title for the plot\n",
    "        save_path (str): Path to save the HTML file\n",
    "        \n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure: The figure object\n",
    "    \"\"\"\n",
    "    # Check for timestamp column\n",
    "    if 'timestamp' not in metrics_data.columns:\n",
    "        raise ValueError(\"metrics_data must contain a 'timestamp' column\")\n",
    "    \n",
    "    # Use all numeric columns if metrics not specified\n",
    "    if metrics is None:\n",
    "        metrics = [col for col in metrics_data.columns \n",
    "                  if col != 'timestamp' and pd.api.types.is_numeric_dtype(metrics_data[col])]\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add line for each metric\n",
    "    for metric in metrics:\n",
    "        if metric in metrics_data.columns:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=metrics_data['timestamp'],\n",
    "                y=metrics_data[metric],\n",
    "                mode='lines+markers',\n",
    "                name=metric\n",
    "            ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis=dict(title='Time'),\n",
    "        yaxis=dict(title='Value'),\n",
    "        width=800,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        fig.write_html(save_path)\n",
    "        print(f\"Interactive time series plot saved to {save_path}\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad2142",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_interactive_dashboard(models_data, confusion_matrices, metrics_over_time=None,\n",
    "                                title='Fake News Detection Dashboard', save_path=None):\n",
    "    \"\"\"\n",
    "    Create an interactive dashboard with Plotly.\n",
    "    \n",
    "    Args:\n",
    "        models_data (dict): Dictionary with model names as keys and metrics as values\n",
    "        confusion_matrices (dict): Dictionary with model names as keys and confusion matrices as values\n",
    "        metrics_over_time (DataFrame): DataFrame with metrics over time\n",
    "        title (str): Title for the dashboard\n",
    "        save_path (str): Path to save the HTML file\n",
    "        \n",
    "    Returns:\n",
    "        plotly.graph_objects.Figure: The figure object\n",
    "    \"\"\"\n",
    "    # Create subplot grid\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Model Accuracy Comparison', 'Model F1 Score Comparison',\n",
    "                       'Confusion Matrix', 'Metrics Over Time'),\n",
    "        specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "              [{'type': 'heatmap'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "    \n",
    "    # Add model comparison bars for accuracy\n",
    "    models = list(models_data.keys())\n",
    "    accuracies = [data.get('accuracy', 0) for data in models_data.values()]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=models, \n",
    "            y=accuracies, \n",
    "            name='Accuracy', \n",
    "            text=[f'{acc:.3f}' for acc in accuracies],\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add model comparison bars for F1 score\n",
    "    f1_scores = [data.get('f1', 0) for data in models_data.values()]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=models, \n",
    "            y=f1_scores, \n",
    "            name='F1 Score',\n",
    "            text=[f'{f1:.3f}' for f1 in f1_scores],\n",
    "            textposition='auto'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Add confusion matrix for the best model\n",
    "    best_model = models[np.argmax(accuracies)] if accuracies else None\n",
    "    if best_model and best_model in confusion_matrices:\n",
    "        cm = confusion_matrices[best_model]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=cm,\n",
    "                x=['Fake', 'Real'],\n",
    "                y=['Fake', 'Real'],\n",
    "                colorscale='Blues',\n",
    "                showscale=True,\n",
    "                text=[[str(int(val)) for val in row] for row in cm],\n",
    "                texttemplate=\"%{text}\",\n",
    "                name='Confusion Matrix'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Add metrics over time if provided\n",
    "    if metrics_over_time is not None and 'timestamp' in metrics_over_time.columns:\n",
    "        metrics = [col for col in metrics_over_time.columns \n",
    "                  if col != 'timestamp' and pd.api.types.is_numeric_dtype(metrics_over_time[col])]\n",
    "        \n",
    "        for metric in metrics:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=metrics_over_time['timestamp'],\n",
    "                    y=metrics_over_time[metric],\n",
    "                    mode='lines+markers',\n",
    "                    name=metric\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_yaxes(title_text='Accuracy', range=[0, 1], row=1, col=1)\n",
    "    fig.update_yaxes(title_text='F1 Score', range=[0, 1], row=1, col=2)\n",
    "    fig.update_xaxes(title_text='Predicted', row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Actual', row=2, col=1)\n",
    "    fig.update_xaxes(title_text='Time', row=2, col=2)\n",
    "    fig.update_yaxes(title_text='Value', row=2, col=2)\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        fig.write_html(save_path)\n",
    "        print(f\"Interactive dashboard saved to {save_path}\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c84db2",
   "metadata": {},
   "source": [
    "### Grafana Integration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc9af0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def export_metrics_for_grafana(metrics_data, output_path, filename='metrics.json'):\n",
    "    \"\"\"\n",
    "    Export metrics in a format suitable for Grafana.\n",
    "    \n",
    "    Args:\n",
    "        metrics_data (dict or DataFrame): Metrics data to export\n",
    "        output_path (str): Directory to save the exported file\n",
    "        filename (str): Name of the output file\n",
    "        \n",
    "    Returns:\n",
    "        str: Path to the exported file\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Full path to the output file\n",
    "    export_path = os.path.join(output_path, filename)\n",
    "    \n",
    "    # Convert DataFrame to dict if needed\n",
    "    if isinstance(metrics_data, pd.DataFrame):\n",
    "        # Convert to records format\n",
    "        metrics_dict = metrics_data.to_dict(orient='records')\n",
    "    else:\n",
    "        metrics_dict = metrics_data\n",
    "    \n",
    "    # Export to JSON\n",
    "    with open(export_path, 'w') as f:\n",
    "        json.dump(metrics_dict, f, indent=2)\n",
    "    \n",
    "    print(f\"Metrics exported to {export_path}\")\n",
    "    return export_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e2338d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def setup_grafana_datasource(metrics_path, dashboard_title='Fake News Detection'):\n",
    "    \"\"\"\n",
    "    Generate a Grafana datasource configuration.\n",
    "    \n",
    "    Args:\n",
    "        metrics_path (str): Path to the metrics file\n",
    "        dashboard_title (str): Title for the Grafana dashboard\n",
    "        \n",
    "    Returns:\n",
    "        dict: Grafana datasource configuration\n",
    "    \"\"\"\n",
    "    # Create datasource configuration\n",
    "    datasource = {\n",
    "        \"name\": \"FakeNewsMetrics\",\n",
    "        \"type\": \"json\",\n",
    "        \"url\": metrics_path,\n",
    "        \"access\": \"direct\",\n",
    "        \"jsonData\": {\n",
    "            \"timeField\": \"timestamp\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create dashboard configuration\n",
    "    dashboard = {\n",
    "        \"title\": dashboard_title,\n",
    "        \"uid\": \"fake-news-detection\",\n",
    "        \"panels\": [\n",
    "            {\n",
    "                \"title\": \"Model Accuracy\",\n",
    "                \"type\": \"gauge\",\n",
    "                \"datasource\": \"FakeNewsMetrics\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"target\": \"accuracy\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"title\": \"Fake vs Real News\",\n",
    "                \"type\": \"piechart\",\n",
    "                \"datasource\": \"FakeNewsMetrics\",\n",
    "                \"targets\": [\n",
    "                    {\n",
    "                        \"target\": \"fake_count\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"target\": \"real_count\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"datasource\": datasource,\n",
    "        \"dashboard\": dashboard\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d33a6",
   "metadata": {},
   "source": [
    "## Complete Visualization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee13da4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_visualization_pipeline(\n",
    "    model_metrics_path=\"/tmp/fake_news_detection/results/model_metrics.json\",\n",
    "    confusion_matrices_path=\"/tmp/fake_news_detection/results/confusion_matrices.json\",\n",
    "    time_series_path=\"/tmp/fake_news_detection/results/metrics_over_time.csv\",\n",
    "    feature_importance_path=\"/tmp/fake_news_detection/results/feature_importance.json\",\n",
    "    output_dir=\"/tmp/fake_news_detection/visualizations\",\n",
    "    grafana_export=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the complete visualization pipeline for fake news detection.\n",
    "    \n",
    "    Args:\n",
    "        model_metrics_path (str): Path to the model metrics JSON file\n",
    "        confusion_matrices_path (str): Path to the confusion matrices JSON file\n",
    "        time_series_path (str): Path to the time series metrics CSV file\n",
    "        feature_importance_path (str): Path to the feature importance JSON file\n",
    "        output_dir (str): Directory to save visualization outputs\n",
    "        grafana_export (bool): Whether to export data for Grafana\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with references to visualization results\n",
    "    \"\"\"\n",
    "    print(\"Starting visualization pipeline...\")\n",
    "    \n",
    "    # 1. Set up configuration\n",
    "    config = create_visualization_config(\n",
    "        grafana_export=grafana_export,\n",
    "        grafana_export_path=os.path.join(output_dir, 'grafana')\n",
    "    )\n",
    "    \n",
    "    config, output_dir = setup_visualization_environment(config, output_dir)\n",
    "    \n",
    "    # Create subdirectories\n",
    "    static_dir = os.path.join(output_dir, 'static')\n",
    "    interactive_dir = os.path.join(output_dir, 'interactive')\n",
    "    \n",
    "    os.makedirs(static_dir, exist_ok=True)\n",
    "    os.makedirs(interactive_dir, exist_ok=True)\n",
    "    \n",
    "    # 2. Load data\n",
    "    model_metrics = {}\n",
    "    confusion_matrices = {}\n",
    "    time_series_metrics = pd.DataFrame()\n",
    "    feature_importance = {}\n",
    "    \n",
    "    # Try to load model metrics\n",
    "    try:\n",
    "        model_metrics = load_model_metrics(model_metrics_path)\n",
    "    except:\n",
    "        print(\"Warning: Could not load model metrics. Using sample data.\")\n",
    "        # Create sample data\n",
    "        model_metrics = {\n",
    "            \"RandomForest\": {\"accuracy\": 0.92, \"precision\": 0.91, \"recall\": 0.90, \"f1\": 0.90},\n",
    "            \"LogisticRegression\": {\"accuracy\": 0.88, \"precision\": 0.87, \"recall\": 0.86, \"f1\": 0.86},\n",
    "            \"LSTM\": {\"accuracy\": 0.94, \"precision\": 0.93, \"recall\": 0.92, \"f1\": 0.92}\n",
    "        }\n",
    "    \n",
    "    # Try to load confusion matrices\n",
    "    try:\n",
    "        confusion_matrices = load_confusion_matrices(confusion_matrices_path)\n",
    "    except:\n",
    "        print(\"Warning: Could not load confusion matrices. Using sample data.\")\n",
    "        # Create sample data\n",
    "        confusion_matrices = {\n",
    "            \"RandomForest\": np.array([[450, 50], [40, 460]]),\n",
    "            \"LogisticRegression\": np.array([[430, 70], [60, 440]]),\n",
    "            \"LSTM\": np.array([[460, 40], [30, 470]])\n",
    "        }\n",
    "    \n",
    "    # Try to load time series metrics\n",
    "    try:\n",
    "        time_series_metrics = load_time_series_metrics(time_series_path)\n",
    "    except:\n",
    "        print(\"Warning: Could not load time series metrics. Using sample data.\")\n",
    "        # Create sample data\n",
    "        dates = pd.date_range(start='2025-01-01', periods=10, freq='D')\n",
    "        time_series_metrics = pd.DataFrame({\n",
    "            'timestamp': dates,\n",
    "            'accuracy': np.linspace(0.85, 0.95, 10),\n",
    "            'precision': np.linspace(0.84, 0.94, 10),\n",
    "            'recall': np.linspace(0.83, 0.93, 10),\n",
    "            'f1': np.linspace(0.83, 0.93, 10)\n",
    "        })\n",
    "    \n",
    "    # Try to load feature importance\n",
    "    try:\n",
    "        with open(feature_importance_path, 'r') as f:\n",
    "            feature_importance = json.load(f)\n",
    "    except:\n",
    "        print(\"Warning: Could not load feature importance. Using sample data.\")\n",
    "        # Create sample data\n",
    "        feature_importance = {\n",
    "            \"features\": [f\"feature_{i}\" for i in range(20)],\n",
    "            \"importance\": np.random.rand(20).tolist()\n",
    "        }\n",
    "    \n",
    "    # 3. Create static visualizations\n",
    "    \n",
    "    # Model comparison plots\n",
    "    for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "        plot_model_comparison(\n",
    "            model_metrics, \n",
    "            metric=metric,\n",
    "            title=f'Model Comparison - {metric.capitalize()}',\n",
    "            config=config,\n",
    "            save_path=os.path.join(static_dir, f'model_comparison_{metric}.png')\n",
    "        )\n",
    "    \n",
    "    # Confusion matrix for each model\n",
    "    for model, cm in confusion_matrices.items():\n",
    "        plot_confusion_matrix(\n",
    "            cm,\n",
    "            classes=['Fake', 'Real'],\n",
    "            title=f'Confusion Matrix - {model}',\n",
    "            config=config,\n",
    "            save_path=os.path.join(static_dir, f'confusion_matrix_{model}.png')\n",
    "        )\n",
    "        \n",
    "        # Also create normalized version\n",
    "        plot_confusion_matrix(\n",
    "            cm,\n",
    "            classes=['Fake', 'Real'],\n",
    "            title=f'Normalized Confusion Matrix - {model}',\n",
    "            normalize=True,\n",
    "            config=config,\n",
    "            save_path=os.path.join(static_dir, f'confusion_matrix_{model}_normalized.png')\n",
    "        )\n",
    "    \n",
    "    # Metrics over time\n",
    "    if not time_series_metrics.empty and 'timestamp' in time_series_metrics.columns:\n",
    "        plot_metrics_over_time(\n",
    "            time_series_metrics,\n",
    "            title='Performance Metrics Over Time',\n",
    "            config=config,\n",
    "            save_path=os.path.join(static_dir, 'metrics_over_time.png')\n",
    "        )\n",
    "    \n",
    "    # Feature importance\n",
    "    if 'features' in feature_importance and 'importance' in feature_importance:\n",
    "        plot_feature_importance(\n",
    "            feature_importance['features'],\n",
    "            feature_importance['importance'],\n",
    "            title='Feature Importance',\n",
    "            config=config,\n",
    "            save_path=os.path.join(static_dir, 'feature_importance.png')\n",
    "        )\n",
    "    \n",
    "    # 4. Create interactive visualizations\n",
    "    \n",
    "    # Interactive model comparison\n",
    "    create_interactive_model_comparison(\n",
    "        model_metrics,\n",
    "        metrics=['accuracy', 'precision', 'recall', 'f1'],\n",
    "        title='Model Performance Comparison',\n",
    "        save_path=os.path.join(interactive_dir, 'model_comparison.html')\n",
    "    )\n",
    "    \n",
    "    # Interactive confusion matrix for best model\n",
    "    best_model = max(model_metrics.items(), key=lambda x: x[1].get('accuracy', 0))[0]\n",
    "    if best_model in confusion_matrices:\n",
    "        create_interactive_confusion_matrix(\n",
    "            confusion_matrices[best_model],\n",
    "            classes=['Fake', 'Real'],\n",
    "            title=f'Confusion Matrix - {best_model}',\n",
    "            save_path=os.path.join(interactive_dir, 'confusion_matrix.html')\n",
    "        )\n",
    "    \n",
    "    # Interactive time series\n",
    "    if not time_series_metrics.empty and 'timestamp' in time_series_metrics.columns:\n",
    "        create_interactive_time_series(\n",
    "            time_series_metrics,\n",
    "            title='Performance Metrics Over Time',\n",
    "            save_path=os.path.join(interactive_dir, 'metrics_over_time.html')\n",
    "        )\n",
    "    \n",
    "    # Interactive dashboard\n",
    "    create_interactive_dashboard(\n",
    "        model_metrics,\n",
    "        confusion_matrices,\n",
    "        time_series_metrics,\n",
    "        title='Fake News Detection Dashboard',\n",
    "        save_path=os.path.join(interactive_dir, 'dashboard.html')\n",
    "    )\n",
    "    \n",
    "    # 5. Export data for Grafana if enabled\n",
    "    if grafana_export:\n",
    "        # Export model metrics\n",
    "        export_metrics_for_grafana(\n",
    "            model_metrics,\n",
    "            config['grafana_export_path'],\n",
    "            'model_metrics.json'\n",
    "        )\n",
    "        \n",
    "        # Export time series metrics\n",
    "        if not time_series_metrics.empty:\n",
    "            time_series_metrics.to_csv(\n",
    "                os.path.join(config['grafana_export_path'], 'time_series_metrics.csv'),\n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        # Generate Grafana configuration\n",
    "        grafana_config = setup_grafana_datasource(\n",
    "            os.path.join(config['grafana_export_path'], 'model_metrics.json'),\n",
    "            'Fake News Detection Dashboard'\n",
    "        )\n",
    "        \n",
    "        with open(os.path.join(config['grafana_export_path'], 'grafana_config.json'), 'w') as f:\n",
    "            json.dump(grafana_config, f, indent=2)\n",
    "    \n",
    "    print(f\"Visualization pipeline completed!\")\n",
    "    print(f\"Static visualizations saved to: {static_dir}\")\n",
    "    print(f\"Interactive visualizations saved to: {interactive_dir}\")\n",
    "    if grafana_export:\n",
    "        print(f\"Grafana data exported to: {config['grafana_export_path']}\")\n",
    "    \n",
    "    return {\n",
    "        \"static_dir\": static_dir,\n",
    "        \"interactive_dir\": interactive_dir,\n",
    "        \"grafana_dir\": config['grafana_export_path'] if grafana_export else None,\n",
    "        \"model_metrics\": model_metrics,\n",
    "        \"confusion_matrices\": confusion_matrices,\n",
    "        \"time_series_metrics\": time_series_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1e3863",
   "metadata": {},
   "source": [
    "## Step-by-Step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6bc2f",
   "metadata": {},
   "source": [
    "### 1. Set Up Visualization Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eae1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "config = create_visualization_config(\n",
    "    theme='darkgrid',\n",
    "    fig_size=(10, 6),\n",
    "    dpi=300,\n",
    "    grafana_export=True,\n",
    "    grafana_export_path='/tmp/fake_news_detection/grafana'\n",
    ")\n",
    "\n",
    "# Set up environment\n",
    "config, output_dir = setup_visualization_environment(\n",
    "    config,\n",
    "    output_dir='/tmp/fake_news_detection/visualizations'\n",
    ")\n",
    "\n",
    "print(f\"Visualization environment set up with output directory: {output_dir}\")\n",
    "print(f\"Configuration: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8830243",
   "metadata": {},
   "source": [
    "### 2. Create Sample Data for Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample model metrics\n",
    "model_metrics = {\n",
    "    \"RandomForest\": {\"accuracy\": 0.92, \"precision\": 0.91, \"recall\": 0.90, \"f1\": 0.90},\n",
    "    \"LogisticRegression\": {\"accuracy\": 0.88, \"precision\": 0.87, \"recall\": 0.86, \"f1\": 0.86},\n",
    "    \"LSTM\": {\"accuracy\": 0.94, \"precision\": 0.93, \"recall\": 0.92, \"f1\": 0.92}\n",
    "}\n",
    "\n",
    "# Create sample confusion matrices\n",
    "confusion_matrices = {\n",
    "    \"RandomForest\": np.array([[450, 50], [40, 460]]),\n",
    "    \"LogisticRegression\": np.array([[430, 70], [60, 440]]),\n",
    "    \"LSTM\": np.array([[460, 40], [30, 470]])\n",
    "}\n",
    "\n",
    "# Create sample time series data\n",
    "dates = pd.date_range(start='2025-01-01', periods=10, freq='D')\n",
    "time_series_metrics = pd.DataFrame({\n",
    "    'timestamp': dates,\n",
    "    'accuracy': np.linspace(0.85, 0.95, 10),\n",
    "    'precision': np.linspace(0.84, 0.94, 10),\n",
    "    'recall': np.linspace(0.83, 0.93, 10),\n",
    "    'f1': np.linspace(0.83, 0.93, 10)\n",
    "})\n",
    "\n",
    "# Create sample feature importance\n",
    "feature_names = [f\"feature_{i}\" for i in range(20)]\n",
    "importance_values = np.random.rand(20)\n",
    "importance_values = importance_values / importance_values.sum()  # Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb87f04",
   "metadata": {},
   "source": [
    "### 3. Create Basic Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70308fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm_fig = plot_confusion_matrix(\n",
    "    confusion_matrices[\"RandomForest\"],\n",
    "    classes=['Fake', 'Real'],\n",
    "    title='Confusion Matrix - RandomForest',\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "cm_norm_fig = plot_confusion_matrix(\n",
    "    confusion_matrices[\"RandomForest\"],\n",
    "    classes=['Fake', 'Real'],\n",
    "    title='Normalized Confusion Matrix - RandomForest',\n",
    "    normalize=True,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390ac7e",
   "metadata": {},
   "source": [
    "### 4. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d218fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison for accuracy\n",
    "acc_fig = plot_model_comparison(\n",
    "    model_metrics,\n",
    "    metric='accuracy',\n",
    "    title='Model Comparison - Accuracy',\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Plot model comparison for F1 score\n",
    "f1_fig = plot_model_comparison(\n",
    "    model_metrics,\n",
    "    metric='f1',\n",
    "    title='Model Comparison - F1 Score',\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77fec1a",
   "metadata": {},
   "source": [
    "### 5. Visualize Metrics Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa0d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics over time\n",
    "time_fig = plot_metrics_over_time(\n",
    "    time_series_metrics,\n",
    "    metrics=['accuracy', 'precision', 'recall', 'f1'],\n",
    "    title='Performance Metrics Over Time',\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698fea44",
   "metadata": {},
   "source": [
    "### 6. Visualize Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18efa327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "feat_fig = plot_feature_importance(\n",
    "    feature_names,\n",
    "    importance_values,\n",
    "    title='Feature Importance',\n",
    "    top_n=10,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c0038",
   "metadata": {},
   "source": [
    "### 7. Create Interactive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive confusion matrix\n",
    "interactive_cm = create_interactive_confusion_matrix(\n",
    "    confusion_matrices[\"RandomForest\"],\n",
    "    classes=['Fake', 'Real'],\n",
    "    title='Interactive Confusion Matrix - RandomForest'\n",
    ")\n",
    "\n",
    "# Create interactive model comparison\n",
    "interactive_models = create_interactive_model_comparison(\n",
    "    model_metrics,\n",
    "    metrics=['accuracy', 'precision', 'recall', 'f1'],\n",
    "    title='Interactive Model Performance Comparison'\n",
    ")\n",
    "\n",
    "# Create interactive time series\n",
    "interactive_time = create_interactive_time_series(\n",
    "    time_series_metrics,\n",
    "    metrics=['accuracy', 'f1'],\n",
    "    title='Interactive Performance Metrics Over Time'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931dc66",
   "metadata": {},
   "source": [
    "### 8. Create Interactive Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive dashboard\n",
    "dashboard = create_interactive_dashboard(\n",
    "    model_metrics,\n",
    "    confusion_matrices,\n",
    "    time_series_metrics,\n",
    "    title='Fake News Detection Dashboard'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426847f",
   "metadata": {},
   "source": [
    "### 9. Export Data for Grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf84555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model metrics for Grafana\n",
    "metrics_path = export_metrics_for_grafana(\n",
    "    model_metrics,\n",
    "    config['grafana_export_path'],\n",
    "    'model_metrics.json'\n",
    ")\n",
    "\n",
    "# Export time series metrics for Grafana\n",
    "time_series_metrics.to_csv(\n",
    "    os.path.join(config['grafana_export_path'], 'time_series_metrics.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Generate Grafana configuration\n",
    "grafana_config = setup_grafana_datasource(\n",
    "    metrics_path,\n",
    "    'Fake News Detection Dashboard'\n",
    ")\n",
    "\n",
    "print(f\"Grafana configuration generated: {grafana_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3283d",
   "metadata": {},
   "source": [
    "### 10. Run Complete Visualization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b74dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete visualization pipeline\n",
    "results = run_visualization_pipeline(\n",
    "    output_dir='/tmp/fake_news_detection/visualizations',\n",
    "    grafana_export=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b44dc",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "1. **Visualization Purpose**: Visualizations help interpret model performance, identify patterns in fake news, and communicate results effectively to stakeholders.\n",
    "\n",
    "2. **Static vs. Interactive**: This notebook provides both static visualizations (using Matplotlib/Seaborn) and interactive visualizations (using Plotly) to serve different needs.\n",
    "\n",
    "3. **Grafana Integration**: For real-time monitoring, data can be exported in formats compatible with Grafana dashboards.\n",
    "\n",
    "4. **Customization**: All visualization functions accept configuration parameters to customize appearance and behavior.\n",
    "\n",
    "5. **Performance Considerations**: For large datasets, consider:\n",
    "   - Using sampling for visualizations\n",
    "   - Limiting the number of features in importance plots\n",
    "   - Using static visualizations instead of interactive ones\n",
    "\n",
    "6. **Databricks Integration**: The code is optimized for Databricks Community Edition with appropriate configurations for memory and processing.\n",
    "\n",
    "7. **Saving Visualizations**: All visualizations can be saved to disk for sharing or inclusion in reports.\n",
    "\n",
    "8. **Dashboard Creation**: The interactive dashboard combines multiple visualizations into a single comprehensive view of model performance."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
