{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f8311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deployment Utilities for Fake News Detection\n",
    "\n",
    "This module provides utilities for deploying the fake news detection pipeline\n",
    "to Databricks Community Edition. It includes functions for packaging code,\n",
    "configuring the environment, and setting up the deployment workflow.\n",
    "\n",
    "Author: BDA Team\n",
    "Date: May 27, 2025\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee5d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import subprocess\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeploymentUtils:\n",
    "    \"\"\"\n",
    "    A class for managing deployment of the fake news detection pipeline to Databricks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, project_root=None, config=None):\n",
    "        \"\"\"\n",
    "        Initialize the DeploymentUtils with optional configuration.\n",
    "        \n",
    "        Args:\n",
    "            project_root (str, optional): Root directory of the project.\n",
    "                If None, defaults to the parent directory of this script.\n",
    "            config (dict, optional): Configuration parameters for deployment.\n",
    "                Supported keys:\n",
    "                - include_dirs (list): Directories to include in deployment. Default: All directories\n",
    "                - exclude_dirs (list): Directories to exclude from deployment. Default: ['logs', '.git']\n",
    "                - include_files (list): Specific files to include. Default: []\n",
    "                - exclude_files (list): Specific files to exclude. Default: []\n",
    "                - databricks_workspace (str): Databricks workspace path. Default: '/Shared/fake_news_detection'\n",
    "        \"\"\"\n",
    "        # Set project root\n",
    "        if project_root is None:\n",
    "            # Default to parent directory of current script\n",
    "            self.project_root = str(Path(__file__).resolve().parent.parent)\n",
    "        else:\n",
    "            self.project_root = project_root\n",
    "        \n",
    "        # Default configuration\n",
    "        default_config = {\n",
    "            'include_dirs': [],  # Empty means include all directories\n",
    "            'exclude_dirs': ['logs', '.git', '__pycache__', '.ipynb_checkpoints'],\n",
    "            'include_files': [],  # Empty means include all files\n",
    "            'exclude_files': ['.DS_Store', '*.pyc', '*.pyo', '*.pyd', '.Python', '*.so'],\n",
    "            'databricks_workspace': '/Shared/fake_news_detection'\n",
    "        }\n",
    "        \n",
    "        # Use provided config or default\n",
    "        self.config = config if config else default_config\n",
    "        \n",
    "        # Ensure exclude_dirs includes standard exclusions\n",
    "        if 'exclude_dirs' not in self.config:\n",
    "            self.config['exclude_dirs'] = default_config['exclude_dirs']\n",
    "        \n",
    "        # Ensure exclude_files includes standard exclusions\n",
    "        if 'exclude_files' not in self.config:\n",
    "            self.config['exclude_files'] = default_config['exclude_files']\n",
    "    \n",
    "    def create_deployment_package(self, output_path=None):\n",
    "        \"\"\"\n",
    "        Create a deployment package (zip file) containing all necessary files.\n",
    "        \n",
    "        Args:\n",
    "            output_path (str, optional): Path to save the deployment package.\n",
    "                If None, defaults to 'deployment_package.zip' in the project root.\n",
    "                \n",
    "        Returns:\n",
    "            str: Path to the created deployment package\n",
    "        \"\"\"\n",
    "        if output_path is None:\n",
    "            output_path = os.path.join(self.project_root, 'deployment_package.zip')\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n",
    "        \n",
    "        # Create zip file\n",
    "        with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            # Walk through project directory\n",
    "            for root, dirs, files in os.walk(self.project_root):\n",
    "                # Filter directories\n",
    "                dirs[:] = [d for d in dirs if self._should_include_dir(d)]\n",
    "                \n",
    "                # Get relative path\n",
    "                rel_path = os.path.relpath(root, self.project_root)\n",
    "                if rel_path == '.':\n",
    "                    rel_path = ''\n",
    "                \n",
    "                # Add files\n",
    "                for file in files:\n",
    "                    if self._should_include_file(file):\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        arcname = os.path.join(rel_path, file)\n",
    "                        zipf.write(file_path, arcname)\n",
    "        \n",
    "        print(f\"Deployment package created at: {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def _should_include_dir(self, dirname):\n",
    "        \"\"\"\n",
    "        Check if a directory should be included in the deployment package.\n",
    "        \n",
    "        Args:\n",
    "            dirname (str): Directory name to check\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if the directory should be included, False otherwise\n",
    "        \"\"\"\n",
    "        # Check if explicitly included\n",
    "        if self.config['include_dirs'] and dirname in self.config['include_dirs']:\n",
    "            return True\n",
    "        \n",
    "        # Check if explicitly excluded\n",
    "        if dirname in self.config['exclude_dirs']:\n",
    "            return False\n",
    "        \n",
    "        # If include_dirs is empty, include all directories not explicitly excluded\n",
    "        return not self.config['include_dirs']\n",
    "    \n",
    "    def _should_include_file(self, filename):\n",
    "        \"\"\"\n",
    "        Check if a file should be included in the deployment package.\n",
    "        \n",
    "        Args:\n",
    "            filename (str): File name to check\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if the file should be included, False otherwise\n",
    "        \"\"\"\n",
    "        # Check if explicitly included\n",
    "        if self.config['include_files'] and any(\n",
    "            filename.endswith(pattern) if pattern.startswith('*.') else filename == pattern\n",
    "            for pattern in self.config['include_files']\n",
    "        ):\n",
    "            return True\n",
    "        \n",
    "        # Check if explicitly excluded\n",
    "        if any(\n",
    "            filename.endswith(pattern[1:]) if pattern.startswith('*.') else filename == pattern\n",
    "            for pattern in self.config['exclude_files']\n",
    "        ):\n",
    "            return False\n",
    "        \n",
    "        # If include_files is empty, include all files not explicitly excluded\n",
    "        return not self.config['include_files']\n",
    "    \n",
    "    def generate_databricks_init_script(self, output_path=None):\n",
    "        \"\"\"\n",
    "        Generate an initialization script for Databricks.\n",
    "        \n",
    "        Args:\n",
    "            output_path (str, optional): Path to save the initialization script.\n",
    "                If None, defaults to 'databricks_init.sh' in the project root.\n",
    "                \n",
    "        Returns:\n",
    "            str: Path to the created initialization script\n",
    "        \"\"\"\n",
    "        if output_path is None:\n",
    "            output_path = os.path.join(self.project_root, 'databricks_init.sh')\n",
    "        \n",
    "        # Create initialization script\n",
    "        script_content = \"\"\"#!/bin/bash\n",
    "# Databricks initialization script for Fake News Detection project\n",
    "\n",
    "# Install required packages\n",
    "pip install --upgrade pip\n",
    "pip install nltk scikit-learn pandas numpy matplotlib seaborn plotly\n",
    "\n",
    "# Download NLTK resources\n",
    "python -c \"import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')\"\n",
    "\n",
    "# Set up environment variables\n",
    "export PYTHONPATH=$PYTHONPATH:/dbfs/FileStore/fake_news_detection\n",
    "\n",
    "# Create necessary directories\n",
    "mkdir -p /dbfs/FileStore/fake_news_detection/logs\n",
    "mkdir -p /dbfs/FileStore/fake_news_detection/models\n",
    "mkdir -p /dbfs/FileStore/fake_news_detection/data\n",
    "\n",
    "echo \"Initialization complete\"\n",
    "\"\"\"\n",
    "        \n",
    "        # Write script to file\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(script_content)\n",
    "        \n",
    "        # Make script executable\n",
    "        os.chmod(output_path, 0o755)\n",
    "        \n",
    "        print(f\"Databricks initialization script created at: {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def generate_deployment_instructions(self, output_path=None):\n",
    "        \"\"\"\n",
    "        Generate deployment instructions for Databricks Community Edition.\n",
    "        \n",
    "        Args:\n",
    "            output_path (str, optional): Path to save the instructions.\n",
    "                If None, defaults to 'deployment_instructions.md' in the project root.\n",
    "                \n",
    "        Returns:\n",
    "            str: Path to the created instructions file\n",
    "        \"\"\"\n",
    "        if output_path is None:\n",
    "            output_path = os.path.join(self.project_root, 'deployment_instructions.md')\n",
    "        \n",
    "        # Create instructions\n",
    "        instructions = \"\"\"# Deployment Instructions for Fake News Detection Project\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks Community Edition account\n",
    "- Web browser\n",
    "\n",
    "## Step 1: Upload Deployment Package\n",
    "1. Log in to your Databricks Community Edition account\n",
    "2. Navigate to the Data tab in the left sidebar\n",
    "3. Click on \"Create\" > \"File Upload\"\n",
    "4. Select the deployment package zip file (`deployment_package.zip`)\n",
    "5. Upload to `/FileStore/fake_news_detection/`\n",
    "\n",
    "## Step 2: Extract the Package\n",
    "1. Create a new notebook\n",
    "2. Add the following code to extract the package:\n",
    "\n",
    "```python\n",
    "# Extract deployment package\n",
    "import zipfile\n",
    "zip_path = \"/dbfs/FileStore/fake_news_detection/deployment_package.zip\"\n",
    "extract_path = \"/dbfs/FileStore/fake_news_detection/\"\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "print(\"Extraction complete\")\n",
    "```\n",
    "\n",
    "3. Run the cell to extract the package\n",
    "\n",
    "## Step 3: Upload Data Files\n",
    "1. Navigate to the Data tab\n",
    "2. Upload the following data files to `/FileStore/fake_news_detection/data/`:\n",
    "   - `True.csv`\n",
    "   - `Fake.csv`\n",
    "   - `stream1.csv` (if using streaming pipeline)\n",
    "\n",
    "## Step 4: Set Up Environment\n",
    "1. Create a new cluster with the following configuration:\n",
    "   - Databricks Runtime: 11.3 LTS (includes Apache Spark 3.3.0, Scala 2.12)\n",
    "   - Python Version: 3.9\n",
    "   - Worker Type: Standard_DS3_v2\n",
    "   - Driver Type: Standard_DS3_v2\n",
    "   - Workers: 1-2 (Community Edition limitation)\n",
    "\n",
    "2. Create a new notebook and attach it to the cluster\n",
    "3. Run the initialization script:\n",
    "\n",
    "```python\n",
    "# Run initialization script\n",
    "dbutils.fs.cp(\"file:/dbfs/FileStore/fake_news_detection/databricks_init.sh\", \"dbfs:/databricks/init/fake_news_init.sh\")\n",
    "```\n",
    "\n",
    "4. Restart the cluster to apply the initialization script\n",
    "\n",
    "## Step 5: Run the Pipeline\n",
    "1. Navigate to the Workspace tab\n",
    "2. Create a new folder called \"Fake News Detection\"\n",
    "3. Import the notebooks from the deployment package:\n",
    "   - 01_data_ingestion.ipynb\n",
    "   - 02_preprocessing.ipynb\n",
    "   - 03_feature_engineering.ipynb\n",
    "   - 04_modeling.ipynb\n",
    "   - 05_graph_analysis.ipynb\n",
    "   - 06_clustering.ipynb\n",
    "   - 07_streaming.ipynb\n",
    "   - 08_visualization.ipynb\n",
    "\n",
    "4. Run the notebooks in sequence to execute the pipeline\n",
    "\n",
    "## Step 6: Monitor and Visualize Results\n",
    "1. The pipeline will generate results in `/FileStore/fake_news_detection/logs/`\n",
    "2. Visualization dashboards will be available in the notebooks\n",
    "3. For Grafana integration, follow the instructions in the visualization notebook\n",
    "\n",
    "## Troubleshooting\n",
    "- If you encounter package import errors, ensure the initialization script ran successfully\n",
    "- For memory issues, try reducing the sample size in the data ingestion notebook\n",
    "- Check the logs for detailed error messages\n",
    "\n",
    "## Community Edition Limitations\n",
    "- Limited compute resources (max 15GB memory)\n",
    "- No API access\n",
    "- Limited cluster size\n",
    "- No MLflow integration\n",
    "- No job scheduler\n",
    "\n",
    "## Workarounds for Limitations\n",
    "- Use smaller data samples\n",
    "- Implement simplified models\n",
    "- Use file-based metrics export instead of direct Grafana integration\n",
    "- Run notebooks manually instead of using job scheduler\n",
    "\"\"\"\n",
    "        \n",
    "        # Write instructions to file\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(instructions)\n",
    "        \n",
    "        print(f\"Deployment instructions created at: {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def create_databricks_notebook_config(self, output_path=None):\n",
    "        \"\"\"\n",
    "        Create configuration for Databricks notebooks.\n",
    "        \n",
    "        Args:\n",
    "            output_path (str, optional): Path to save the configuration.\n",
    "                If None, defaults to 'notebook_config.json' in the project root.\n",
    "                \n",
    "        Returns:\n",
    "            str: Path to the created configuration file\n",
    "        \"\"\"\n",
    "        if output_path is None:\n",
    "            output_path = os.path.join(self.project_root, 'notebook_config.json')\n",
    "        \n",
    "        # Create configuration\n",
    "        config = {\n",
    "            \"notebooks\": [\n",
    "                {\n",
    "                    \"path\": \"01_data_ingestion.ipynb\",\n",
    "                    \"source\": \"01_data_ingestion/data_loader.py\",\n",
    "                    \"description\": \"Data loading and preparation\",\n",
    "                    \"depends_on\": []\n",
    "                },\n",
    "                {\n",
    "                    \"path\": \"02_preprocessing.ipynb\",\n",
    "                    \"source\": \"02_preprocessing/text_preprocessor.py\",\n",
    "                    \"description\": \"Text preprocessing and cleaning\",\n",
    "                    \"depends_on\": [\"01_data_ingestion.ipynb\"]\n",
    "                },\n",
    "                {\n",
    "                    \"path\": \"03_feature_engineering.ipynb\",\n",
    "                    \"source\": \"03_feature_engineering/extract_metadata.py\",\n",
    "                    \"description\": \"Feature extraction and engineering\",\n",
    "                    \"depends_on\": [\"02_preprocessing.ipynb\"]\n",
    "                },\n",
    "                {\n",
    "                    \"path\": \"04_modeling.ipynb\",\n",
    "                    \"source\": [\"04_modeling/baseline_model.py\", \"04_modeling/lstm_model.py\", \"04_modeling/naive_bayes_model.py\"],\n",
    "                    \"description\": \"Model training and evaluation\",\n",
    "                    \"depends_on\": [\"03_feature_engineering.ipynb\"]\n",
    "                },\n",
    "                {\n",
    "                    \"path\": \"05_graph_analysis.ipynb\",\n",
    "                    \"source\": [\"05_graph_analysis/graphx_entity_analysis.py\", \"05_graph_analysis/non_graphx_entity_analysis.py\"],\n",
    "                    \"description\": \"Graph-based entity analysis\",\n",
    "                    \"depends_on\": [\"03_feature_engineering.ipynb\"]\n",
    "                },\n",
    "                {\n",
    "                    \"path\": \"06_clustering.ipynb\",\n",
    "                    \"source\": \"06_clustering/clustering_analysis.py\",\n",
    "                    \"description\": \"Clustering analysis\",\n",
    "                    \"depends_on\": [\"03_feature_engineering.ipynb\"]\n",
    "                },\n",
    "                {\n",
    "                    \"path\": \"07_streaming.ipynb\",\n",
    "                    \"source\": \"07_streaming/streaming_pipeline.py\",\n",
    "                    \"description\": \"Streaming pipeline\",\n",
    "                    \"depends_on\": [\"04_modeling.ipynb\"]\n",
    "                },\n",
    "                {\n",
    "                    \"path\": \"08_visualization.ipynb\",\n",
    "                    \"source\": \"08_visualization/visualization_setup.py\",\n",
    "                    \"description\": \"Visualization and monitoring\",\n",
    "                    \"depends_on\": [\"04_modeling.ipynb\", \"07_streaming.ipynb\"]\n",
    "                }\n",
    "            ],\n",
    "            \"workspace\": self.config['databricks_workspace'],\n",
    "            \"data_path\": \"/FileStore/fake_news_detection/data\",\n",
    "            \"models_path\": \"/FileStore/fake_news_detection/models\",\n",
    "            \"logs_path\": \"/FileStore/fake_news_detection/logs\"\n",
    "        }\n",
    "        \n",
    "        # Write configuration to file\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"Notebook configuration created at: {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def convert_script_to_notebook(self, script_path, output_path=None):\n",
    "        \"\"\"\n",
    "        Convert a Python script to a Jupyter notebook.\n",
    "        \n",
    "        Args:\n",
    "            script_path (str): Path to the Python script\n",
    "            output_path (str, optional): Path to save the notebook.\n",
    "                If None, defaults to the same name with .ipynb extension.\n",
    "                \n",
    "        Returns:\n",
    "            str: Path to the created notebook\n",
    "        \"\"\"\n",
    "        if output_path is None:\n",
    "            output_path = os.path.splitext(script_path)[0] + '.ipynb'\n",
    "        \n",
    "        try:\n",
    "            # Check if jupytext is installed\n",
    "            subprocess.run(['jupytext', '--version'], \n",
    "                          stdout=subprocess.PIPE, \n",
    "                          stderr=subprocess.PIPE, \n",
    "                          check=True)\n",
    "            \n",
    "            # Convert script to notebook\n",
    "            subprocess.run(['jupytext', '--to', 'notebook', script_path, '-o', output_path], \n",
    "                          check=True)\n",
    "            \n",
    "            print(f\"Converted {script_path} to notebook: {output_path}\")\n",
    "            return output_path\n",
    "            \n",
    "        except (subprocess.SubprocessError, FileNotFoundError):\n",
    "            print(\"jupytext not found. Installing...\")\n",
    "            try:\n",
    "                # Install jupytext\n",
    "                subprocess.run(['pip', 'install', 'jupytext'], check=True)\n",
    "                \n",
    "                # Try conversion again\n",
    "                subprocess.run(['jupytext', '--to', 'notebook', script_path, '-o', output_path], \n",
    "                              check=True)\n",
    "                \n",
    "                print(f\"Converted {script_path} to notebook: {output_path}\")\n",
    "                return output_path\n",
    "                \n",
    "            except subprocess.SubprocessError as e:\n",
    "                print(f\"Error converting script to notebook: {e}\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2dec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deployment_utils(project_root=None, config=None):\n",
    "    \"\"\"\n",
    "    Create a DeploymentUtils instance with the specified configuration.\n",
    "    \n",
    "    Args:\n",
    "        project_root (str, optional): Root directory of the project\n",
    "        config (dict, optional): Configuration parameters for deployment\n",
    "            \n",
    "    Returns:\n",
    "        DeploymentUtils: Configured deployment utilities instance\n",
    "    \"\"\"\n",
    "    return DeploymentUtils(project_root=project_root, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate deployment utilities.\n",
    "    \"\"\"\n",
    "    # Create deployment utilities\n",
    "    deploy_utils = create_deployment_utils()\n",
    "    \n",
    "    # Generate deployment artifacts\n",
    "    package_path = deploy_utils.create_deployment_package()\n",
    "    init_script_path = deploy_utils.generate_databricks_init_script()\n",
    "    instructions_path = deploy_utils.generate_deployment_instructions()\n",
    "    config_path = deploy_utils.create_databricks_notebook_config()\n",
    "    \n",
    "    print(\"\\nDeployment artifacts created:\")\n",
    "    print(f\"- Deployment package: {package_path}\")\n",
    "    print(f\"- Initialization script: {init_script_path}\")\n",
    "    print(f\"- Deployment instructions: {instructions_path}\")\n",
    "    print(f\"- Notebook configuration: {config_path}\")\n",
    "    \n",
    "    print(\"\\nTo deploy to Databricks Community Edition, follow the instructions in:\")\n",
    "    print(instructions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Last modified: May 29, 2025
