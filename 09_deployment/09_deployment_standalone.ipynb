{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e31eac4",
   "metadata": {},
   "source": [
    "# Fake News Detection: Deployment\n",
    "\n",
    "This notebook contains all the necessary code for deploying the fake news detection project to Databricks Community Edition. The code is organized into independent functions, without dependencies on external modules or classes, to facilitate execution in Databricks Community Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e833126",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cc4b4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "import zipfile\n",
    "import subprocess\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1ea80",
   "metadata": {},
   "source": [
    "## Reusable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d76a8b",
   "metadata": {},
   "source": [
    "### Configuration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fffefd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_deployment_config(include_dirs=None, exclude_dirs=None, include_files=None, \n",
    "                            exclude_files=None, databricks_workspace=None):\n",
    "    \"\"\"\n",
    "    Create a configuration dictionary for deployment.\n",
    "    \n",
    "    Args:\n",
    "        include_dirs (list): Directories to include in deployment\n",
    "        exclude_dirs (list): Directories to exclude from deployment\n",
    "        include_files (list): Specific files to include\n",
    "        exclude_files (list): Specific files to exclude\n",
    "        databricks_workspace (str): Databricks workspace path\n",
    "        \n",
    "    Returns:\n",
    "        dict: Configuration dictionary\n",
    "    \"\"\"\n",
    "    # Default configuration\n",
    "    config = {\n",
    "        'include_dirs': include_dirs if include_dirs is not None else [],\n",
    "        'exclude_dirs': exclude_dirs if exclude_dirs is not None else ['logs', '.git', '__pycache__', '.ipynb_checkpoints'],\n",
    "        'include_files': include_files if include_files is not None else [],\n",
    "        'exclude_files': exclude_files if exclude_files is not None else ['.DS_Store', '*.pyc', '*.pyo', '*.pyd', '.Python', '*.so'],\n",
    "        'databricks_workspace': databricks_workspace if databricks_workspace is not None else '/Shared/fake_news_detection'\n",
    "    }\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d85d0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_project_root(current_file=None):\n",
    "    \"\"\"\n",
    "    Get the root directory of the project.\n",
    "    \n",
    "    Args:\n",
    "        current_file (str): Path to the current file\n",
    "        \n",
    "    Returns:\n",
    "        str: Absolute path to the project root directory\n",
    "    \"\"\"\n",
    "    if current_file is None:\n",
    "        # Use the current working directory\n",
    "        current_path = os.getcwd()\n",
    "    else:\n",
    "        # Use the directory of the current file\n",
    "        current_path = os.path.dirname(os.path.abspath(current_file))\n",
    "    \n",
    "    # Find the project root (assuming it's the parent of the current directory)\n",
    "    project_root = os.path.abspath(os.path.join(current_path, '..'))\n",
    "    \n",
    "    # Check if we're already at the project root\n",
    "    if os.path.basename(current_path) == 'BDA':\n",
    "        project_root = current_path\n",
    "    \n",
    "    print(f\"Project root identified as: {project_root}\")\n",
    "    return project_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb10378",
   "metadata": {},
   "source": [
    "### Package Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1dfa5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def should_include_dir(dirname, config):\n",
    "    \"\"\"\n",
    "    Check if a directory should be included in the deployment package.\n",
    "    \n",
    "    Args:\n",
    "        dirname (str): Directory name to check\n",
    "        config (dict): Deployment configuration\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the directory should be included, False otherwise\n",
    "    \"\"\"\n",
    "    # Check if explicitly included\n",
    "    if config['include_dirs'] and dirname in config['include_dirs']:\n",
    "        return True\n",
    "    \n",
    "    # Check if explicitly excluded\n",
    "    if dirname in config['exclude_dirs']:\n",
    "        return False\n",
    "    \n",
    "    # If include_dirs is empty, include all directories not explicitly excluded\n",
    "    return not config['include_dirs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c233c3e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def should_include_file(filename, config):\n",
    "    \"\"\"\n",
    "    Check if a file should be included in the deployment package.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): File name to check\n",
    "        config (dict): Deployment configuration\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the file should be included, False otherwise\n",
    "    \"\"\"\n",
    "    # Check if explicitly included\n",
    "    if config['include_files'] and any(\n",
    "        filename.endswith(pattern[1:]) if pattern.startswith('*.') else filename == pattern\n",
    "        for pattern in config['include_files']\n",
    "    ):\n",
    "        return True\n",
    "    \n",
    "    # Check if explicitly excluded\n",
    "    if any(\n",
    "        filename.endswith(pattern[1:]) if pattern.startswith('*.') else filename == pattern\n",
    "        for pattern in config['exclude_files']\n",
    "    ):\n",
    "        return False\n",
    "    \n",
    "    # If include_files is empty, include all files not explicitly excluded\n",
    "    return not config['include_files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be328d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_deployment_package(project_root, config, output_path=None):\n",
    "    \"\"\"\n",
    "    Create a deployment package (zip file) containing all necessary files.\n",
    "    \n",
    "    Args:\n",
    "        project_root (str): Root directory of the project\n",
    "        config (dict): Deployment configuration\n",
    "        output_path (str): Path to save the deployment package\n",
    "            \n",
    "    Returns:\n",
    "        str: Path to the created deployment package\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = os.path.join(project_root, 'deployment_package.zip')\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n",
    "    \n",
    "    print(f\"Creating deployment package from {project_root}...\")\n",
    "    print(f\"Output path: {output_path}\")\n",
    "    \n",
    "    # Create zip file\n",
    "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Walk through project directory\n",
    "        for root, dirs, files in os.walk(project_root):\n",
    "            # Filter directories\n",
    "            dirs[:] = [d for d in dirs if should_include_dir(d, config)]\n",
    "            \n",
    "            # Get relative path\n",
    "            rel_path = os.path.relpath(root, project_root)\n",
    "            if rel_path == '.':\n",
    "                rel_path = ''\n",
    "            \n",
    "            # Add files\n",
    "            for file in files:\n",
    "                if should_include_file(file, config):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.join(rel_path, file)\n",
    "                    zipf.write(file_path, arcname)\n",
    "                    print(f\"Added: {arcname}\")\n",
    "    \n",
    "    print(f\"Deployment package created at: {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d4668",
   "metadata": {},
   "source": [
    "### Databricks Integration Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d59c0c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_databricks_init_script(project_root, output_path=None):\n",
    "    \"\"\"\n",
    "    Generate an initialization script for Databricks.\n",
    "    \n",
    "    Args:\n",
    "        project_root (str): Root directory of the project\n",
    "        output_path (str): Path to save the initialization script\n",
    "            \n",
    "    Returns:\n",
    "        str: Path to the created initialization script\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = os.path.join(project_root, 'databricks_init.sh')\n",
    "    \n",
    "    # Create initialization script\n",
    "    script_content = \"\"\"#!/bin/bash\n",
    "# Databricks initialization script for Fake News Detection project\n",
    "\n",
    "# Install required packages\n",
    "pip install --upgrade pip\n",
    "pip install nltk scikit-learn pandas numpy matplotlib seaborn plotly\n",
    "\n",
    "# Download NLTK resources\n",
    "python -c \"import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')\"\n",
    "\n",
    "# Set up environment variables\n",
    "export PYTHONPATH=$PYTHONPATH:/dbfs/FileStore/fake_news_detection\n",
    "\n",
    "# Create necessary directories\n",
    "mkdir -p /dbfs/FileStore/fake_news_detection/logs\n",
    "mkdir -p /dbfs/FileStore/fake_news_detection/models\n",
    "mkdir -p /dbfs/FileStore/fake_news_detection/data\n",
    "\n",
    "echo \"Initialization complete\"\n",
    "\"\"\"\n",
    "    \n",
    "    # Write script to file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(script_content)\n",
    "    \n",
    "    # Make script executable\n",
    "    os.chmod(output_path, 0o755)\n",
    "    \n",
    "    print(f\"Databricks initialization script created at: {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efee356",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_deployment_instructions(project_root, output_path=None):\n",
    "    \"\"\"\n",
    "    Generate deployment instructions for Databricks Community Edition.\n",
    "    \n",
    "    Args:\n",
    "        project_root (str): Root directory of the project\n",
    "        output_path (str): Path to save the instructions\n",
    "            \n",
    "    Returns:\n",
    "        str: Path to the created instructions file\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = os.path.join(project_root, 'deployment_instructions.md')\n",
    "    \n",
    "    # Create instructions\n",
    "    instructions = \"\"\"# Deployment Instructions for Fake News Detection Project\n",
    "\n",
    "## Prerequisites\n",
    "- Databricks Community Edition account\n",
    "- Web browser\n",
    "\n",
    "## Step 1: Upload Deployment Package\n",
    "1. Log in to your Databricks Community Edition account\n",
    "2. Navigate to the Data tab in the left sidebar\n",
    "3. Click on \"Create\" > \"File Upload\"\n",
    "4. Select the deployment package zip file (`deployment_package.zip`)\n",
    "5. Upload to `/FileStore/fake_news_detection/`\n",
    "\n",
    "## Step 2: Extract the Package\n",
    "1. Create a new notebook\n",
    "2. Add the following code to extract the package:\n",
    "\n",
    "```python\n",
    "# Extract deployment package\n",
    "import zipfile\n",
    "zip_path = \"/dbfs/FileStore/fake_news_detection/deployment_package.zip\"\n",
    "extract_path = \"/dbfs/FileStore/fake_news_detection/\"\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "print(\"Extraction complete\")\n",
    "```\n",
    "\n",
    "3. Run the cell to extract the package\n",
    "\n",
    "## Step 3: Upload Data Files\n",
    "1. Navigate to the Data tab\n",
    "2. Upload the following data files to `/FileStore/fake_news_detection/data/`:\n",
    "   - `True.csv`\n",
    "   - `Fake.csv`\n",
    "   - `stream1.csv` (if using streaming pipeline)\n",
    "\n",
    "## Step 4: Set Up Environment\n",
    "1. Create a new cluster with the following configuration:\n",
    "   - Databricks Runtime: 11.3 LTS (includes Apache Spark 3.3.0, Scala 2.12)\n",
    "   - Python Version: 3.9\n",
    "   - Worker Type: Standard_DS3_v2\n",
    "   - Driver Type: Standard_DS3_v2\n",
    "   - Workers: 1-2 (Community Edition limitation)\n",
    "\n",
    "2. Create a new notebook and attach it to the cluster\n",
    "3. Run the initialization script:\n",
    "\n",
    "```python\n",
    "# Run initialization script\n",
    "dbutils.fs.cp(\"file:/dbfs/FileStore/fake_news_detection/databricks_init.sh\", \"dbfs:/databricks/init/fake_news_init.sh\")\n",
    "```\n",
    "\n",
    "4. Restart the cluster to apply the initialization script\n",
    "\n",
    "## Step 5: Run the Pipeline\n",
    "1. Navigate to the Workspace tab\n",
    "2. Create a new folder called \"Fake News Detection\"\n",
    "3. Import the notebooks from the deployment package:\n",
    "   - 01_data_ingestion_standalone.ipynb\n",
    "   - 02_preprocessing_standalone.ipynb\n",
    "   - 03_feature_engineering_standalone.ipynb\n",
    "   - 04_traditional_models_standalone.ipynb\n",
    "   - 05_graph_analysis_standalone.ipynb\n",
    "   - 06_clustering_standalone.ipynb\n",
    "   - 07_streaming_standalone.ipynb\n",
    "   - 08_visualization_standalone.ipynb\n",
    "\n",
    "4. Run the notebooks in sequence to execute the pipeline\n",
    "\n",
    "## Step 6: Monitor and Visualize Results\n",
    "1. The pipeline will generate results in `/FileStore/fake_news_detection/logs/`\n",
    "2. Visualization dashboards will be available in the notebooks\n",
    "3. For Grafana integration, follow the instructions in the visualization notebook\n",
    "\n",
    "## Troubleshooting\n",
    "- If you encounter package import errors, ensure the initialization script ran successfully\n",
    "- For memory issues, try reducing the sample size in the data ingestion notebook\n",
    "- Check the logs for detailed error messages\n",
    "\n",
    "## Community Edition Limitations\n",
    "- Limited compute resources (max 15GB memory)\n",
    "- No API access\n",
    "- Limited cluster size\n",
    "- No MLflow integration\n",
    "- No job scheduler\n",
    "\n",
    "## Workarounds for Limitations\n",
    "- Use smaller data samples\n",
    "- Implement simplified models\n",
    "- Use file-based metrics export instead of direct Grafana integration\n",
    "- Run notebooks manually instead of using job scheduler\n",
    "\"\"\"\n",
    "    \n",
    "    # Write instructions to file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(instructions)\n",
    "    \n",
    "    print(f\"Deployment instructions created at: {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f424b2c5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_databricks_notebook_config(project_root, output_path=None):\n",
    "    \"\"\"\n",
    "    Create configuration for Databricks notebooks.\n",
    "    \n",
    "    Args:\n",
    "        project_root (str): Root directory of the project\n",
    "        output_path (str): Path to save the configuration\n",
    "            \n",
    "    Returns:\n",
    "        str: Path to the created configuration file\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = os.path.join(project_root, 'notebook_config.json')\n",
    "    \n",
    "    # Create configuration\n",
    "    config = {\n",
    "        \"notebooks\": [\n",
    "            {\n",
    "                \"path\": \"01_data_ingestion_standalone.ipynb\",\n",
    "                \"description\": \"Data loading and preparation\",\n",
    "                \"depends_on\": []\n",
    "            },\n",
    "            {\n",
    "                \"path\": \"02_preprocessing_standalone.ipynb\",\n",
    "                \"description\": \"Text preprocessing and cleaning\",\n",
    "                \"depends_on\": [\"01_data_ingestion_standalone.ipynb\"]\n",
    "            },\n",
    "            {\n",
    "                \"path\": \"03_feature_engineering_standalone.ipynb\",\n",
    "                \"description\": \"Feature extraction and engineering\",\n",
    "                \"depends_on\": [\"02_preprocessing_standalone.ipynb\"]\n",
    "            },\n",
    "            {\n",
    "                \"path\": \"04_traditional_models_standalone.ipynb\",\n",
    "                \"description\": \"Traditional machine learning models\",\n",
    "                \"depends_on\": [\"03_feature_engineering_standalone.ipynb\"]\n",
    "            },\n",
    "            {\n",
    "                \"path\": \"05_graph_analysis_standalone.ipynb\",\n",
    "                \"description\": \"Graph-based analysis\",\n",
    "                \"depends_on\": [\"03_feature_engineering_standalone.ipynb\"]\n",
    "            },\n",
    "            {\n",
    "                \"path\": \"06_clustering_standalone.ipynb\",\n",
    "                \"description\": \"Clustering analysis\",\n",
    "                \"depends_on\": [\"03_feature_engineering_standalone.ipynb\"]\n",
    "            },\n",
    "            {\n",
    "                \"path\": \"07_streaming_standalone.ipynb\",\n",
    "                \"description\": \"Streaming analysis\",\n",
    "                \"depends_on\": [\"04_traditional_models_standalone.ipynb\"]\n",
    "            },\n",
    "            {\n",
    "                \"path\": \"08_visualization_standalone.ipynb\",\n",
    "                \"description\": \"Visualization and dashboards\",\n",
    "                \"depends_on\": [\"04_traditional_models_standalone.ipynb\", \"05_graph_analysis_standalone.ipynb\", \n",
    "                              \"06_clustering_standalone.ipynb\", \"07_streaming_standalone.ipynb\"]\n",
    "            }\n",
    "        ],\n",
    "        \"data\": {\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"path\": \"/FileStore/fake_news_detection/data/True.csv\",\n",
    "                    \"description\": \"True news articles\"\n",
    "                },\n",
    "                {\n",
    "                    \"path\": \"/FileStore/fake_news_detection/data/Fake.csv\",\n",
    "                    \"description\": \"Fake news articles\"\n",
    "                }\n",
    "            ],\n",
    "            \"output\": [\n",
    "                {\n",
    "                    \"path\": \"/FileStore/fake_news_detection/models/\",\n",
    "                    \"description\": \"Trained models\"\n",
    "                },\n",
    "                {\n",
    "                    \"path\": \"/FileStore/fake_news_detection/logs/\",\n",
    "                    \"description\": \"Logs and metrics\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"cluster\": {\n",
    "            \"runtime\": \"11.3 LTS\",\n",
    "            \"python_version\": \"3.9\",\n",
    "            \"worker_type\": \"Standard_DS3_v2\",\n",
    "            \"driver_type\": \"Standard_DS3_v2\",\n",
    "            \"workers\": 2\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Write configuration to file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"Databricks notebook configuration created at: {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf2bf95",
   "metadata": {},
   "source": [
    "### Deployment Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c77fa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def validate_deployment_package(package_path):\n",
    "    \"\"\"\n",
    "    Validate a deployment package by checking its contents.\n",
    "    \n",
    "    Args:\n",
    "        package_path (str): Path to the deployment package\n",
    "            \n",
    "    Returns:\n",
    "        bool: True if the package is valid, False otherwise\n",
    "    \"\"\"\n",
    "    print(f\"Validating deployment package: {package_path}\")\n",
    "    \n",
    "    # Check if package exists\n",
    "    if not os.path.exists(package_path):\n",
    "        print(f\"Error: Package not found at {package_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Check if package is a zip file\n",
    "    if not zipfile.is_zipfile(package_path):\n",
    "        print(f\"Error: {package_path} is not a valid zip file\")\n",
    "        return False\n",
    "    \n",
    "    # Check package contents\n",
    "    required_files = [\n",
    "        'databricks_init.sh',\n",
    "        'deployment_instructions.md',\n",
    "        'notebook_config.json'\n",
    "    ]\n",
    "    \n",
    "    required_dirs = [\n",
    "        '01_data_ingestion',\n",
    "        '02_preprocessing',\n",
    "        '03_feature_engineering',\n",
    "        '04_modeling',\n",
    "        '05_graph_analysis',\n",
    "        '06_clustering',\n",
    "        '07_streaming',\n",
    "        '08_visualization'\n",
    "    ]\n",
    "    \n",
    "    # Extract file list from zip\n",
    "    with zipfile.ZipFile(package_path, 'r') as zipf:\n",
    "        file_list = zipf.namelist()\n",
    "    \n",
    "    # Check required files\n",
    "    missing_files = []\n",
    "    for req_file in required_files:\n",
    "        if not any(f.endswith(req_file) for f in file_list):\n",
    "            missing_files.append(req_file)\n",
    "    \n",
    "    # Check required directories\n",
    "    missing_dirs = []\n",
    "    for req_dir in required_dirs:\n",
    "        if not any(f.startswith(req_dir + '/') for f in file_list):\n",
    "            missing_dirs.append(req_dir)\n",
    "    \n",
    "    # Report validation results\n",
    "    if missing_files:\n",
    "        print(f\"Warning: Missing required files: {', '.join(missing_files)}\")\n",
    "    \n",
    "    if missing_dirs:\n",
    "        print(f\"Warning: Missing required directories: {', '.join(missing_dirs)}\")\n",
    "    \n",
    "    is_valid = not (missing_files or missing_dirs)\n",
    "    \n",
    "    if is_valid:\n",
    "        print(\"Validation successful: Deployment package contains all required files and directories\")\n",
    "    else:\n",
    "        print(\"Validation failed: Deployment package is missing required files or directories\")\n",
    "    \n",
    "    return is_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e4af1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_databricks_compatibility(project_root):\n",
    "    \"\"\"\n",
    "    Check if the project is compatible with Databricks Community Edition.\n",
    "    \n",
    "    Args:\n",
    "        project_root (str): Root directory of the project\n",
    "            \n",
    "    Returns:\n",
    "        dict: Dictionary with compatibility check results\n",
    "    \"\"\"\n",
    "    print(\"Checking Databricks Community Edition compatibility...\")\n",
    "    \n",
    "    compatibility_issues = []\n",
    "    \n",
    "    # Check for large files (>100MB)\n",
    "    large_files = []\n",
    "    for root, dirs, files in os.walk(project_root):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            if os.path.getsize(file_path) > 100 * 1024 * 1024:  # 100MB\n",
    "                large_files.append(os.path.relpath(file_path, project_root))\n",
    "    \n",
    "    if large_files:\n",
    "        compatibility_issues.append(f\"Large files detected (>100MB): {', '.join(large_files)}\")\n",
    "    \n",
    "    # Check for unsupported libraries\n",
    "    unsupported_libraries = []\n",
    "    requirements_path = os.path.join(project_root, 'requirements.txt')\n",
    "    if os.path.exists(requirements_path):\n",
    "        with open(requirements_path, 'r') as f:\n",
    "            requirements = f.read().splitlines()\n",
    "        \n",
    "        # List of libraries known to be problematic in Databricks CE\n",
    "        problematic_libs = ['tensorflow', 'pytorch', 'torch', 'keras', 'ray', 'dask']\n",
    "        \n",
    "        for lib in problematic_libs:\n",
    "            if any(req.startswith(lib) for req in requirements):\n",
    "                unsupported_libraries.append(lib)\n",
    "    \n",
    "    if unsupported_libraries:\n",
    "        compatibility_issues.append(f\"Potentially problematic libraries detected: {', '.join(unsupported_libraries)}\")\n",
    "    \n",
    "    # Check for GPU dependencies\n",
    "    gpu_dependencies = False\n",
    "    for root, dirs, files in os.walk(project_root):\n",
    "        for file in files:\n",
    "            if file.endswith('.py') or file.endswith('.ipynb'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        content = f.read()\n",
    "                        if 'cuda' in content.lower() or 'gpu' in content.lower():\n",
    "                            gpu_dependencies = True\n",
    "                            break\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    if gpu_dependencies:\n",
    "        compatibility_issues.append(\"GPU dependencies detected, which are not supported in Databricks Community Edition\")\n",
    "    \n",
    "    # Prepare result\n",
    "    result = {\n",
    "        \"compatible\": len(compatibility_issues) == 0,\n",
    "        \"issues\": compatibility_issues\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    if result[\"compatible\"]:\n",
    "        print(\"Project is compatible with Databricks Community Edition\")\n",
    "    else:\n",
    "        print(\"Project may have compatibility issues with Databricks Community Edition:\")\n",
    "        for issue in result[\"issues\"]:\n",
    "            print(f\"- {issue}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f5f87",
   "metadata": {},
   "source": [
    "## Complete Deployment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c211b09",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_deployment_pipeline(project_root=None, output_dir=None, include_dirs=None, exclude_dirs=None):\n",
    "    \"\"\"\n",
    "    Run the complete deployment pipeline for the fake news detection project.\n",
    "    \n",
    "    Args:\n",
    "        project_root (str): Root directory of the project\n",
    "        output_dir (str): Directory to save deployment artifacts\n",
    "        include_dirs (list): Directories to include in deployment\n",
    "        exclude_dirs (list): Directories to exclude from deployment\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with references to deployment artifacts\n",
    "    \"\"\"\n",
    "    print(\"Starting deployment pipeline...\")\n",
    "    \n",
    "    # 1. Set up configuration\n",
    "    if project_root is None:\n",
    "        project_root = get_project_root()\n",
    "    \n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.join(project_root, 'deployment')\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create deployment configuration\n",
    "    config = create_deployment_config(\n",
    "        include_dirs=include_dirs,\n",
    "        exclude_dirs=exclude_dirs,\n",
    "        databricks_workspace='/Shared/fake_news_detection'\n",
    "    )\n",
    "    \n",
    "    # 2. Check Databricks compatibility\n",
    "    compatibility = check_databricks_compatibility(project_root)\n",
    "    \n",
    "    if not compatibility[\"compatible\"]:\n",
    "        print(\"Warning: Project may have compatibility issues with Databricks Community Edition\")\n",
    "        print(\"Continuing with deployment process...\")\n",
    "    \n",
    "    # 3. Generate deployment artifacts\n",
    "    \n",
    "    # Create deployment package\n",
    "    package_path = create_deployment_package(\n",
    "        project_root,\n",
    "        config,\n",
    "        output_path=os.path.join(output_dir, 'deployment_package.zip')\n",
    "    )\n",
    "    \n",
    "    # Generate Databricks initialization script\n",
    "    init_script_path = generate_databricks_init_script(\n",
    "        project_root,\n",
    "        output_path=os.path.join(output_dir, 'databricks_init.sh')\n",
    "    )\n",
    "    \n",
    "    # Generate deployment instructions\n",
    "    instructions_path = generate_deployment_instructions(\n",
    "        project_root,\n",
    "        output_path=os.path.join(output_dir, 'deployment_instructions.md')\n",
    "    )\n",
    "    \n",
    "    # Create notebook configuration\n",
    "    notebook_config_path = create_databricks_notebook_config(\n",
    "        project_root,\n",
    "        output_path=os.path.join(output_dir, 'notebook_config.json')\n",
    "    )\n",
    "    \n",
    "    # 4. Validate deployment package\n",
    "    is_valid = validate_deployment_package(package_path)\n",
    "    \n",
    "    if not is_valid:\n",
    "        print(\"Warning: Deployment package validation failed\")\n",
    "        print(\"Review the validation results and fix any issues before deploying\")\n",
    "    \n",
    "    # 5. Prepare result\n",
    "    result = {\n",
    "        \"package_path\": package_path,\n",
    "        \"init_script_path\": init_script_path,\n",
    "        \"instructions_path\": instructions_path,\n",
    "        \"notebook_config_path\": notebook_config_path,\n",
    "        \"is_valid\": is_valid,\n",
    "        \"compatibility\": compatibility\n",
    "    }\n",
    "    \n",
    "    print(f\"Deployment pipeline completed!\")\n",
    "    print(f\"Deployment artifacts saved to: {output_dir}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b3fcf1",
   "metadata": {},
   "source": [
    "## Step-by-Step Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c946c2",
   "metadata": {},
   "source": [
    "### 1. Set Up Deployment Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project root and output directory\n",
    "project_root = get_project_root()\n",
    "output_dir = os.path.join(project_root, 'deployment')\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create deployment configuration\n",
    "config = create_deployment_config(\n",
    "    exclude_dirs=['logs', '.git', '__pycache__', '.ipynb_checkpoints', 'venv'],\n",
    "    databricks_workspace='/Shared/fake_news_detection'\n",
    ")\n",
    "\n",
    "print(f\"Deployment environment set up with output directory: {output_dir}\")\n",
    "print(f\"Configuration: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4417f71",
   "metadata": {},
   "source": [
    "### 2. Check Databricks Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87daa0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the project is compatible with Databricks Community Edition\n",
    "compatibility = check_databricks_compatibility(project_root)\n",
    "\n",
    "if compatibility[\"compatible\"]:\n",
    "    print(\"✅ Project is compatible with Databricks Community Edition\")\n",
    "else:\n",
    "    print(\"⚠️ Project may have compatibility issues with Databricks Community Edition:\")\n",
    "    for issue in compatibility[\"issues\"]:\n",
    "        print(f\"  - {issue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed1c8e",
   "metadata": {},
   "source": [
    "### 3. Create Deployment Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333813d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment package\n",
    "package_path = create_deployment_package(\n",
    "    project_root,\n",
    "    config,\n",
    "    output_path=os.path.join(output_dir, 'deployment_package.zip')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be06b01",
   "metadata": {},
   "source": [
    "### 4. Generate Databricks Initialization Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Databricks initialization script\n",
    "init_script_path = generate_databricks_init_script(\n",
    "    project_root,\n",
    "    output_path=os.path.join(output_dir, 'databricks_init.sh')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d222c",
   "metadata": {},
   "source": [
    "### 5. Generate Deployment Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate deployment instructions\n",
    "instructions_path = generate_deployment_instructions(\n",
    "    project_root,\n",
    "    output_path=os.path.join(output_dir, 'deployment_instructions.md')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7985674",
   "metadata": {},
   "source": [
    "### 6. Create Notebook Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16fb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create notebook configuration\n",
    "notebook_config_path = create_databricks_notebook_config(\n",
    "    project_root,\n",
    "    output_path=os.path.join(output_dir, 'notebook_config.json')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62cd86",
   "metadata": {},
   "source": [
    "### 7. Validate Deployment Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7998c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate deployment package\n",
    "is_valid = validate_deployment_package(package_path)\n",
    "\n",
    "if is_valid:\n",
    "    print(\"✅ Deployment package validation successful\")\n",
    "else:\n",
    "    print(\"⚠️ Deployment package validation failed\")\n",
    "    print(\"   Review the validation results and fix any issues before deploying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efbb18b",
   "metadata": {},
   "source": [
    "### 8. Run Complete Deployment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559bea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete deployment pipeline\n",
    "result = run_deployment_pipeline(\n",
    "    project_root=project_root,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# Print deployment results\n",
    "print(\"\\nDeployment Results:\")\n",
    "print(f\"Package: {result['package_path']}\")\n",
    "print(f\"Init Script: {result['init_script_path']}\")\n",
    "print(f\"Instructions: {result['instructions_path']}\")\n",
    "print(f\"Notebook Config: {result['notebook_config_path']}\")\n",
    "print(f\"Valid: {'Yes' if result['is_valid'] else 'No'}\")\n",
    "print(f\"Compatible: {'Yes' if result['compatibility']['compatible'] else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ddccb",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "1. **Databricks Community Edition Limitations**: The Community Edition has several limitations:\n",
    "   - Limited compute resources (max 15GB memory)\n",
    "   - No API access\n",
    "   - Limited cluster size\n",
    "   - No MLflow integration\n",
    "   - No job scheduler\n",
    "\n",
    "2. **Deployment Package**: The deployment package contains all necessary files for running the fake news detection pipeline in Databricks. It includes:\n",
    "   - Standalone notebooks for each phase\n",
    "   - Initialization script for setting up the environment\n",
    "   - Configuration files for notebooks and clusters\n",
    "   - Deployment instructions\n",
    "\n",
    "3. **Standalone Notebooks**: The deployment uses standalone notebooks that don't rely on external modules or classes. This ensures compatibility with Databricks Community Edition.\n",
    "\n",
    "4. **Data Upload**: You'll need to manually upload the data files to Databricks FileStore. The deployment instructions include steps for this process.\n",
    "\n",
    "5. **Environment Setup**: The initialization script installs required packages and sets up the environment. You'll need to run this script when creating a new cluster.\n",
    "\n",
    "6. **Execution Order**: The notebooks should be run in the specified order to ensure proper data flow through the pipeline.\n",
    "\n",
    "7. **Troubleshooting**: If you encounter issues, check the logs and review the deployment instructions for troubleshooting tips.\n",
    "\n",
    "8. **Performance Optimization**: For better performance in Databricks Community Edition:\n",
    "   - Use smaller data samples\n",
    "   - Implement simplified models\n",
    "   - Optimize Spark configurations\n",
    "   - Use checkpointing to save intermediate results"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
